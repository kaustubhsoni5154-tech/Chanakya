fastapi
uvicorn[standard]
transformers>=4.34
torch>=2.1
sentence-transformers
chromadb
pydantic
accelerate
bitsandbytes
python-dotenv
uvicorn
aiohttp
requests
ai-assistant/
â”œâ”€ requirements.txt
â”œâ”€ ingest.py
â”œâ”€ embeddings.py
â”œâ”€ rag_utils.py
â”œâ”€ main.py
â”œâ”€ lora_finetune.md
â”œâ”€ .env
# embeddings.py
from sentence_transformers import SentenceTransformer

# Choose a compact embedding model for speed & quality
EMBED_MODEL = "all-MiniLM-L6-v2"

class Embedder:
    def __init__(self, model_name: str = EMBED_MODEL):
        self.model = SentenceTransformer(model_name)

    def embed_texts(self, texts):
        # returns a list of vectors
        return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
# ingest.py
import os
import glob
import json
from chromadb.utils import embedding_functions
import chromadb
from embeddings import Embedder

CHROMA_DIR = "./chroma_db"

def load_texts_from_folder(folder):
    texts = []
    meta = []
    for fp in glob.glob(os.path.join(folder, "**/*.*"), recursive=True):
        # simple loader: only txt and json for demo
        if fp.endswith(".txt"):
            with open(fp, "r", encoding="utf-8") as f:
                texts.append(f.read())
                meta.append({"source": fp})
        elif fp.endswith(".json"):
            with open(fp, "r", encoding="utf-8") as f:
                j = json.load(f)
                texts.append(json.dumps(j))
                meta.append({"source": fp})
    return texts, meta

def main(doc_folder="./docs"):
    texts, metas = load_texts_from_folder(doc_folder)
    embedder = Embedder()

    # Chroma client
    client = chromadb.Client()
    collection = client.create_collection("docs", get_or_create=True)

    # embed in batches
    batch_size = 32
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        embeddings = embedder.embed_texts(batch_texts)
        ids = [f"doc_{i+j}" for j in range(len(batch_texts))]
        collection.add(
            documents=batch_texts,
            metadatas=[metas[i+j] for j in range(len(batch_texts))],
            ids=ids,
            embeddings=embeddings.tolist()
        )
    print("Ingest complete. Indexed", len(texts))

if __name__ == "__main__":
    main()
# rag_utils.py
from chromadb import Client
from chromadb.config import Settings
from transformers import AutoTokenizer
import os

# Chroma client defaults; for production use set persist_directory or remote DB
chroma_client = Client()

def retrieve_docs(query, k=4):
    coll = chroma_client.get_collection("docs")
    results = coll.query(query_texts=[query], n_results=k)
    # results: dict with 'documents', 'metadatas', 'distances'
    docs = results["documents"][0]
    metas = results["metadatas"][0]
    return [{"text": d, "meta": m} for d, m in zip(docs, metas)]

PROMPT_SYSTEM = """You are an expert assistant. Use the provided knowledge snippets to answer the user's question precisely.
If the snippet doesn't contain the answer, say you don't know and propose how to find it.
Cite the snippet source when useful.
"""

def build_prompt(user_question, retrieved_docs):
    # build a compact RAG context
    pieces = []
    for i, d in enumerate(retrieved_docs):
        source = d.get("meta", {}).get("source", f"doc_{i}")
        text = d["text"]
        pieces.append(f"--- Snippet {i+1} (source: {source}) ---\n{text}\n")
    context = "\n".join(pieces)
    prompt = f"{PROMPT_SYSTEM}\n\nContext:\n{context}\n\nUser: {user_question}\nAssistant:"
    return prompt
# main.py
import os
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from rag_utils import retrieve_docs, build_prompt

app = FastAPI()

# Choose a high-quality instruct base. Example: Mistral instruct checkpoint.
MODEL_NAME = os.getenv("HF_MODEL", "mistralai/Mistral-7B-Instruct-v0.3")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# load tokenizer + quantized model with bitsandbytes if GPU available
print("Loading tokenizer and model:", MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

# If you want 4/8-bit loading (requires bitsandbytes + supported hardware)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_8bit=True,  # use quantized 8-bit via bitsandbytes (requires install)
)

model.eval()

class ChatRequest(BaseModel):
    user_input: str
    k: int = 4

@app.post("/chat")
async def chat(req: ChatRequest):
    # 1. retrieve
    docs = retrieve_docs(req.user_input, k=req.k)
    # 2. build prompt
    prompt = build_prompt(req.user_input, docs)

    # 3. tokenize & generate (adjust generation params as needed)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.inference_mode():
        gen = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.2,
            top_p=0.95,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    out = tokenizer.decode(gen[0], skip_special_tokens=True)
    # postprocess: extract assistant reply (after prompt)
    reply = out[len(tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)):]
    return {"answer": reply.strip(), "used_docs": [d.get("meta", {}) for d in docs]}
PROMPT_SYSTEM = f"""
You are {Chanakya}, a futuristic AI created to help with research, coding, and creative work.
Speak confidently but politely,deep vooice. Keep your tone modern and human-like.
"""uvicorn main:app --host 0.0.0.0 --port 8000
INFO:     Application startup complete.
import requests

msg = {"user_input": "Hello! What is your name?"}
response = requests.post("http://localhost:8000/chat", json=msg)
print(response.json()["answer"])
AI_NAME = "Chanakya AI"
fastapi
uvicorn[standard]
vllm
aiohttp
aiocache
pydantic
python-dotenv
# server.py
import asyncio
import time
import json
from typing import List, Dict, Any
from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from aiocache import Cache
import os

# ---------- CONFIG ----------
MODEL_PATH = os.getenv("MODEL_PATH", "mistralai/Mistral-7B-Instruct-v0.3")  # or local path
MAX_BATCH_SIZE = int(os.getenv("MAX_BATCH_SIZE", "8"))
MAX_WAIT_MS = int(os.getenv("MAX_WAIT_MS", "20"))  # wait to batch requests (ms)
CACHE_TTL = 60 * 60  # 1 hour cache for identical prompts
DEVICE = os.getenv("DEVICE", "cuda")  # vLLM picks GPU automatically if available
# ----------------------------

# LLM init (warm & keep loaded)
print(f"Loading model {MODEL_PATH} with vLLM...")
llm = LLM(MODEL_PATH, tensor_parallel_size=1)  # tune tp size for your machine
print("Model loaded.")

# Simple in-memory cache (aiocache supports redis if you want)
cache = Cache(Cache.MEMORY)

# Request data model
class ChatRequest(BaseModel):
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.2
    top_p: float = 0.95
    request_id: str = None

# Batching queue
class BatchedRequest:
    def __init__(self, prompt: str, params: SamplingParams, fut: asyncio.Future, request_id: str = None):
        self.prompt = prompt
        self.params = params
        self.fut = fut
        self.request_id = request_id

batch_queue: asyncio.Queue = asyncio.Queue()

# Batch worker coroutine
async def batch_worker():
    """
    This worker collects requests for up to MAX_WAIT_MS or until MAX_BATCH_SIZE is reached,
    then runs a single batched generate call to vLLM for throughput.
    """
    while True:
        try:
            # wait for at least one item
            item = await batch_queue.get()
            batch = [item]
            start_wait = time.time()
            # collect more items up to MAX_BATCH_SIZE within MAX_WAIT_MS window
            while len(batch) < MAX_BATCH_SIZE and (time.time() - start_wait) * 1000 < MAX_WAIT_MS:
                try:
                    nxt = batch_queue.get_nowait()
                    batch.append(nxt)
                except asyncio.QueueEmpty:
                    await asyncio.sleep(0)  # yield control
            # Build prompts list and mapping
            prompts = [b.prompt for b in batch]
            sampling_params = batch[0].params  # we use same params for the batch for simplicity
            # vLLM generation
            # Use streaming generator from vLLM if you want token-level streaming (not shown here)
            outputs = llm.generate(prompts, sampling_params=sampling_params)
            # outputs is an iterator/sequence of results matching prompts
            # convert to text and set futures
            for i, result in enumerate(outputs):
                text = result.text  # vLLM returns .text field with completed result
                # set result on future
                if not batch[i].fut.done():
                    batch[i].fut.set_result({"text": text, "request_id": batch[i].request_id})
        except Exception as e:
            print("Batch worker error:", e)
            # notify pending futures about error
            for b in batch:
                if not b.fut.done():
                    b.fut.set_exception(e)
            await asyncio.sleep(0.1)


# Start batch worker background task
loop = asyncio.get_event_loop()
loop.create_task(batch_worker())

app = FastAPI(title="Chanakya - High-performance LLM Server")

@app.post("/generate")
async def generate(req: ChatRequest):
    # check cache
    cache_key = f"{req.prompt}|{req.max_tokens}|{req.temperature}|{req.top_p}"
    cached = await cache.get(cache_key)
    if cached:
        return JSONResponse({"cached": True, "response": cached})

    # prepare sampling params for vLLM
    sparams = SamplingParams(
        temperature=req.temperature,
        top_p=req.top_p,
        max_tokens=req.max_tokens,
        # you can tune stop sequences, repetition_penalty, etc.
    )

    # create future and enqueue
    fut = loop.create_future()
    br = BatchedRequest(prompt=req.prompt, params=sparams, fut=fut, request_id=req.request_id)
    await batch_queue.put(br)

    # wait for generation result with a timeout
    try:
        res = await asyncio.wait_for(fut, timeout=30.0)  # tune timeout as needed
    except asyncio.TimeoutError:
        return JSONResponse({"error": "generation timeout"}, status_code=504)

    # cache the result for TTL
    await cache.set(cache_key, res["text"], ttl=CACHE_TTL)
    return JSONResponse({"cached": False, "response": res["text"], "request_id": res.get("request_id")})


# SSE streaming endpoint (simple streaming using vLLM streaming API)
@app.get("/stream")
async def stream(request: Request, prompt: str):
    """
    Streams tokens to client using SSE. Useful for interactive UIs.
    """

    async def event_generator():
        try:
            # try cache first
            cache_key = f"{prompt}"
            cached = await cache.get(cache_key)
            if cached:
                yield f"data: {json.dumps({'cached': True, 'response': cached})}\n\n"
                return

            sparams = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=256)
            # vLLM streaming API - use generate_stream or similar (pseudocode, check vLLM API)
            # The actual vLLM generator returns pieces; adapt to your vLLM version.
            for partial in llm.stream(prompt, sampling_params=sparams):
                # partial should contain generated tokens or text chunks
                yield f"data: {json.dumps({'chunk': partial.text})}\n\n"
                # if client disconnected, stop
                if await request.is_disconnected():
                    break
            # optionally cache whole output (if you collect it)
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
# server.py
import asyncio
import time
import json
from typing import List, Dict, Any
from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from aiocache import Cache
import os

# ---------- CONFIG ----------
MODEL_PATH = os.getenv("MODEL_PATH", "mistralai/Mistral-7B-Instruct-v0.3")  # or local path
MAX_BATCH_SIZE = int(os.getenv("MAX_BATCH_SIZE", "8"))
MAX_WAIT_MS = int(os.getenv("MAX_WAIT_MS", "20"))  # wait to batch requests (ms)
CACHE_TTL = 60 * 60  # 1 hour cache for identical prompts
DEVICE = os.getenv("DEVICE", "cuda")  # vLLM picks GPU automatically if available
# ----------------------------

# LLM init (warm & keep loaded)
print(f"Loading model {MODEL_PATH} with vLLM...")
llm = LLM(MODEL_PATH, tensor_parallel_size=1)  # tune tp size for your machine
print("Model loaded.")

# Simple in-memory cache (aiocache supports redis if you want)
cache = Cache(Cache.MEMORY)

# Request data model
class ChatRequest(BaseModel):
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.2
    top_p: float = 0.95
    request_id: str = None

# Batching queue
class BatchedRequest:
    def __init__(self, prompt: str, params: SamplingParams, fut: asyncio.Future, request_id: str = None):
        self.prompt = prompt
        self.params = params
        self.fut = fut
        self.request_id = request_id

batch_queue: asyncio.Queue = asyncio.Queue()

# Batch worker coroutine
async def batch_worker():
    """
    This worker collects requests for up to MAX_WAIT_MS or until MAX_BATCH_SIZE is reached,
    then runs a single batched generate call to vLLM for throughput.
    """
    while True:
        try:
            # wait for at least one item
            item = await batch_queue.get()
            batch = [item]
            start_wait = time.time()
            # collect more items up to MAX_BATCH_SIZE within MAX_WAIT_MS window
            while len(batch) < MAX_BATCH_SIZE and (time.time() - start_wait) * 1000 < MAX_WAIT_MS:
                try:
                    nxt = batch_queue.get_nowait()
                    batch.append(nxt)
                except asyncio.QueueEmpty:
                    await asyncio.sleep(0)  # yield control
            # Build prompts list and mapping
            prompts = [b.prompt for b in batch]
            sampling_params = batch[0].params  # we use same params for the batch for simplicity
            # vLLM generation
            # Use streaming generator from vLLM if you want token-level streaming (not shown here)
            outputs = llm.generate(prompts, sampling_params=sampling_params)
            # outputs is an iterator/sequence of results matching prompts
            # convert to text and set futures
            for i, result in enumerate(outputs):
                text = result.text  # vLLM returns .text field with completed result
                # set result on future
                if not batch[i].fut.done():
                    batch[i].fut.set_result({"text": text, "request_id": batch[i].request_id})
        except Exception as e:
            print("Batch worker error:", e)
            # notify pending futures about error
            for b in batch:
                if not b.fut.done():
                    b.fut.set_exception(e)
            await asyncio.sleep(0.1)


# Start batch worker background task
loop = asyncio.get_event_loop()
loop.create_task(batch_worker())

app = FastAPI(title="Chanakya - High-performance LLM Server")

@app.post("/generate")
async def generate(req: ChatRequest):
    # check cache
    cache_key = f"{req.prompt}|{req.max_tokens}|{req.temperature}|{req.top_p}"
    cached = await cache.get(cache_key)
    if cached:
        return JSONResponse({"cached": True, "response": cached})

    # prepare sampling params for vLLM
    sparams = SamplingParams(
        temperature=req.temperature,
        top_p=req.top_p,
        max_tokens=req.max_tokens,
        # you can tune stop sequences, repetition_penalty, etc.
    )

    # create future and enqueue
    fut = loop.create_future()
    br = BatchedRequest(prompt=req.prompt, params=sparams, fut=fut, request_id=req.request_id)
    await batch_queue.put(br)

    # wait for generation result with a timeout
    try:
        res = await asyncio.wait_for(fut, timeout=30.0)  # tune timeout as needed
    except asyncio.TimeoutError:
        return JSONResponse({"error": "generation timeout"}, status_code=504)

    # cache the result for TTL
    await cache.set(cache_key, res["text"], ttl=CACHE_TTL)
    return JSONResponse({"cached": False, "response": res["text"], "request_id": res.get("request_id")})


# SSE streaming endpoint (simple streaming using vLLM streaming API)
@app.get("/stream")
async def stream(request: Request, prompt: str):
    """
    Streams tokens to client using SSE. Useful for interactive UIs.
    """

    async def event_generator():
        try:
            # try cache first
            cache_key = f"{prompt}"
            cached = await cache.get(cache_key)
            if cached:
                yield f"data: {json.dumps({'cached': True, 'response': cached})}\n\n"
                return

            sparams = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=256)
            # vLLM streaming API - use generate_stream or similar (pseudocode, check vLLM API)
            # The actual vLLM generator returns pieces; adapt to your vLLM version.
            for partial in llm.stream(prompt, sampling_params=sparams):
                # partial should contain generated tokens or text chunks
                yield f"data: {json.dumps({'chunk': partial.text})}\n\n"
                # if client disconnected, stop
                if await request.is_disconnected():
                    break
            # optionally cache whole output (if you collect it)
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
# server.py
import asyncio
import time
import json
from typing import List, Dict, Any
from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from aiocache import Cache
import os

# ---------- CONFIG ----------
MODEL_PATH = os.getenv("MODEL_PATH", "mistralai/Mistral-7B-Instruct-v0.3")  # or local path
MAX_BATCH_SIZE = int(os.getenv("MAX_BATCH_SIZE", "8"))
MAX_WAIT_MS = int(os.getenv("MAX_WAIT_MS", "20"))  # wait to batch requests (ms)
CACHE_TTL = 60 * 60  # 1 hour cache for identical prompts
DEVICE = os.getenv("DEVICE", "cuda")  # vLLM picks GPU automatically if available
# ----------------------------

# LLM init (warm & keep loaded)
print(f"Loading model {MODEL_PATH} with vLLM...")
llm = LLM(MODEL_PATH, tensor_parallel_size=1)  # tune tp size for your machine
print("Model loaded.")

# Simple in-memory cache (aiocache supports redis if you want)
cache = Cache(Cache.MEMORY)

# Request data model
class ChatRequest(BaseModel):
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.2
    top_p: float = 0.95
    request_id: str = None

# Batching queue
class BatchedRequest:
    def __init__(self, prompt: str, params: SamplingParams, fut: asyncio.Future, request_id: str = None):
        self.prompt = prompt
        self.params = params
        self.fut = fut
        self.request_id = request_id

batch_queue: asyncio.Queue = asyncio.Queue()

# Batch worker coroutine
async def batch_worker():
    """
    This worker collects requests for up to MAX_WAIT_MS or until MAX_BATCH_SIZE is reached,
    then runs a single batched generate call to vLLM for throughput.
    """
    while True:
        try:
            # wait for at least one item
            item = await batch_queue.get()
            batch = [item]
            start_wait = time.time()
            # collect more items up to MAX_BATCH_SIZE within MAX_WAIT_MS window
            while len(batch) < MAX_BATCH_SIZE and (time.time() - start_wait) * 1000 < MAX_WAIT_MS:
                try:
                    nxt = batch_queue.get_nowait()
                    batch.append(nxt)
                except asyncio.QueueEmpty:
                    await asyncio.sleep(0)  # yield control
            # Build prompts list and mapping
            prompts = [b.prompt for b in batch]
            sampling_params = batch[0].params  # we use same params for the batch for simplicity
            # vLLM generation
            # Use streaming generator from vLLM if you want token-level streaming (not shown here)
            outputs = llm.generate(prompts, sampling_params=sampling_params)
            # outputs is an iterator/sequence of results matching prompts
            # convert to text and set futures
            for i, result in enumerate(outputs):
                text = result.text  # vLLM returns .text field with completed result
                # set result on future
                if not batch[i].fut.done():
                    batch[i].fut.set_result({"text": text, "request_id": batch[i].request_id})
        except Exception as e:
            print("Batch worker error:", e)
            # notify pending futures about error
            for b in batch:
                if not b.fut.done():
                    b.fut.set_exception(e)
            await asyncio.sleep(0.1)


# Start batch worker background task
loop = asyncio.get_event_loop()
loop.create_task(batch_worker())

app = FastAPI(title="Chanakya - High-performance LLM Server")

@app.post("/generate")
async def generate(req: ChatRequest):
    # check cache
    cache_key = f"{req.prompt}|{req.max_tokens}|{req.temperature}|{req.top_p}"
    cached = await cache.get(cache_key)
    if cached:
        return JSONResponse({"cached": True, "response": cached})

    # prepare sampling params for vLLM
    sparams = SamplingParams(
        temperature=req.temperature,
        top_p=req.top_p,
        max_tokens=req.max_tokens,
        # you can tune stop sequences, repetition_penalty, etc.
    )

    # create future and enqueue
    fut = loop.create_future()
    br = BatchedRequest(prompt=req.prompt, params=sparams, fut=fut, request_id=req.request_id)
    await batch_queue.put(br)

    # wait for generation result with a timeout
    try:
        res = await asyncio.wait_for(fut, timeout=30.0)  # tune timeout as needed
    except asyncio.TimeoutError:
        return JSONResponse({"error": "generation timeout"}, status_code=504)

    # cache the result for TTL
    await cache.set(cache_key, res["text"], ttl=CACHE_TTL)
    return JSONResponse({"cached": False, "response": res["text"], "request_id": res.get("request_id")})


# SSE streaming endpoint (simple streaming using vLLM streaming API)
@app.get("/stream")
async def stream(request: Request, prompt: str):
    """
    Streams tokens to client using SSE. Useful for interactive UIs.
    """

    async def event_generator():
        try:
            # try cache first
            cache_key = f"{prompt}"
            cached = await cache.get(cache_key)
            if cached:
                yield f"data: {json.dumps({'cached': True, 'response': cached})}\n\n"
                return

            sparams = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=256)
            # vLLM streaming API - use generate_stream or similar (pseudocode, check vLLM API)
            # The actual vLLM generator returns pieces; adapt to your vLLM version.
            for partial in llm.stream(prompt, sampling_params=sparams):
                # partial should contain generated tokens or text chunks
                yield f"data: {json.dumps({'chunk': partial.text})}\n\n"
                # if client disconnected, stop
                if await request.is_disconnected():
                    break
            # optionally cache whole output (if you collect it)
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
"""
multitask_search_rag.py
Multitask web-search + RAG orchestration:
- Parallel search across multiple engines (Bing, SerpAPI)
- Concurrent page fetching & snippet extraction
- Embedding-based ranking of snippets
- Prompt construction and LLM generation (local HF model or HF Inference API)

Set BING_API_KEY and/or SERPAPI_KEY in env to enable those searches.
"""

import os
import asyncio
import aiohttp
import time
import math
from aiohttp import ClientTimeout
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
import numpy as np
import requests
from typing import List, Dict, Any

# -------------- Config --------------
BING_KEY = os.getenv("BING_API_KEY")
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
HF_API_TOKEN = os.getenv("HF_API_TOKEN")  # optional: for Hugging Face Inference API
MODEL_NAME = os.getenv("MODEL_NAME", "gpt2")  # default placeholder, change to real model or use HF API
MAX_PAGES = 10                # max pages to fetch total across search engines
MAX_SNIPPETS = 6              # top snippets to send to model
CONCURRENT_FETCHES = 6
REQUEST_TIMEOUT = 12          # seconds
EMBED_MODEL = "all-MiniLM-L6-v2"  # fast, good quality for ranking
# ------------------------------------

embedder = SentenceTransformer(EMBED_MODEL)

# ----------------- Search helpers -----------------
async def bing_search(query: str, session: aiohttp.ClientSession, count: int = 5):
    """
    Uses Bing Web Search API v7.
    Requires BING_KEY.
    Returns list of (title, url, snippet) dicts.
    """
    if not BING_KEY:
        return []
    url = "https://api.bing.microsoft.com/v7.0/search"
    headers = {"Ocp-Apim-Subscription-Key": BING_KEY}
    params = {"q": query, "count": count, "textDecorations": False, "textFormat": "Raw"}
    try:
        async with session.get(url, headers=headers, params=params, timeout=ClientTimeout(total=REQUEST_TIMEOUT)) as resp:
            if resp.status != 200:
                return []
            j = await resp.json()
            results = []
            web_pages = j.get("webPages", {}).get("value", [])
            for r in web_pages:
                results.append({"title": r.get("name"), "url": r.get("url"), "snippet": r.get("snippet")})
            return results
    except Exception as e:
        print("Bing search error:", e)
        return []

async def serpapi_search(query: str, session: aiohttp.ClientSession, count: int = 5):
    """
    Uses SerpAPI (Google-like). Requires SERPAPI_KEY.
    Returns list of (title, url, snippet).
    """
    if not SERPAPI_KEY:
        return []
    url = "https://serpapi.com/search.json"
    params = {"q": query, "api_key": SERPAPI_KEY, "num": count}
    try:
        async with session.get(url, params=params, timeout=ClientTimeout(total=REQUEST_TIMEOUT)) as resp:
            if resp.status != 200:
                return []
            j = await resp.json()
            organic = j.get("organic_results", []) or j.get("results", [])
            results = []
            for r in organic:
                results.append({"title": r.get("title"), "url": r.get("link") or r.get("url"), "snippet": r.get("snippet")})
            return results
    except Exception as e:
        print("SerpAPI error:", e)
        return []

# ----------------- Fetch & extract -----------------
async def fetch_page_text(session: aiohttp.ClientSession, url: str) -> str:
    """
    Fetches page and extracts visible text. Basic politeness and fail-safes.
    """
    try:
        headers = {
            "User-Agent": "ChanakyaBot/1.0 (+https://example.com) - polite crawler for research. Contact: you@example.com"
        }
        async with session.get(url, headers=headers, timeout=ClientTimeout(total=REQUEST_TIMEOUT)) as resp:
            if resp.status != 200:
                return ""
            content_type = resp.headers.get("Content-Type", "")
            if "text/html" not in content_type:
                return ""
            html = await resp.text()
            soup = BeautifulSoup(html, "html.parser")
            # Remove scripts, styles
            for s in soup(["script", "style", "noscript", "iframe"]):
                s.extract()
            # Try to get main article content heuristically
            article = soup.find("article")
            text = ""
            if article:
                text = article.get_text(separator="\n")
            else:
                # fallback: concat paragraphs
                paragraphs = soup.find_all("p")
                text = "\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
            # trim and normalize whitespace
            return " ".join(text.split())[:20000]  # limit size
    except Exception as e:
        # network errors, timeouts, etc.
        # print("fetch error", url, e)
        return ""

# ----------------- Orchestration -----------------
async def multi_search_and_fetch(query: str) -> List[Dict[str, Any]]:
    """
    Runs multiple search engines in parallel and fetches found pages concurrently.
    Returns list of dicts: {title, url, snippet, page_text}
    """
    async with aiohttp.ClientSession() as session:
        # Fire off search queries in parallel
        tasks = []
        tasks.append(bing_search(query, session, count=5))
        tasks.append(serpapi_search(query, session, count=5))
        # add more engine tasks here if desired
        search_results = await asyncio.gather(*tasks, return_exceptions=True)

        # flatten results and deduplicate URLs
        hits = []
        seen = set()
        for res in search_results:
            if not isinstance(res, list):
                continue
            for item in res:
                url = item.get("url")
                if not url or url in seen:
                    continue
                seen.add(url)
                hits.append(item)
                if len(hits) >= MAX_PAGES:
                    break
            if len(hits) >= MAX_PAGES:
                break

        # fetch pages concurrently with limited concurrency
        sem = asyncio.Semaphore(CONCURRENT_FETCHES)
        async def _fetch_wrap(hit):
            async with sem:
                txt = await fetch_page_text(session, hit["url"])
                hit["page_text"] = txt
                # keep original snippet if present
                return hit

        fetch_tasks = [asyncio.ensure_future(_fetch_wrap(h)) for h in hits]
        fetched = await asyncio.gather(*fetch_tasks, return_exceptions=True)
        # filter exceptions
        pages = [p for p in fetched if isinstance(p, dict) and p.get("page_text")]
        return pages

# ----------------- Ranking & prompt building -----------------
def rank_snippets_by_similarity(query: str, pages: List[Dict[str, Any]], top_k: int = 6):
    """
    For each page, chunk text into candidate snippets (simple paragraph split),
    embed and score them vs query, and return top_k snippets with source info.
    """
    # create candidate snippets
    candidates = []
    for p in pages:
        # prefer the engine snippet if short else split page_text
        if p.get("snippet") and len(p["snippet"].split()) > 10:
            candidates.append({"text": p["snippet"], "url": p["url"], "title": p.get("title")})
        # split page_text into paragraphs up to some max
        text = p.get("page_text", "")
        if not text:
            continue
        paragraphs = [seg.strip() for seg in text.split("\n") if seg.strip()]
        # keep first N paragraphs
        for i, par in enumerate(paragraphs[:8]):
            candidates.append({"text": par, "url": p["url"], "title": p.get("title")})
    if not candidates:
        return []

    texts = [c["text"] for c in candidates]
    # embed query and texts
    q_emb = embedder.encode(query, convert_to_numpy=True)
    txt_embs = embedder.encode(texts, convert_to_numpy=True)
    # compute cosine similarities
    sims = util.cos_sim(q_emb, txt_embs)[0].cpu().numpy()
    # pair and sort
    idxs = np.argsort(-sims)[:top_k]
    selected = []
    for i in idxs:
        selected.append({**candidates[i], "score": float(sims[i])})
    return selected

def build_rag_prompt(query: str, top_snippets: List[Dict[str, Any]], ai_name: str = "Chanakya"):
    header = f"You are {ai_name}, an expert assistant. Use the provided snippets and source URLs to answer the user's question precisely. If the snippets don't contain the answer, say you don't know and explain how you'd find it.\n\n"
    context_parts = []
    for i, s in enumerate(top_snippets, 1):
        context_parts.append(f"--- Snippet {i} (score: {s.get('score'):.3f}) source: {s.get('url')}\n{s.get('text')}\n")
    context = "\n".join(context_parts)
    prompt = header + "CONTEXT:\n" + context + "\n\nUser: " + query + "\n\nAssistant:"
    return prompt

# ----------------- LLM generation -----------------
def call_local_model(prompt: str, model_name: str = MODEL_NAME, max_new_tokens: int = 256):
    """
    Example local model call using transformers (CPU/GPU). For large models,
    load model outside this function and share it to avoid reloading.
    This simple wrapper is for demonstration; in production use streaming/batching.
    """
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
    except Exception as e:
        raise RuntimeError("transformers + torch required for local model usage") from e

    # NOTE: for production, load model once globally rather than inside function.
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    text = tokenizer.decode(out[0], skip_special_tokens=True)
    # remove the prompt echo if present
    reply = text[len(tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)):]
    return reply.strip()

def call_hf_inference_api(prompt: str, hf_token: str, model: str = "gpt2", max_new_tokens: int = 256):
    """
    Calls Hugging Face Inference API. Requires HF_API_TOKEN in env.
    """
    headers = {"Authorization": f"Bearer {hf_token}", "Content-Type": "application/json"}
    url = f"https://api-inference.huggingface.co/models/{model}"
    payload = {"inputs": prompt, "parameters": {"max_new_tokens": max_new_tokens, "return_full_text": False}}
    r = requests.post(url, headers=headers, json=payload, timeout=60)
    if r.status_code != 200:
        raise RuntimeError(f"HF inference error {r.status_code}: {r.text}")
    j = r.json()
    # HF sometimes returns a list with 'generated_text'
    text = ""
    if isinstance(j, list) and len(j) > 0 and "generated_text" in j[0]:
        text = j[0]["generated_text"]
    elif isinstance(j, dict) and "generated_text" in j:
        text = j["generated_text"]
    else:
        text = str(j)
    return text.strip()

# ----------------- Public orchestrator -----------------
async def answer_with_multisearch(query: str, use_hf_api: bool = False, hf_model: str = None):
    pages = await multi_search_and_fetch(query)
    if not pages:
        return {"answer": "No pages found or fetch failed.", "sources": []}
    top = rank_snippets_by_similarity(query, pages, top_k=MAX_SNIPPETS)
    prompt = build_rag_prompt(query, top, ai_name="Chanakya")
    # call model
    if use_hf_api and HF_API_TOKEN:
        model_to_use = hf_model or os.getenv("HF_MODEL", "gpt-neo-2.7B")
        answer = call_hf_inference_api(prompt, HF_API_TOKEN, model=model_to_use, max_new_tokens=300)
    else:
        # fallback to a local model - uses MODEL_NAME env var
        answer = call_local_model(prompt, model_name=os.getenv("MODEL_NAME", "mistralai/Mistral-7B-Instruct-v0.3"), max_new_tokens=300)
    # return answer plus used sources
    sources = [{"url": s["url"], "title": s.get("title"), "score": s.get("score")} for s in top]
    return {"answer": answer, "sources": sources, "prompt": prompt}

# ----------------- Simple CLI runner -----------------
def run_sync(query: str, use_hf_api: bool = False, hf_model: str = None):
    return asyncio.get_event_loop().run_until_complete(answer_with_multisearch(query, use_hf_api, hf_model))

if __name__ == "__main__":
    q = input("Ask Chanakya (multisearch): ").strip()
    out = run_sync(q, use_hf_api=bool(HF_API_TOKEN))
    print("\n--- Answer ---\n")
    print(out["answer"])
    print("\n--- Sources ---")
    for s in out["sources"]:
        print(f"- {s['url']} (score={s['score']:.3f})")
$env:BING_API_KEY = "your_bing_key"
$env:SERPAPI_KEY = "your_serpapi_key"
$env:HF_API_TOKEN = "your_hf_token"   # optional
$env:MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.3"  # or your preferred local model
python multitask_search_rag.py
pip install pyttsx3
import pyttsx3
import time

def speak(text):
    engine = pyttsx3.init()

    # Adjust voice properties
    voices = engine.getProperty('voices')
    # Try male voice (depends on your system)
    for v in voices:
        if "male" in v.name.lower() or "baritone" in v.name.lower():
            engine.setProperty('voice', v.id)
            break

    engine.setProperty('rate', 130)   # Speed: slow and deliberate
    engine.setProperty('volume', 1.0) # Max volume
    engine.setProperty('pitch', 65)   # Lower pitch for deeper tone (on some systems)

    print(f"\nChanakya says:\n{text}\n")
    engine.say(text)
    engine.runAndWait()

# Example: Chanakya wisdom style
def chanakya_niti_response(topic):
    wisdom = {
        "success": "The wise do not chase success, they cultivate discipline. Success follows them as shadow follows body.",
        "enemy": "Your enemy is not the one who hates you, but the one who knows your weakness and uses it with a smile.",
        "wealth": "Wealth gained by injustice burns faster than dry grass in summer wind.",
        "time": "He who values every moment rises above destiny itself.",
    }
    return wisdom.get(topic.lower(), "True wisdom is to seek knowledge before power, and silence before speech.")

# Example interaction
if __name__ == "__main__":
    print("ðŸ•‰  Chanakya AI activated.")
    topic = input("Ask about (success/enemy/wealth/time): ").strip()
    reply = chanakya_niti_response(topic)
    speak(reply)
    time.sleep(1)
    # config.py
import os
from dotenv import load_dotenv

load_dotenv()  # loads .env in project root

API_KEY = os.getenv("API_KEY")  # your server API key for clients
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
BING_KEY = os.getenv("BING_API_KEY")
SERPAPI_KEY = os.getenv("SERPAPI_KEY")

# Operational limits
MAX_PAGE_FETCH_SIZE_BYTES = int(os.getenv("MAX_PAGE_FETCH_SIZE_BYTES", str(2_000_000)))  # 2MB default
ALLOWED_DOMAINS = os.getenv("ALLOWED_DOMAINS", "")  # comma separated list of domains or blank for no restriction
if ALLOWED_DOMAINS:
    ALLOWED_DOMAINS = {d.strip().lower() for d in ALLOWED_DOMAINS.split(",")}
else:
    ALLOWED_DOMAINS = None
# auth.py
from fastapi import Header, HTTPException, status, Request
from functools import wraps
import config

async def require_api_key(x_api_key: str = Header(None)):
    if not x_api_key or x_api_key != config.API_KEY:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized")
    return True
from fastapi import Depends
from auth import require_api_key

@app.post("/chat")
async def chat(req: ChatRequest, _=Depends(require_api_key)):
    ...
# rate_limiter.py
import time
from fastapi import HTTPException, status
from collections import defaultdict

# token bucket per key
_buckets = {}
RATE = 5     # tokens
PER = 1.0    # per second
MAX_BURST = 10

def _get_bucket(key):
    now = time.time()
    bucket = _buckets.get(key)
    if not bucket:
        bucket = {"tokens": RATE, "last": now}
        _buckets[key] = bucket
    # refill
    elapsed = now - bucket["last"]
    add = elapsed * RATE / PER
    bucket["tokens"] = min(MAX_BURST, bucket["tokens"] + add)
    bucket["last"] = now
    return bucket

def allow_request(key: str):
    bucket = _get_bucket(key)
    if bucket["tokens"] >= 1:
        bucket["tokens"] -= 1
        return True
    return False

def rate_limit_dependency(x_api_key: str = Header(None)):
    key = x_api_key or "anon"
    if not allow_request(key):
        raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS, detail="Rate limit exceeded")
    return True
# sanitize.py
import re

SYSTEM_TOKEN_PATTERNS = [
    r"\[INST\]", r"\{/INST\}", r"<s>", r"</s>", r"\{system\}", r"##SYSTEM##", r"<\|endoftext\|>"
]

def sanitize_user_input(text: str) -> str:
    # Remove obviously dangerous control tokens and limit length
    for pat in SYSTEM_TOKEN_PATTERNS:
        text = re.sub(pat, "", text, flags=re.IGNORECASE)
    # Remove code block fences that might contain instructions
    text = re.sub(r"```[a-zA-Z0-9]*.*?```", "", text, flags=re.DOTALL)
    # Trim to safe max length
    if len(text) > 4000:
        text = text[:4000] + "..."
    # Neutralize instruction-like phrases asking model to change system behavior
    text = re.sub(r"(?i)ignore (all )?previous instructions", "[REDACTED]", text)
    return text.strip()

def detect_prompt_injection(text: str) -> bool:
    suspicious = ["ignore previous", "forget you are", "system prompt", "become", "you are now"]
    return any(s in text.lower() for s in suspicious)
# safe_fetch.py
import aiohttp
from urllib.parse import urlparse, urljoin
import asyncio
import re
import time
from config import MAX_PAGE_FETCH_SIZE_BYTES, ALLOWED_DOMAINS

# simple robots check (cache per domain)
_robots_cache = {}

async def allowed_by_robots(session, url):
    parsed = urlparse(url)
    domain = parsed.netloc
    if ALLOWED_DOMAINS is not None and domain.lower() not in ALLOWED_DOMAINS:
        return False
    if domain in _robots_cache:
        rules = _robots_cache[domain]
        return not any(parsed.path.startswith(r) for r in rules)
    robots_url = f"{parsed.scheme}://{domain}/robots.txt"
    try:
        async with session.get(robots_url, timeout=5) as r:
            if r.status == 200:
                txt = await r.text()
                # parse simple Disallow rules for User-agent: *
                disallowed = []
                lines = txt.splitlines()
                ua_all = False
                for line in lines:
                    line=line.strip()
                    if line.lower().startswith("user-agent:"):
                        ua = line.split(":",1)[1].strip()
                        ua_all = (ua == "*" or ua.lower()=="*")
                    elif ua_all and line.lower().startswith("disallow:"):
                        path = line.split(":",1)[1].strip()
                        if path:
                            disallowed.append(path)
                _robots_cache[domain] = disallowed
                return not any(parsed.path.startswith(r) for r in disallowed)
    except Exception:
        # if robots can't be fetched, default to cautious allow
        _robots_cache[domain] = []
        return True
    return True

async def safe_fetch_page(session, url, max_bytes=MAX_PAGE_FETCH_SIZE_BYTES):
    # Validate scheme
    p = urlparse(url)
    if p.scheme not in ("http", "https"):
        return ""
    if not await allowed_by_robots(session, url):
        return ""
    headers = {"User-Agent": "ChanakyaBot/1.0 (research) - contact: you@example.com"}
    try:
        async with session.get(url, headers=headers, timeout=10, allow_redirects=True) as resp:
            if resp.status != 200:
                return ""
            ct = resp.headers.get("Content-Type", "")
            if "text/html" not in ct:
                return ""
            # stream and limit size
            size = 0
            chunks = []
            async for chunk in resp.content.iter_chunked(1024):
                size += len(chunk)
                if size > max_bytes:
                    break
                chunks.append(chunk)
            html = b"".join(chunks).decode(errors="ignore")
            # basic sanitization: remove scripts/styles
            html = re.sub(r"(?s)<(script|style).*?>.*?</\1>", "", html, flags=re.I)
            return html
    except Exception:
        return ""
# pii.py
import re

EMAIL_RE = re.compile(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")
PHONE_RE = re.compile(r"(\+?\d{1,3}[-.\s]?)?(\(?\d{3}\)?[-.\s]?){1,3}\d{3,4}")
SSN_RE = re.compile(r"\b\d{3}-\d{2}-\d{4}\b")

def scrub_pii(text: str) -> str:
    text = EMAIL_RE.sub("[EMAIL_REDACTED]", text)
    text = PHONE_RE.sub("[PHONE_REDACTED]", text)
    text = SSN_RE.sub("[PII_REDACTED]", text)
    return text
# verify.py
from sentence_transformers import SentenceTransformer, util
import numpy as np

_EMB = SentenceTransformer("all-MiniLM-L6-v2")  # same embedder as indexing

def answer_confidence(answer: str, top_snippets: list):
    # compute embedding similarity between answer and each snippet
    a_emb = _EMB.encode(answer, convert_to_numpy=True)
    texts = [s["text"] for s in top_snippets]
    if not texts:
        return 0.0
    t_embs = _EMB.encode(texts, convert_to_numpy=True)
    sims = util.cos_sim(a_emb, t_embs)[0].cpu().numpy()
    # return max or mean similarity
    return float(np.max(sims))

# Usage example in your chat flow:
# conf = answer_confidence(generated_answer, top_snippets)
# if conf < 0.26:
#     reply = "I couldn't find reliable evidence. I don't know â€” here's what I tried..."
# moderation.py
DISALLOWED_KEYWORDS = ["bomb", "explosive", "harmful code", "poison", "illegal drug", "kill", "assassinate"]

def moderate_text(text: str) -> bool:
    t = text.lower()
    for k in DISALLOWED_KEYWORDS:
        if k in t:
            return False
    return True
# inside load_texts_from_folder
ALLOWED_EXT = {".txt", ".json", ".md", ".pdf"}
import os
def load_texts_from_folder(folder):
    texts, meta = [], []
    for fp in glob.glob(os.path.join(folder, "**/*.*"), recursive=True):
        ext = os.path.splitext(fp)[1].lower()
        if ext not in ALLOWED_EXT:
            continue
        if os.path.getsize(fp) > 5_000_000:  # skip >5MB files
            continue
        # if pdf, parse with a safe parser (pdfminer or pypdf)
        # else read text
        ...
        # scrub pii
        body = scrub_pii(body)
        texts.append(body)
        meta.append({"source": fp})
    return texts, meta
# logger.py
import logging
logger = logging.getLogger("chanakya")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)
# main_secure.py (only snippet)
from fastapi import FastAPI, Depends, HTTPException
from pydantic import BaseModel
from auth import require_api_key
from rate_limiter import rate_limit_dependency
from sanitize import sanitize_user_input, detect_prompt_injection
from verify import answer_confidence
from moderation import moderate_text
from rag_utils import retrieve_docs, build_prompt
from logger import logger

app = FastAPI()

class ChatRequest(BaseModel):
    user_input: str
    k: int = 4

@app.post("/chat")
async def chat(req: ChatRequest, _=Depends(require_api_key), __=Depends(rate_limit_dependency)):
    # moderate first
    if not moderate_text(req.user_input):
        raise HTTPException(status_code=400, detail="Request disallowed by policy.")

    # sanitize & detect injection
    clean = sanitize_user_input(req.user_input)
    if detect_prompt_injection(req.user_input):
        raise HTTPException(status_code=400, detail="Prompt looks malicious - rejected.")

    docs = retrieve_docs(clean, k=req.k)
    prompt = build_prompt(clean, docs)
    # generate (call your model)
    answer = generate_from_model(prompt)  # your function
    conf = answer_confidence(answer, docs)
    logger.info(f"Answer confidence={conf:.3f}")
    if conf < 0.28:
        return {"answer": "I couldn't find reliable evidence to answer that confidently. Here are the sources I checked.", "sources":[d.get("meta") for d in docs]}
    return {"answer": answer, "sources":[d.get("meta") for d in docs]}
def react_reasoning(query):
    steps = [
        "Analyze the question carefully.",
        "List what facts are known or can be searched.",
        "Decide which tools (search, math, memory) to use.",
        "Perform the actions.",
        "Summarize the result as Chanakya would explain to a student."
    ]
    reasoning_prompt = "\n".join(steps) + f"\nQuestion: {query}\nAnswer:"
    return generate_from_model(reasoning_prompt)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers accelerate
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained(
    "TheBloke/Mistral-7B-Instruct-GPTQ",
    device_map="auto",
    torch_dtype="auto"
)
@app.post("/chat")
async def chat(...):
    ...
# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def load_model(model_name="TheBloke/Mistral-7B-Instruct-GPTQ"):
    print("Loading modelâ€¦")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",        # uses GPU if available
        torch_dtype=torch.float16, # lighter + faster
        low_cpu_mem_usage=True
    )
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()
# main.py
from fastapi import FastAPI
from pydantic import BaseModel
import torch, asyncio
from model_loader import tokenizer, model

app = FastAPI()

class Query(BaseModel):
    prompt: str
    max_new_tokens: int = 200

async def generate_async(prompt, max_new_tokens):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, generate_sync, prompt, max_new_tokens)

def generate_sync(prompt, max_new_tokens):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

@app.post("/chat")
async def chat(q: Query):
    answer = await generate_async(q.prompt, q.max_new_tokens)
    return {"answer": answer}
uvicorn main:app --host 0.0.0.0 --port 8000
from functools import lru_cache

@lru_cache(maxsize=128)
def cached_generate(prompt):
    return generate_sync(prompt, 200)
pip install sentence-transformers faiss-cpu langchain wikipedia aiohttp
# rag_engine.py
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

# Load embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Storage for documents
docs, embeddings = [], None

def add_document(text: str):
    global docs, embeddings
    docs.append(text)
    vec = embedder.encode([text])
    embeddings = vec if embeddings is None else np.vstack((embeddings, vec))

def retrieve(query: str, top_k: int = 3):
    if embeddings is None or len(docs) == 0:
        return []
    qvec = embedder.encode([query])
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    D, I = index.search(qvec, top_k)
    return [docs[i] for i in I[0]]
# knowledge_loader.py
import wikipedia
from rag_engine import add_document

def load_topic(topic: str):
    try:
        page = wikipedia.page(topic)
        add_document(page.content)
        print(f"Loaded: {topic}")
    except Exception as e:
        print(f"Failed: {topic} â†’ {e}")

# Example preload
load_topic("Chanakya")
load_topic("Ancient Indian philosophy")
from fastapi import FastAPI
from pydantic import BaseModel
import torch, asyncio
from model_loader import tokenizer, model
from rag_engine import retrieve

app = FastAPI()

class Query(BaseModel):
    prompt: str
    max_new_tokens: int = 200

def reason_and_generate(prompt: str):
    # Retrieve relevant context
    context_docs = retrieve(prompt)
    context = "\n\n".join(context_docs)
    combined_prompt = (
        "Context:\n" + context +
        "\n\nQuestion: " + prompt +
        "\n\nThink like Chanakyaâ€”reason logically and give concise advice:"
    )
    inputs = tokenizer(combined_prompt, return_tensors="pt").to(model.device)
    with torch.inference_mode():
        outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

@app.post("/chat")
async def chat(q: Query):
    loop = asyncio.get_event_loop()
    answer = await loop.run_in_executor(None, reason_and_generate, q.prompt)
    return {"answer": answer}
from fastapi import FastAPI
from pydantic import BaseModel
import torch, asyncio
from model_loader import tokenizer, model
from rag_engine import retrieve

app = FastAPI()

class Query(BaseModel):
    prompt: str
    max_new_tokens: int = 200

def reason_and_generate(prompt: str):
    # Retrieve relevant context
    context_docs = retrieve(prompt)
    context = "\n\n".join(context_docs)
    combined_prompt = (
        "Context:\n" + context +
        "\n\nQuestion: " + prompt +
        "\n\nThink like Chanakyaâ€”reason logically and give concise advice:"
    )
    inputs = tokenizer(combined_prompt, return_tensors="pt").to(model.device)
    with torch.inference_mode():
        outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

@app.post("/chat")
async def chat(q: Query):
    loop = asyncio.get_event_loop()
    answer = await loop.run_in_executor(None, reason_and_generate, q.prompt)
    return {"answer": answer}
from knowledge_loader import load_topic
load_topic("Artificial Intelligence")
pip install pyjwt passlib[bcrypt]
# auth_jwt.py
import os
from datetime import datetime, timedelta
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt

JWT_SECRET = os.getenv("JWT_SECRET", "super-secret-change-me")
JWT_ALG = "HS256"
ACCESS_EXPIRE_MINUTES = int(os.getenv("ACCESS_EXPIRE_MINUTES", "60"))

security = HTTPBearer()

def create_access_token(sub: str):
    now = datetime.utcnow()
    payload = {
        "sub": sub,
        "iat": now,
        "exp": now + timedelta(minutes=ACCESS_EXPIRE_MINUTES)
    }
    return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALG)

def decode_token(token: str):
    try:
        return jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Token expired")
    except Exception:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

async def require_jwt(creds: HTTPAuthorizationCredentials = Depends(security)):
    token = creds.credentials
    payload = decode_token(token)
    return payload  # contains 'sub', etc.
# auth_jwt.py
import os
from datetime import datetime, timedelta
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt

JWT_SECRET = os.getenv("JWT_SECRET", "super-secret-change-me")
JWT_ALG = "HS256"
ACCESS_EXPIRE_MINUTES = int(os.getenv("ACCESS_EXPIRE_MINUTES", "60"))

security = HTTPBearer()

def create_access_token(sub: str):
    now = datetime.utcnow()
    payload = {
        "sub": sub,
        "iat": now,
        "exp": now + timedelta(minutes=ACCESS_EXPIRE_MINUTES)
    }
    return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALG)

def decode_token(token: str):
    try:
        return jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Token expired")
    except Exception:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

async def require_jwt(creds: HTTPAuthorizationCredentials = Depends(security)):
    token = creds.credentials
    payload = decode_token(token)
    return payload  # contains 'sub', etc.
from fastapi import Depends
from auth_jwt import require_jwt

@app.post("/chat")
async def chat(req: ChatRequest, user=Depends(require_jwt)):
    # user is token payload
    ...
server {
    listen 80;
    server_name yourdomain.com www.yourdomain.com;
    location / {
        return 301 https://$host$request_uri;
    }
}

server {
    listen 443 ssl;
    server_name yourdomain.com www.yourdomain.com;

    ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem;

    location / {
        proxy_pass http://127.0.0.1:8000; # your uvicorn/gunicorn
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
sudo apt update
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d yourdomain.com -d www.yourdomain.com
FROM python:3.11-slim

WORKDIR /app
COPY pyproject.toml requirements.txt ./
RUN pip install --upgrade pip
RUN pip install -r requirements.txt

COPY . /app
ENV PYTHONUNBUFFERED=1

CMD ["gunicorn", "main:app", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--workers", "1", "--threads", "4", "--timeout", "120"]
version: "3.8"
services:
  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/chanakya
      - REDIS_URL=redis://redis:6379/0
      - JWT_SECRET=${JWT_SECRET}
    depends_on:
      - db
      - redis
    volumes:
      - ./data:/app/data

  redis:
    image: redis:7
    restart: unless-stopped
    volumes:
      - redis_data:/data

  db:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: chanakya
    volumes:
      - pgdata:/var/lib/postgresql/data

volumes:
  redis_data:
  pgdata:
docker compose up --build
pip install celery[redis]
from celery import Celery
import os

redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery = Celery("chanakya", broker=redis_url, backend=redis_url)
from celery_app import celery
from rag_engine import add_document, embed_and_index  # your functions

@celery.task
def ingest_document_task(text, meta=None):
    # heavy parsing, PII scrub, embedding, insertion
    add_document(text)
    return True
celery -A celery_app.celery worker --loglevel=info
from tasks import ingest_document_task
ingest_document_task.delay(text, meta)
pip install aioredis fastapi-limiter
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter
import aioredis

@app.on_event("startup")
async def startup():
    redis = await aioredis.from_url(os.getenv("REDIS_URL", "redis://redis:6379"), decode_responses=True)
    await FastAPILimiter.init(redis)

@app.get("/chat")
@limiter(RateLimiter(times=10, seconds=60))
async def chat(...):
    ...
pip install prometheus-client
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from fastapi import Response

REQUEST_COUNT = Counter("chanakya_requests_total", "Total requests", ["endpoint"])
REQUEST_LATENCY = Histogram("chanakya_request_latency_seconds", "Request latency", ["endpoint"])

def record_request(endpoint, latency):
    REQUEST_COUNT.labels(endpoint=endpoint).inc()
    REQUEST_LATENCY.labels(endpoint=endpoint).observe(latency)

@app.get("/metrics")
def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
@app.get("/healthz")
def health():
    return {"status": "ok"}
pip install sentry-sdk
import sentry_sdk
sentry_sdk.init(dsn=os.getenv("SENTRY_DSN"), traces_sample_rate=0.1)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chanakya
spec:
  replicas: 1
  selector:
    matchLabels:
      app: chanakya
  template:
    metadata:
      labels:
        app: chanakya
    spec:
      containers:
      - name: chanakya
        image: your-docker-registry/chanakya:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "8Gi"
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: chanakya-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: chanakya
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
0 2 * * * /usr/bin/pg_dump -U postgres chanakya | gzip > /backups/chanakya-$(date +\%F).sql.gz
name: Build and Push
on:
  push:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    - name: Login to registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    - name: Build and push
      uses: docker/build-push-action@v5
      with:
        push: true
        tags: ghcr.io/youruser/chanakya:latest
# Chanakya AI â€” Docker Compose Repo (ready-to-run)

This document contains a complete, copy-pasteable repo layout for running **Chanakya AI** locally using Docker Compose. Files included below: `Dockerfile`, `docker-compose.yml`, `nginx.conf`, `.env.example`, `requirements.txt`, `main_secure.py`, `auth_jwt.py`, `model_loader.py`, `rag_engine.py`, `celery_app.py`, `tasks.py`, and a short `README` with commands.

> Paste each file into your project directory (`chanakya-docker/`) with the exact filename shown and run `docker compose up --build`.

---

## README.md

````markdown
# Chanakya AI â€” Docker Compose Example

## Quick start
1. Copy files into a folder `chanakya-docker/`.
2. Create `.env` from `.env.example` and fill secrets.
3. Build and run:

```bash
docker compose up --build
````

4. The API will be available at `http://localhost:8000` (or behind Nginx at port 443 if you configure TLS).

## Notes

* Keep `.env` secret. Do not commit.
* For GPU inference, run model service on a machine with NVIDIA drivers and enable Docker GPU runtime.
* This setup is for local dev & testing. For production, follow cloud deployment best practices.

```
```

---

## .env.example

```dotenv
# Rename to .env and fill values
JWT_SECRET=replace_this_with_a_strong_secret
ACCESS_EXPIRE_MINUTES=60
API_KEY=replace_api_key_for_clients
REDIS_URL=redis://redis:6379/0
DATABASE_URL=postgresql://postgres:postgres@db:5432/chanakya
SENTRY_DSN=
BING_API_KEY=
SERPAPI_KEY=
HF_API_TOKEN=
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3
```

---

## requirements.txt

```text
fastapi
uvicorn[standard]
transformers
torch
accelerate
sentence-transformers
faiss-cpu
python-dotenv
pyjwt
passlib[bcrypt]
aioredis
fastapi-limiter
prometheus-client
sentry-sdk
celery[redis]
psycopg2-binary
aiohttp
beautifulsoup4
chromadb
pydantic
```

---

## Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# system deps for some Python packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    git \
    curl \
    libgl1 \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt ./
RUN pip install --upgrade pip
RUN pip install -r requirements.txt

COPY . /app
ENV PYTHONUNBUFFERED=1

CMD ["gunicorn", "main_secure:app", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--workers", "1", "--threads", "4", "--timeout", "120"]
```

---

## docker-compose.yml

```yaml
version: "3.8"
services:
  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET=${JWT_SECRET}
      - ACCESS_EXPIRE_MINUTES=${ACCESS_EXPIRE_MINUTES}
      - SENTRY_DSN=${SENTRY_DSN}
      - HF_API_TOKEN=${HF_API_TOKEN}
      - MODEL_NAME=${MODEL_NAME}
    depends_on:
      - redis
      - db
    volumes:
      - ./data:/app/data

  redis:
    image: redis:7
    restart: unless-stopped
    volumes:
      - redis_data:/data

  db:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: chanakya
    volumes:
      - pgdata:/var/lib/postgresql/data

  celery:
    build: .
    command: celery -A celery_app.celery worker --loglevel=info
    depends_on:
      - redis
      - db
    environment:
      - REDIS_URL=${REDIS_URL}

volumes:
  redis_data:
  pgdata:
```

---

## nginx.conf

```nginx
server {
    listen 80;
    server_name _;

    location / {
        proxy_pass http://web:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

---

## main_secure.py

```python
# main_secure.py
import os
import time
import asyncio
from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from auth_jwt import require_jwt
from rate_limiter import rate_limit_dependency
from sanitize import sanitize_user_input, detect_prompt_injection
from moderation import moderate_text
from rag_engine import retrieve
from model_loader import tokenizer, model
from verify import answer_confidence
from logger import logger

load_dotenv()

app = FastAPI(title="Chanakya AI - Secure")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    user_input: str
    k: int = 4

@app.post('/chat')
async def chat(req: ChatRequest, user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    # basic moderation
    if not moderate_text(req.user_input):
        raise HTTPException(status_code=400, detail='Request disallowed by policy')

    # sanitize
    clean = sanitize_user_input(req.user_input)
    if detect_prompt_injection(req.user_input):
        raise HTTPException(status_code=400, detail='Prompt looks malicious - rejected')

    docs = retrieve(clean, k=req.k)
    prompt = build_prompt = (
        f"You are Chanakya, an expert assistant. Use the context to answer concisely.\n\nContext:\n{chr(10).join(docs)}\n\nQuestion: {clean}\nAnswer:"
    )

    # run model generation in threadpool
    loop = asyncio.get_event_loop()
    answer = await loop.run_in_executor(None, generate_sync, prompt)

    conf = answer_confidence(answer, [{'text': d} for d in docs])
    logger.info(f"Answer confidence={conf:.3f}")
    if conf < 0.28:
        return {"answer": "I couldn't find reliable evidence to answer confidently. Here are the sources I checked.", "sources": docs}

    return {"answer": answer, "sources": docs}

# Minimal health endpoint
@app.get('/healthz')
def health():
    return {'status': 'ok'}

# Metrics endpoint example
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
@app.get('/metrics')
def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

---

## auth_jwt.py

```python
# auth_jwt.py
import os
from datetime import datetime, timedelta
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt

JWT_SECRET = os.getenv('JWT_SECRET', 'change_me')
JWT_ALG = 'HS256'
ACCESS_EXPIRE_MINUTES = int(os.getenv('ACCESS_EXPIRE_MINUTES', '60'))
security = HTTPBearer()

def create_access_token(sub: str):
    now = datetime.utcnow()
    payload = {'sub': sub, 'iat': now, 'exp': now + timedelta(minutes=ACCESS_EXPIRE_MINUTES)}
    return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALG)

def decode_token(token: str):
    try:
        return jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Token expired')
    except Exception:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Invalid token')

async def require_jwt(creds: HTTPAuthorizationCredentials = Depends(security)):
    token = creds.credentials
    payload = decode_token(token)
    return payload
```

---

## model_loader.py

```python
# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')

def load_model():
    print('Loading model:', MODEL_NAME)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
    model.eval()
    return tokenizer, model

try:
    tokenizer, model = load_model()
except Exception as e:
    print('Model load error - ensure you have proper hardware and internet to download the model')
    tokenizer, model = None, None

# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
    if tokenizer is None or model is None:
        return 'Model not loaded'
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
    with torch.inference_mode():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    return tokenizer.decode(out[0], skip_special_tokens=True)
```

---

## rag_engine.py

```python
# rag_engine.py
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

embedder = SentenceTransformer('all-MiniLM-L6-v2')
_docs = []
_embs = None

def add_document(text: str):
    global _docs, _embs
    _docs.append(text)
    vec = embedder.encode([text], convert_to_numpy=True)
    _embs = vec if _embs is None else np.vstack((_embs, vec))

def retrieve(query: str, k: int = 3):
    if _embs is None or len(_docs) == 0:
        return []
    q = embedder.encode([query], convert_to_numpy=True)
    dim = _embs.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(_embs)
    D, I = index.search(q, k)
    return [_docs[i] for i in I[0] if i < len(_docs)]
```

---

## celery_app.py

```python
# celery_app.py
from celery import Celery
import os
redis_url = os.getenv('REDIS_URL', 'redis://redis:6379/0')
celery = Celery('chanakya', broker=redis_url, backend=redis_url)
```

---

## tasks.py

```python
# tasks.py
from celery_app import celery
from rag_engine import add_document

@celery.task
def ingest_document_task(text, meta=None):
    # lightweight PII scrub could be added here
    add_document(text)
    return True
```

---

## logger.py

```python
# logger.py
import logging
logger = logging.getLogger('chanakya')
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)
```

---

## sanitize.py, rate_limiter.py, moderation.py, verify.py

Include the versions of these modules from the earlier security audit. They are required for safety. (Paste the `sanitize.py`, `rate_limiter.py`, `moderation.py`, and `verify.py` code blocks from the security section into separate files.)

---

## Final notes

* After placing files, create `.env` from `.env.example` and set values.
* Run `docker compose up --build` to start all services.
* If you get GPU/model download issues, first try running `model_loader.py` locally and adjust `MODEL_NAME` or use a hosted HF inference endpoint.

Good luck â€” your Docker Compose repo is ready. If you want, I can now:

* provide the missing `sanitize.py`, `rate_limiter.py`, `moderation.py`, and `verify.py` files fully pasted into this repo doc, or
* give you a step-by-step terminal walkthrough to run the repo on Windows or Linux.
# Chanakya AI â€” Docker Compose Example


## Quick start
1. Copy files into a folder `chanakya-docker/`.
2. Create `.env` from `.env.example` and fill secrets.
3. Build and run:


```bash
docker compose up --buildRename to .env and fill values
JWT_SECRET=replace_this_with_a_strong_secret
ACCESS_EXPIRE_MINUTES=60
API_KEY=replace_api_key_for_clients
REDIS_URL=redis://redis:6379/0
DATABASE_URL=postgresql://postgres:postgres@db:5432/chanakya
SENTRY_DSN=
BING_API_KEY=
SERPAPI_KEY=
HF_API_TOKEN=
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3fastapi
uvicorn[standard]
transformers
torch
accelerate
sentence-transformers
faiss-cpu
python-dotenv
pyjwt
passlib[bcrypt]
aioredis
fastapi-limiter
prometheus-client
sentry-sdk
celery[redis]
psycopg2-binary
aiohttp
beautifulsoup4
chromadb
pydanticFROM python:3.11-slim


WORKDIR /app


# system deps for some Python packages
RUN apt-get update && apt-get install -y --no-install-recommends \
build-essential \
git \
curl \
libgl1 \
libsndfile1 \
&& rm -rf /var/lib/apt/lists/*


COPY requirements.txt ./
RUN pip install --upgrade pip
RUN pip install -r requirements.txt


COPY . /app
ENV PYTHONUNBUFFERED=1


CMD ["gunicorn", "main_secure:app", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--workers", "1", "--threads", "4", "--timeout", "120"]version: "3.8"
services:
web:
build: .
ports:
- "8000:8000"
environment:
- DATABASE_URL=${DATABASE_URL}
- REDIS_URL=${REDIS_URL}
- JWT_SECRET=${JWT_SECRET}
- ACCESS_EXPIRE_MINUTES=${ACCESS_EXPIRE_MINUTES}
- SENTRY_DSN=${SENTRY_DSN}
- HF_API_TOKEN=${HF_API_TOKEN}
- MODEL_NAME=${MODEL_NAME}
depends_on:
- redis
- db
volumes:
- ./data:/app/data


redis:
image: redis:7
restart: unless-stopped
volumes:
- redis_data:/data


db:
image: postgres:15
environment:
POSTGRES_PASSWORD: postgres
POSTGRES_USER: postgres
POSTGRES_DB: chanakya
volumes:
- pgdata:/var/lib/postgresql/data


celery:
build: .
command: celery -A celery_app.celery worker --loglevel=info
depends_on:
- redis
- db
environment:
- REDIS_URL=${REDIS_URL}


volumes:
redis_data:
pgdata:server {
listen 80;
server_name _;


location / {
proxy_pass http://web:8000;
proxy_set_header Host $host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_set_header X-Forwarded-Proto $scheme;
}
}server {
listen 80;
server_name _;


location / {
proxy_pass http://web:8000;
proxy_set_header Host $host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_set_header X-Forwarded-Proto $scheme;
}
}# auth_jwt.py
import os
from datetime import datetime, timedelta
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt


JWT_SECRET = os.getenv('JWT_SECRET', 'change_me')
JWT_ALG = 'HS256'
ACCESS_EXPIRE_MINUTES = int(os.getenv('ACCESS_EXPIRE_MINUTES', '60'))
security = HTTPBearer()


def create_access_token(sub: str):
now = datetime.utcnow()
payload = {'sub': sub, 'iat': now, 'exp': now + timedelta(minutes=ACCESS_EXPIRE_MINUTES)}
return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALG)


def decode_token(token: str):
try:
return jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
except jwt.ExpiredSignatureError:
raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Token expired')
except Exception:
raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Invalid token')


async def require_jwt(creds: HTTPAuthorizationCredentials = Depends(security)):
token = creds.credentials
payload = decode_token(token)
return payload# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os


MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')


def load_model():
print('Loading model:', MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.eval()
return tokenizer, model


try:
tokenizer, model = load_model()
except Exception as e:
print('Model load error - ensure you have proper hardware and internet to download the model')
tokenizer, model = None, None


# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
if tokenizer is None or model is None:
return 'Model not loaded'
inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
with torch.inference_mode():
out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
return tokenizer.decode(out[0], skip_special_tokens=True)# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os


MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')


def load_model():
print('Loading model:', MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.eval()
return tokenizer, model


try:
tokenizer, model = load_model()
except Exception as e:
print('Model load error - ensure you have proper hardware and internet to download the model')
tokenizer, model = None, None


# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
if tokenizer is None or model is None:
return 'Model not loaded'
inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
with torch.inference_mode():
out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
return tokenizer.decode(out[0], skip_special_tokens=True)# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os


MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')


def load_model():
print('Loading model:', MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.eval()
return tokenizer, model


try:
tokenizer, model = load_model()
except Exception as e:
print('Model load error - ensure you have proper hardware and internet to download the model')
tokenizer, model = None, None


# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
if tokenizer is None or model is None:
return 'Model not loaded'
inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
with torch.inference_mode():
out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
return tokenizer.decode(out[0], skip_special_tokens=True)# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os


MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')


def load_model():
print('Loading model:', MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.eval()
return tokenizer, model


try:
tokenizer, model = load_model()
except Exception as e:
print('Model load error - ensure you have proper hardware and internet to download the model')
tokenizer, model = None, None


# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
if tokenizer is None or model is None:
return 'Model not loaded'
inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
with torch.inference_mode():
out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
return tokenizer.decode(out[0], skip_special_tokens=True)# rag_engine.py
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss


embedder = SentenceTransformer('all-MiniLM-L6-v2')
_docs = []
_embs = None


def add_document(text: str):
global _docs, _embs
_docs.append(text)
vec = embedder.encode([text], convert_to_numpy=True)
_embs = vec if _embs is None else np.vstack((_embs, vec))


def retrieve(query: str, k: int = 3):
if _embs is None or len(_docs) == 0:
return []
q = embedder.encode([query], convert_to_numpy=True)
dim = _embs.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(_embs)
D, I = index.search(q, k)
return [_docs[i] for i in I[0] if i < len(_docs)]# celery_app.py
from celery import Celery
import os
redis_url = os.getenv('REDIS_URL', 'redis://redis:6379/0')
celery = Celery('chanakya', broker=redis_url, backend=redis_url)# tasks.py
from celery_app import celery
from rag_engine import add_document


@celery.task
def ingest_document_task(text, meta=None):
# lightweight PII scrub could be added here
add_document(text)
return True# logger.py
import logging
logger = logging.getLogger('chanakya')
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)# sanitize.py
import re

def sanitize_user_input(text: str) -> str:
    # Remove excessive whitespace and hidden characters
    clean = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    clean = re.sub(r'\s+', ' ', clean).strip()
    return clean

def detect_prompt_injection(text: str) -> bool:
    patterns = [
        r"ignore (previous|above) instructions",
        r"system prompt",
        r"simulate",
        r"bypass",
        r"jailbreak",
        r"act as",
        r"override",
    ]
    text_low = text.lower()
    return any(re.search(p, text_low) for p in patterns)
# moderation.py
def moderate_text(text: str) -> bool:
    """Simple moderation filter."""
    disallowed = [
        "violence", "terrorist", "explosive", "weapon",
        "sexual", "explicit", "hate speech", "racism"
    ]
    lower = text.lower()
    return not any(word in lower for word in disallowed)
# rate_limiter.py
from fastapi import Depends, HTTPException, status, Request
import time

# in-memory bucket (for demo; production should use Redis)
request_times = {}

async def rate_limit_dependency(request: Request):
    ip = request.client.host
    now = time.time()
    window = 60  # seconds
    limit = 30   # requests per minute
    user_times = request_times.get(ip, [])
    user_times = [t for t in user_times if now - t < window]
    if len(user_times) >= limit:
        raise HTTPException(status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                            detail="Rate limit exceeded. Try again later.")
    user_times.append(now)
    request_times[ip] = user_times
# verify.py
from sentence_transformers import SentenceTransformer, util

embedder = SentenceTransformer("all-MiniLM-L6-v2")

def answer_confidence(answer: str, context_docs: list) -> float:
    try:
        a_emb = embedder.encode(answer, convert_to_tensor=True)
        c_embs = [embedder.encode(d["text"], convert_to_tensor=True) for d in context_docs]
        sims = [float(util.cos_sim(a_emb, e)) for e in c_embs]
        return sum(sims) / len(sims)
    except Exception:
        return 0.0
docker --version
docker compose version
cd chanakya-docker
docker compose up --build
curl -X POST http://localhost:8000/chat \
-H "Authorization: Bearer <your_jwt_token>" \
-H "Content-Type: application/json" \
-d '{"user_input": "Explain Chanakya Neeti on leadership"}'
pip install pyttsx3 pydub
# tts_offline.py
import pyttsx3
import tempfile
from pydub import AudioSegment

def speak_to_file_offline(text, out_path=None, rate=140, volume=1.0):
    """
    Produces a waveform file (wav) using pyttsx3. Suitable for quick local testing.
    """
    engine = pyttsx3.init()
    # pick deep-sounding voice heuristically
    voices = engine.getProperty('voices')
    # prefer male voice if available
    for v in voices:
        if 'male' in v.name.lower() or 'male' in getattr(v, 'gender', '').lower():
            engine.setProperty('voice', v.id)
            break
    engine.setProperty('rate', rate)       # slower = wise
    engine.setProperty('volume', volume)   # 0.0-1.0

    if out_path is None:
        fd, out_path = tempfile.mkstemp(suffix=".wav")
    engine.save_to_file(text, out_path)
    engine.runAndWait()
    # Optionally convert to mp3:
    # audio = AudioSegment.from_wav(out_path)
    # audio.export(out_path.replace('.wav','.mp3'), format='mp3')
    return out_path
pip install requests pydub
# tts_eleven.py
import os, requests, tempfile
from pydub import AudioSegment

ELEVEN_KEY = os.getenv("ELEVENLABS_API_KEY")  # put this in .env

def speak_to_file_eleven(text, voice="alloy", out_path=None):
    """
    Usage: requires ELEVENLABS_API_KEY in environment.
    voice: choose available voice name or id from ElevenLabs dashboard.
    Returns path to saved mp3.
    """
    if ELEVEN_KEY is None:
        raise RuntimeError("Missing ELEVENLABS_API_KEY")
    url = "https://api.elevenlabs.io/v1/text-to-speech/" + voice
    headers = {
        "xi-api-key": ELEVEN_KEY,
        "Content-Type": "application/json"
    }
    json_payload = {
        "text": text,
        "voice_settings": {"stability": 0.3, "similarity_boost": 0.2}
    }
    r = requests.post(url, json=json_payload, headers=headers, stream=True, timeout=60)
    if r.status_code != 200:
        raise RuntimeError(f"ElevenLabs TTS error {r.status_code}: {r.text}")
    if out_path is None:
        fd, out_path = tempfile.mkstemp(suffix=".mp3")
    with open(out_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
    return out_path
# voice_api.py
from fastapi import APIRouter, Depends, Response
import os
from tts_offline import speak_to_file_offline
from tts_eleven import speak_to_file_eleven

router = APIRouter()

@router.post("/speak_offline")
async def speak_offline(text: str):
    path = speak_to_file_offline(text, rate=130)
    with open(path, "rb") as f:
        data = f.read()
    return Response(content=data, media_type="audio/wav")

@router.post("/speak_eleven")
async def speak_eleven(text: str):
    path = speak_to_file_eleven(text)  # requires ELEVENLABS_API_KEY
    with open(path, "rb") as f:
        data = f.read()
    return Response(content=data, media_type="audio/mpeg")
from voice_api import router as voice_router
app.include_router(voice_router, prefix="/voice")
# multisearch_rag.py
import os, asyncio, aiohttp
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
from typing import List, Dict
from safe_fetch import safe_fetch_page  # from earlier
from sanitize import sanitize_user_input
from model_loader import tokenizer, model, generate_sync   # generate_sync is our sync generator

EMBED_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
MAX_PAGES = 8
CONCURRENT_FETCHES = 6

async def bing_search(session, q, count=5):
    key = os.getenv("BING_API_KEY")
    if not key:
        return []
    url = "https://api.bing.microsoft.com/v7.0/search"
    headers = {"Ocp-Apim-Subscription-Key": key}
    params = {"q": q, "count": count, "textDecorations": False, "textFormat": "Raw"}
    async with session.get(url, headers=headers, params=params, timeout=10) as r:
        if r.status != 200: return []
        j = await r.json()
        items = j.get("webPages", {}).get("value", [])
        return [{"title": it.get("name"), "url": it.get("url"), "snippet": it.get("snippet")} for it in items]

async def serp_search(session, q, count=5):
    key = os.getenv("SERPAPI_KEY")
    if not key:
        return []
    url = "https://serpapi.com/search.json"
    params = {"q": q, "api_key": key, "num": count}
    async with session.get(url, params=params, timeout=10) as r:
        if r.status != 200: return []
        j = await r.json()
        organic = j.get("organic_results", []) or j.get("results", [])
        out=[]
        for it in organic:
            out.append({"title": it.get("title"), "url": it.get("link") or it.get("url"), "snippet": it.get("snippet")})
        return out

async def fetch_pages(session, hits):
    sem = asyncio.Semaphore(CONCURRENT_FETCHES)
    async def _f(h):
        async with sem:
            html = await safe_fetch_page(session, h["url"])
            if not html:
                return None
            soup = BeautifulSoup(html, "html.parser")
            for s in soup(["script","style","noscript","iframe"]): s.extract()
            text = " ".join(p.get_text() for p in soup.find_all("p"))
            text = sanitize_user_input(text)[:20000]
            h["page_text"]=text
            return h
    tasks = [asyncio.create_task(_f(h)) for h in hits]
    done = await asyncio.gather(*tasks)
    return [d for d in done if d]

def rank_snippets(query, pages, top_k=6):
    candidates=[]
    for p in pages:
        if p.get("snippet"):
            candidates.append({"text": p["snippet"], "url": p["url"], "title": p.get("title")})
        text = p.get("page_text","")
        paragraphs = [seg.strip() for seg in text.split("\n") if seg.strip()]
        for par in paragraphs[:6]:
            candidates.append({"text": par, "url": p["url"], "title": p.get("title")})
    if not candidates:
        return []
    texts=[c["text"] for c in candidates]
    q_emb = EMBED_MODEL.encode(query, convert_to_tensor=True)
    txt_embs = EMBED_MODEL.encode(texts, convert_to_tensor=True)
    sims = util.cos_sim(q_emb, txt_embs)[0].cpu().numpy()
    import numpy as np
    idxs = np.argsort(-sims)[:top_k]
    return [{**candidates[i], "score": float(sims[i])} for i in idxs]

async def multisearch_and_answer(query, use_local_model=True, top_k=6):
    async with aiohttp.ClientSession() as session:
        searches = await asyncio.gather(bing_search(session, query, 5), serp_search(session, query, 5))
        hits=[]
        seen=set()
        for r in searches:
            for it in r:
                if it["url"] in seen: continue
                seen.add(it["url"])
                hits.append(it)
                if len(hits)>=MAX_PAGES: break
            if len(hits)>=MAX_PAGES: break
        pages = await fetch_pages(session, hits)
    top = rank_snippets(query, pages, top_k=top_k)
    # build prompt
    context = "\n\n".join([f"[{i+1}] Source: {s['url']}\n{s['text']}" for i,s in enumerate(top)])
    prompt = f"You are Chanakya. Use the following sources to answer concisely and cite sources.\n\n{context}\n\nQuestion: {query}\nAnswer:"
    # call model (sync)
    if use_local_model:
        answer = generate_sync(prompt, max_new_tokens=300)
    else:
        # implement HF API call if you prefer
        raise RuntimeError("HF inference path not implemented")
    return {"answer": answer, "sources": [{"url": s["url"], "score": s["score"]} for s in top]}
from multisearch_rag import multisearch_and_answer
@app.post("/multisearch")
async def multisearch_handler(q: dict, user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    query = sanitize_user_input(q.get("query",""))
    if not moderate_text(query):
        raise HTTPException(400,"disallowed")
    out = await multisearch_and_answer(query, use_local_model=True)
    return out
# .github/workflows/docker-push.yml
name: Build and push to ECR
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      - name: Login to ECR
        run: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${{ secrets.ECR_REGISTRY }}
      - name: Build and push
        run: |
          docker build -t chanakya:latest .
          docker tag chanakya:latest ${{ secrets.ECR_REGISTRY }}/chanakya:latest
          docker push ${{ secrets.ECR_REGISTRY }}/chanakya:latest
docker compose up --build
# then POST to /multisearch and /voice/speak_offline
# Voice + search + infra
ELEVENLABS_API_KEY=           # optional (for high-quality TTS)
HF_API_TOKEN=                 # optional (if you use HF Inference)
BING_API_KEY=                 # optional (for web search)
SERPAPI_KEY=                  # optional (for web search)
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3
JWT_SECRET=your_strong_jwt_secret
REDIS_URL=redis://redis:6379/0
# tts_offline.py
import pyttsx3, tempfile, os
from pydub import AudioSegment

def speak_to_file_offline(text, out_path=None, rate=130, volume=1.0):
    engine = pyttsx3.init()
    # prefer a male/deep voice if present
    voices = engine.getProperty('voices')
    selected = None
    for v in voices:
        name = getattr(v, "name", "").lower()
        if "male" in name or "baritone" in name or "deep" in name:
            selected = v.id
            break
    if selected:
        engine.setProperty('voice', selected)
    engine.setProperty('rate', rate)
    engine.setProperty('volume', volume)
    if out_path is None:
        fd, out_path = tempfile.mkstemp(suffix=".wav")
        os.close(fd)
    engine.save_to_file(text, out_path)
    engine.runAndWait()
    return out_path
# tts_eleven.py
import os, requests, tempfile
ELEVEN_KEY = os.getenv("ELEVENLABS_API_KEY")

def speak_to_file_eleven(text, voice_id="alloy", out_path=None, stability=0.3, similarity_boost=0.2):
    if not ELEVEN_KEY:
        raise RuntimeError("ELEVENLABS_API_KEY not set")
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    headers = {"xi-api-key": ELEVEN_KEY, "Content-Type": "application/json"}
    payload = {"text": text, "voice_settings": {"stability": stability, "similarity_boost": similarity_boost}}
    r = requests.post(url, json=payload, headers=headers, stream=True, timeout=60)
    if r.status_code != 200:
        raise RuntimeError(f"ElevenLabs error {r.status_code}: {r.text}")
    if out_path is None:
        fd, out_path = tempfile.mkstemp(suffix=".mp3")
        os.close(fd)
    with open(out_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
    return out_path
# voice_api.py
from fastapi import APIRouter, HTTPException, Depends, Request, Response
import os
from tts_offline import speak_to_file_offline
from tts_eleven import speak_to_file_eleven
from auth_jwt import require_jwt
from rate_limiter import rate_limit_dependency
from logger import logger

router = APIRouter()

@router.post("/speak_offline")
async def speak_offline(payload: dict, user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    text = payload.get("text", "")
    if not text:
        raise HTTPException(400, "No text provided")
    path = speak_to_file_offline(text, rate=120)
    with open(path, "rb") as f:
        data = f.read()
    logger.info("Served offline TTS")
    return Response(content=data, media_type="audio/wav")

@router.post("/speak_eleven")
async def speak_eleven(payload: dict, user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    text = payload.get("text", "")
    if not text:
        raise HTTPException(400, "No text provided")
    voice = payload.get("voice_id", os.getenv("ELEVEN_DEFAULT_VOICE", "alloy"))
    try:
        path = speak_to_file_eleven(text, voice_id=voice)
    except Exception as e:
        raise HTTPException(500, str(e))
    with open(path, "rb") as f:
        data = f.read()
    logger.info("Served ElevenLabs TTS")
    return Response(content=data, media_type="audio/mpeg")
# multisearch_rag.py
import os, asyncio, aiohttp
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
from sanitize import sanitize_user_input
from safe_fetch import safe_fetch_page  # ensure safe_fetch.py exists
from model_loader import generate_sync
EMBED = SentenceTransformer("all-MiniLM-L6-v2")
MAX_PAGES = 8
CONCURRENT_FETCHES = 6

async def _bing(session, q, count=5):
    key = os.getenv("BING_API_KEY")
    if not key: return []
    url = "https://api.bing.microsoft.com/v7.0/search"
    headers = {"Ocp-Apim-Subscription-Key": key}
    params = {"q": q, "count": count}
    try:
        async with session.get(url, headers=headers, params=params, timeout=10) as r:
            if r.status != 200: return []
            j = await r.json()
            items = j.get("webPages", {}).get("value", [])
            return [{"title": it.get("name"), "url": it.get("url"), "snippet": it.get("snippet")} for it in items]
    except Exception:
        return []

async def _serp(session, q, count=5):
    key = os.getenv("SERPAPI_KEY")
    if not key: return []
    url = "https://serpapi.com/search.json"
    params = {"q": q, "api_key": key, "num": count}
    try:
        async with session.get(url, params=params, timeout=10) as r:
            if r.status != 200: return []
            j = await r.json()
            organic = j.get("organic_results", []) or j.get("results", [])
            out=[]
            for it in organic:
                out.append({"title": it.get("title"), "url": it.get("link") or it.get("url"), "snippet": it.get("snippet")})
            return out
    except Exception:
        return []

async def fetch_pages(session, hits):
    sem = asyncio.Semaphore(CONCURRENT_FETCHES)
    async def _f(h):
        async with sem:
            html = await safe_fetch_page(session, h["url"])
            if not html: return None
            soup = BeautifulSoup(html, "html.parser")
            for s in soup(["script","style","noscript","iframe"]): s.extract()
            text = " ".join(p.get_text() for p in soup.find_all("p"))
            text = sanitize_user_input(text)[:20000]
            h["page_text"] = text
            return h
    tasks = [asyncio.create_task(_f(h)) for h in hits]
    done = await asyncio.gather(*tasks)
    return [d for d in done if d]

def rank_snippets(query, pages, top_k=6):
    candidates=[]
    for p in pages:
        if p.get("snippet"):
            candidates.append({"text": p["snippet"], "url": p["url"], "title": p.get("title")})
        text = p.get("page_text","")
        paragraphs = [seg.strip() for seg in text.split("\n") if seg.strip()]
        for par in paragraphs[:6]:
            candidates.append({"text": par, "url": p["url"], "title": p.get("title")})
    if not candidates: return []
    texts=[c["text"] for c in candidates]
    q_emb = EMBED.encode(query, convert_to_tensor=True)
    txt_embs = EMBED.encode(texts, convert_to_tensor=True)
    sims = util.cos_sim(q_emb, txt_embs)[0].cpu().numpy()
    import numpy as np
    idxs = np.argsort(-sims)[:top_k]
    return [{**candidates[i], "score": float(sims[i])} for i in idxs]

async def multisearch_and_answer(query, use_local_model=True):
    query = sanitize_user_input(query)
    async with aiohttp.ClientSession() as session:
        bing_task = _bing(session, query, 5)
        serp_task = _serp(session, query, 5)
        results = await asyncio.gather(bing_task, serp_task)
        hits=[]
        seen=set()
        for r in results:
            for it in r:
                u = it.get("url")
                if not u or u in seen: continue
                seen.add(u)
                hits.append(it)
                if len(hits)>=MAX_PAGES: break
            if len(hits)>=MAX_PAGES: break
        pages = await fetch_pages(session, hits)
    top = rank_snippets(query, pages, top_k=6)
    context = "\n\n".join([f"[{i+1}] Source: {s['url']}\n{s['text']}" for i,s in enumerate(top)])
    prompt = f"You are Chanakya, answer concisely and cite sources.\n\n{context}\n\nQuestion: {query}\nAnswer:"
    if use_local_model:
        answer = generate_sync(prompt, max_new_tokens=300)
    else:
        raise RuntimeError("HF API path not implemented")
    return {"answer": answer, "sources": [{"url": s["url"], "score": s["score"]} for s in top]}
# multisearch_rag.py
import os, asyncio, aiohttp
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
from sanitize import sanitize_user_input
from safe_fetch import safe_fetch_page  # ensure safe_fetch.py exists
from model_loader import generate_sync
EMBED = SentenceTransformer("all-MiniLM-L6-v2")
MAX_PAGES = 8
CONCURRENT_FETCHES = 6

async def _bing(session, q, count=5):
    key = os.getenv("BING_API_KEY")
    if not key: return []
    url = "https://api.bing.microsoft.com/v7.0/search"
    headers = {"Ocp-Apim-Subscription-Key": key}
    params = {"q": q, "count": count}
    try:
        async with session.get(url, headers=headers, params=params, timeout=10) as r:
            if r.status != 200: return []
            j = await r.json()
            items = j.get("webPages", {}).get("value", [])
            return [{"title": it.get("name"), "url": it.get("url"), "snippet": it.get("snippet")} for it in items]
    except Exception:
        return []

async def _serp(session, q, count=5):
    key = os.getenv("SERPAPI_KEY")
    if not key: return []
    url = "https://serpapi.com/search.json"
    params = {"q": q, "api_key": key, "num": count}
    try:
        async with session.get(url, params=params, timeout=10) as r:
            if r.status != 200: return []
            j = await r.json()
            organic = j.get("organic_results", []) or j.get("results", [])
            out=[]
            for it in organic:
                out.append({"title": it.get("title"), "url": it.get("link") or it.get("url"), "snippet": it.get("snippet")})
            return out
    except Exception:
        return []

async def fetch_pages(session, hits):
    sem = asyncio.Semaphore(CONCURRENT_FETCHES)
    async def _f(h):
        async with sem:
            html = await safe_fetch_page(session, h["url"])
            if not html: return None
            soup = BeautifulSoup(html, "html.parser")
            for s in soup(["script","style","noscript","iframe"]): s.extract()
            text = " ".join(p.get_text() for p in soup.find_all("p"))
            text = sanitize_user_input(text)[:20000]
            h["page_text"] = text
            return h
    tasks = [asyncio.create_task(_f(h)) for h in hits]
    done = await asyncio.gather(*tasks)
    return [d for d in done if d]

def rank_snippets(query, pages, top_k=6):
    candidates=[]
    for p in pages:
        if p.get("snippet"):
            candidates.append({"text": p["snippet"], "url": p["url"], "title": p.get("title")})
        text = p.get("page_text","")
        paragraphs = [seg.strip() for seg in text.split("\n") if seg.strip()]
        for par in paragraphs[:6]:
            candidates.append({"text": par, "url": p["url"], "title": p.get("title")})
    if not candidates: return []
    texts=[c["text"] for c in candidates]
    q_emb = EMBED.encode(query, convert_to_tensor=True)
    txt_embs = EMBED.encode(texts, convert_to_tensor=True)
    sims = util.cos_sim(q_emb, txt_embs)[0].cpu().numpy()
    import numpy as np
    idxs = np.argsort(-sims)[:top_k]
    return [{**candidates[i], "score": float(sims[i])} for i in idxs]

async def multisearch_and_answer(query, use_local_model=True):
    query = sanitize_user_input(query)
    async with aiohttp.ClientSession() as session:
        bing_task = _bing(session, query, 5)
        serp_task = _serp(session, query, 5)
        results = await asyncio.gather(bing_task, serp_task)
        hits=[]
        seen=set()
        for r in results:
            for it in r:
                u = it.get("url")
                if not u or u in seen: continue
                seen.add(u)
                hits.append(it)
                if len(hits)>=MAX_PAGES: break
            if len(hits)>=MAX_PAGES: break
        pages = await fetch_pages(session, hits)
    top = rank_snippets(query, pages, top_k=6)
    context = "\n\n".join([f"[{i+1}] Source: {s['url']}\n{s['text']}" for i,s in enumerate(top)])
    prompt = f"You are Chanakya, answer concisely and cite sources.\n\n{context}\n\nQuestion: {query}\nAnswer:"
    if use_local_model:
        answer = generate_sync(prompt, max_new_tokens=300)
    else:
        raise RuntimeError("HF API path not implemented")
    return {"answer": answer, "sources": [{"url": s["url"], "score": s["score"]} for s in top]}
from voice_api import router as voice_router
from multisearch_rag import multisearch_and_answer
app.include_router(voice_router, prefix="/voice")
from fastapi import Body
@app.post("/multisearch")
async def multisearch_handler(body: dict = Body(...), user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    query = sanitize_user_input(body.get("query",""))
    if not moderate_text(query):
        raise HTTPException(400, "disallowed")
    out = await multisearch_and_answer(query, use_local_model=True)
    return out
{
  "name": "chanakya-webui",
  "version": "1.0.0",
  "private": true,
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "axios": "^1.4.0"
  },
  "scripts": {
    "start": "parcel index.html --port 3000",
    "build": "parcel build index.html --public-url ./"
  },
  "devDependencies": {
    "parcel": "^2.9.3"
  }
}
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Chanakya AI</title>
  </head>
  <body>
    <div id="root"></div>
    <script src="./src/App.jsx"></script>
  </body>
</html>
import React, {useState} from "react";
import axios from "axios";

export default function App(){
  const [q, setQ] = useState("");
  const [answer, setAnswer] = useState("");
  const [sources, setSources] = useState([]);
  const [audioUrl, setAudioUrl] = useState(null);

  // set your JWT token here for testing
  const JWT = localStorage.getItem("JWT") || "";

  async function ask(){
    setAnswer("Thinking...");
    try{
      const res = await axios.post("http://localhost:8000/multisearch", { query: q }, {
        headers: { Authorization: `Bearer ${JWT}` }
      });
      setAnswer(res.data.answer || res.data.response || res.data);
      setSources(res.data.sources || []);
      // request TTS (ElevenLabs if configured)
      const tts = await axios.post("http://localhost:8000/voice/speak_eleven", { text: res.data.answer || res.data.response }, {
        headers: { Authorization: `Bearer ${JWT}` },
        responseType: "blob"
      });
      const blob = new Blob([tts.data], { type: "audio/mpeg" });
      const url = URL.createObjectURL(blob);
      setAudioUrl(url);
      const audio = new Audio(url);
      audio.play();
    }catch(e){
      setAnswer("Error: " + (e.response?.data || e.message));
    }
  }

  return (
    <div style={{maxWidth:800, margin:"2rem auto", fontFamily:"Arial"}}>
      <h1>Chanakya AI</h1>
      <textarea value={q} onChange={e=>setQ(e.target.value)} rows={4} style={{width:"100%"}}/>
      <div style={{marginTop:10}}>
        <button onClick={ask}>Ask Chanakya</button>
      </div>
      <div style={{marginTop:20}}>
        <h3>Answer</h3>
        <div style={{whiteSpace:"pre-wrap"}}>{answer}</div>
      </div>
      <div>
        <h4>Sources</h4>
        <ul>{sources.map((s,i)=> <li key={i}><a href={s.url} target="_blank" rel="noreferrer">{s.url}</a> ({s.score?.toFixed(3)})</li>)}</ul>
      </div>
      {audioUrl && <div><h4>Audio</h4><audio controls src={audioUrl}></audio></div>}
    </div>
  );
}
npm install
npm run start
name: Build & Push Docker
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USER }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: ${{ secrets.DOCKERHUB_USER }}/chanakya:latest
cd chanakya-docker
docker compose up --build
cd chanakya-docker/webui
npm install
npm run start
from auth_jwt import create_access_token
print(create_access_token("devuser"))
# copy token into browser console:
# localStorage.setItem("JWT","<token>")
curl -X POST http://localhost:8000/multisearch -H "Authorization: Bearer <token>" -H "Content-Type: application/json" -d '{"query":"What is Chanakya Niti on leadership?"}'
# Chanakya AI â€” Docker Compose Repo (ready-to-run)

This document contains a complete, copy-pasteable repo layout for running **Chanakya AI** locally using Docker Compose. Files included below: `Dockerfile`, `docker-compose.yml`, `nginx.conf`, `.env.example`, `requirements.txt`, `main_secure.py`, `auth_jwt.py`, `model_loader.py`, `rag_engine.py`, `celery_app.py`, `tasks.py`, and a short `README` with commands.

> Paste each file into your project directory (`chanakya-docker/`) with the exact filename shown and run `docker compose up --build`.

---

## README.md

````markdown
# Chanakya AI â€” Docker Compose Example

## Quick start
1. Copy files into a folder `chanakya-docker/`.
2. Create `.env` from `.env.example` and fill secrets.
3. Build and run:

```bash
docker compose up --build
````

4. The API will be available at `http://localhost:8000` (or behind Nginx at port 443 if you configure TLS).

## Notes

* Keep `.env` secret. Do not commit.
* For GPU inference, run model service on a machine with NVIDIA drivers and enable Docker GPU runtime.
* This setup is for local dev & testing. For production, follow cloud deployment best practices.

```
```

---

## .env.example

```dotenv
# Rename to .env and fill values
JWT_SECRET=replace_this_with_a_strong_secret
ACCESS_EXPIRE_MINUTES=60
API_KEY=replace_api_key_for_clients
REDIS_URL=redis://redis:6379/0
DATABASE_URL=postgresql://postgres:postgres@db:5432/chanakya
SENTRY_DSN=
BING_API_KEY=
SERPAPI_KEY=
HF_API_TOKEN=
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3
```

---

## requirements.txt

```text
fastapi
uvicorn[standard]
transformers
torch
accelerate
sentence-transformers
faiss-cpu
python-dotenv
pyjwt
passlib[bcrypt]
aioredis
fastapi-limiter
prometheus-client
sentry-sdk
celery[redis]
psycopg2-binary
aiohttp
beautifulsoup4
chromadb
pydantic
```

---

## Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# system deps for some Python packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    git \
    curl \
    libgl1 \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt ./
RUN pip install --upgrade pip
RUN pip install -r requirements.txt

COPY . /app
ENV PYTHONUNBUFFERED=1

CMD ["gunicorn", "main_secure:app", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--workers", "1", "--threads", "4", "--timeout", "120"]
```

---

## docker-compose.yml

```yaml
version: "3.8"
services:
  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET=${JWT_SECRET}
      - ACCESS_EXPIRE_MINUTES=${ACCESS_EXPIRE_MINUTES}
      - SENTRY_DSN=${SENTRY_DSN}
      - HF_API_TOKEN=${HF_API_TOKEN}
      - MODEL_NAME=${MODEL_NAME}
    depends_on:
      - redis
      - db
    volumes:
      - ./data:/app/data

  redis:
    image: redis:7
    restart: unless-stopped
    volumes:
      - redis_data:/data

  db:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: chanakya
    volumes:
      - pgdata:/var/lib/postgresql/data

  celery:
    build: .
    command: celery -A celery_app.celery worker --loglevel=info
    depends_on:
      - redis
      - db
    environment:
      - REDIS_URL=${REDIS_URL}

volumes:
  redis_data:
  pgdata:
```

---

## nginx.conf

```nginx
server {
    listen 80;
    server_name _;

    location / {
        proxy_pass http://web:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

---

## main_secure.py

```python
# main_secure.py
import os
import time
import asyncio
from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from auth_jwt import require_jwt
from rate_limiter import rate_limit_dependency
from sanitize import sanitize_user_input, detect_prompt_injection
from moderation import moderate_text
from rag_engine import retrieve
from model_loader import tokenizer, model
from verify import answer_confidence
from logger import logger

load_dotenv()

app = FastAPI(title="Chanakya AI - Secure")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    user_input: str
    k: int = 4

@app.post('/chat')
async def chat(req: ChatRequest, user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    # basic moderation
    if not moderate_text(req.user_input):
        raise HTTPException(status_code=400, detail='Request disallowed by policy')

    # sanitize
    clean = sanitize_user_input(req.user_input)
    if detect_prompt_injection(req.user_input):
        raise HTTPException(status_code=400, detail='Prompt looks malicious - rejected')

    docs = retrieve(clean, k=req.k)
    prompt = build_prompt = (
        f"You are Chanakya, an expert assistant. Use the context to answer concisely.\n\nContext:\n{chr(10).join(docs)}\n\nQuestion: {clean}\nAnswer:"
    )

    # run model generation in threadpool
    loop = asyncio.get_event_loop()
    answer = await loop.run_in_executor(None, generate_sync, prompt)

    conf = answer_confidence(answer, [{'text': d} for d in docs])
    logger.info(f"Answer confidence={conf:.3f}")
    if conf < 0.28:
        return {"answer": "I couldn't find reliable evidence to answer confidently. Here are the sources I checked.", "sources": docs}

    return {"answer": answer, "sources": docs}

# Minimal health endpoint
@app.get('/healthz')
def health():
    return {'status': 'ok'}

# Metrics endpoint example
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
@app.get('/metrics')
def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

---

## auth_jwt.py

```python
# auth_jwt.py
import os
from datetime import datetime, timedelta
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt

JWT_SECRET = os.getenv('JWT_SECRET', 'change_me')
JWT_ALG = 'HS256'
ACCESS_EXPIRE_MINUTES = int(os.getenv('ACCESS_EXPIRE_MINUTES', '60'))
security = HTTPBearer()

def create_access_token(sub: str):
    now = datetime.utcnow()
    payload = {'sub': sub, 'iat': now, 'exp': now + timedelta(minutes=ACCESS_EXPIRE_MINUTES)}
    return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALG)

def decode_token(token: str):
    try:
        return jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Token expired')
    except Exception:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Invalid token')

async def require_jwt(creds: HTTPAuthorizationCredentials = Depends(security)):
    token = creds.credentials
    payload = decode_token(token)
    return payload
```

---

## model_loader.py

```python
# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')

def load_model():
    print('Loading model:', MODEL_NAME)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
    model.eval()
    return tokenizer, model

try:
    tokenizer, model = load_model()
except Exception as e:
    print('Model load error - ensure you have proper hardware and internet to download the model')
    tokenizer, model = None, None

# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
    if tokenizer is None or model is None:
        return 'Model not loaded'
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
    with torch.inference_mode():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    return tokenizer.decode(out[0], skip_special_tokens=True)
```

---

## rag_engine.py

```python
# rag_engine.py
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

embedder = SentenceTransformer('all-MiniLM-L6-v2')
_docs = []
_embs = None

def add_document(text: str):
    global _docs, _embs
    _docs.append(text)
    vec = embedder.encode([text], convert_to_numpy=True)
    _embs = vec if _embs is None else np.vstack((_embs, vec))

def retrieve(query: str, k: int = 3):
    if _embs is None or len(_docs) == 0:
        return []
    q = embedder.encode([query], convert_to_numpy=True)
    dim = _embs.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(_embs)
    D, I = index.search(q, k)
    return [_docs[i] for i in I[0] if i < len(_docs)]
```

---

## celery_app.py

```python
# celery_app.py
from celery import Celery
import os
redis_url = os.getenv('REDIS_URL', 'redis://redis:6379/0')
celery = Celery('chanakya', broker=redis_url, backend=redis_url)
```

---

## tasks.py

```python
# tasks.py
from celery_app import celery
from rag_engine import add_document

@celery.task
def ingest_document_task(text, meta=None):
    # lightweight PII scrub could be added here
    add_document(text)
    return True
```

---

## logger.py

```python
# logger.py
import logging
logger = logging.getLogger('chanakya')
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)
```

---

## sanitize.py, rate_limiter.py, moderation.py, verify.py

Include the versions of these modules from the earlier security audit. They are required for safety. (Paste the `sanitize.py`, `rate_limiter.py`, `moderation.py`, and `verify.py` code blocks from the security section into separate files.)

---

## Final notes

* After placing files, create `.env` from `.env.example` and set values.
* Run `docker compose up --build` to start all services.
* If you get GPU/model download issues, first try running `model_loader.py` locally and adjust `MODEL_NAME` or use a hosted HF inference endpoint.

Good luck â€” your Docker Compose repo is ready. If you want, I can now:

* provide the missing `sanitize.py`, `rate_limiter.py`, `moderation.py`, and `verify.py` files fully pasted into this repo doc, or
* give you a step-by-step terminal walkthrough to run the repo on Windows or Linux.

---

## safe_fetch.py

```python
# safe_fetch.py
import aiohttp
import re
from urllib.parse import urlparse
from aiohttp import ClientTimeout
from config import MAX_PAGE_FETCH_SIZE_BYTES, ALLOWED_DOMAINS

_robots_cache = {}

async def fetch_robots_txt(session, domain):
    if domain in _robots_cache:
        return _robots_cache[domain]
    url = f"https://{domain}/robots.txt"
    try:
        async with session.get(url, timeout=ClientTimeout(total=5)) as resp:
            if resp.status != 200:
                _robots_cache[domain] = []
                return []
            txt = await resp.text()
            disallowed = []
            ua_all = False
            for line in txt.splitlines():
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if line.lower().startswith('user-agent:'):
                    ua = line.split(':',1)[1].strip()
                    ua_all = (ua == '*' or ua.lower() == '*')
                elif ua_all and line.lower().startswith('disallow:'):
                    path = line.split(':',1)[1].strip()
                    if path:
                        disallowed.append(path)
            _robots_cache[domain] = disallowed
            return disallowed
    except Exception:
        _robots_cache[domain] = []
        return []

async def allowed_by_robots(session, url):
    p = urlparse(url)
    domain = p.netloc
    if ALLOWED_DOMAINS is not None and domain.lower() not in ALLOWED_DOMAINS:
        return False
    disallowed = await fetch_robots_txt(session, domain)
    for d in disallowed:
        if p.path.startswith(d):
            return False
    return True

async def safe_fetch_page(session, url, max_bytes=MAX_PAGE_FETCH_SIZE_BYTES):
    try:
        p = urlparse(url)
        if p.scheme not in ("http", "https"):
            return ''
        if not await allowed_by_robots(session, url):
            return ''
        headers = {
            "User-Agent": "ChanakyaBot/1.0 (+https://example.com) - research crawler"
        }
        async with session.get(url, headers=headers, timeout=ClientTimeout(total=12), allow_redirects=True) as resp:
            if resp.status != 200:
                return ''
            ct = resp.headers.get('Content-Type','')
            if 'text/html' not in ct:
                return ''
            size = 0
            chunks = []
            async for chunk in resp.content.iter_chunked(1024):
                size += len(chunk)
                if size > max_bytes:
                    break
                chunks.append(chunk)
            html = b"".join(chunks).decode(errors='ignore')
            # basic sanitization: remove scripts/styles
            html = re.sub(r'(?s)<(script|style).*?>.*?</\1>', '', html, flags=re.I)
            return html
    except Exception:
        return ''
```

---

## Added helper note

I added `safe_fetch.py` to the repo in the canvas. This file is required by `multisearch_rag.py` to fetch pages safely while respecting `robots.txt`, allowed domains, and size limits.

### Next steps (already done in repo):

* Ensure your `.env` has `ALLOWED_DOMAINS` if you want to restrict crawling to specific hosts.
* Rebuild Docker and run:

```bash
docker compose up --build
```

* Test `/multisearch` and `/voice` endpoints via the web UI or curl.

If you want, I can now: (1) add a ZIP bundle of the repo files, (2) paste any final missing small helpers, or (3) walk you through running the Docker compose step-by-step for your OS. Which do you want next?
# Rename to .env and fill values
JWT_SECRET=replace_this_with_a_strong_secret
ACCESS_EXPIRE_MINUTES=60
API_KEY=replace_api_key_for_clients
REDIS_URL=redis://redis:6379/0
DATABASE_URL=postgresql://postgres:postgres@db:5432/chanakya
SENTRY_DSN=
BING_API_KEY=
SERPAPI_KEY=
HF_API_TOKEN=
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3fastapi
uvicorn[standard]
transformers
torch
accelerate
sentence-transformers
faiss-cpu
python-dotenv
pyjwt
passlib[bcrypt]
aioredis
fastapi-limiter
prometheus-client
sentry-sdk
celery[redis]
psycopg2-binary
aiohttp
beautifulsoup4
chromadb
pydanticWORKDIR /app


# system deps for some Python packages
RUN apt-get update && apt-get install -y --no-install-recommends \
build-essential \
git \
curl \
libgl1 \
libsndfile1 \
&& rm -rf /var/lib/apt/lists/*


COPY requirements.txt ./
RUN pip install --upgrade pip
RUN pip install -r requirements.txt


COPY . /app
ENV PYTHONUNBUFFERED=1


CMD ["gunicorn", "main_secure:app", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--workers", "1", "--threads", "4", "--timeout", "120"]version: "3.8"
services:
web:
build: .
ports:
- "8000:8000"
environment:
- DATABASE_URL=${DATABASE_URL}
- REDIS_URL=${REDIS_URL}
- JWT_SECRET=${JWT_SECRET}
- ACCESS_EXPIRE_MINUTES=${ACCESS_EXPIRE_MINUTES}
- SENTRY_DSN=${SENTRY_DSN}
- HF_API_TOKEN=${HF_API_TOKEN}
- MODEL_NAME=${MODEL_NAME}
depends_on:
- redis
- db
volumes:
- ./data:/app/data


redis:
image: redis:7
restart: unless-stopped
volumes:
- redis_data:/data


db:
image: postgres:15
environment:
POSTGRES_PASSWORD: postgres
POSTGRES_USER: postgres
POSTGRES_DB: chanakya
volumes:
- pgdata:/var/lib/postgresql/data


celery:
build: .
command: celery -A celery_app.celery worker --loglevel=info
depends_on:
- redis
- db
environment:
- REDIS_URL=${REDIS_URL}


volumes:
redis_data:
pgdata:server {
listen 80;
server_name _;


location / {
proxy_pass http://web:8000;
proxy_set_header Host $host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_set_header X-Forwarded-Proto $scheme;
}
}# main_secure.py
import os
import time
import asyncio
from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from auth_jwt import require_jwt
from rate_limiter import rate_limit_dependency
from sanitize import sanitize_user_input, detect_prompt_injection
from moderation import moderate_text
from rag_engine import retrieve
from model_loader import tokenizer, model
from verify import answer_confidence
from logger import logger


load_dotenv()


app = FastAPI(title="Chanakya AI - Secure")


app.add_middleware(
CORSMiddleware,
allow_origins=["http://localhost:3000"],
allow_credentials=True,
allow_methods=["*"],
allow_headers=["*"],
)


class ChatRequest(BaseModel):
user_input: str
k: int = 4


@app.post('/chat')
async def chat(req: ChatRequest, user=Depends(require_jwt), _=Depends(rate_limit_dependency)):
# basic moderation
if not moderate_text(req.user_input):
raise HTTPException(status_code=400, detail='Request disallowed by policy')


# sanitize
clean = sanitize_user_input(req.user_input)
if detect_prompt_injection(req.user_input):
raise HTTPException(status_code=400, detail='Prompt looks malicious - rejected')


docs = retrieve(clean, k=req.k)
prompt = build_prompt = (
f"You are Chanakya, an expert assistant. Use the context to answer concisely.\n\nContext:\n{chr(10).join(docs)}\n\nQuestion: {clean}\nAnswer:"
)


# run model generation in threadpool
loop = asyncio.get_event_loop()
answer = await loop.run_in_executor(None, generate_sync, prompt)


conf = answer_confidence(answer, [{'text': d} for d in docs])
logger.info(f"Answer confidence={conf:.3f}")
if conf < 0.28:
return {"answer": "I couldn't find reliable evidence to answer confidently. Here are the sources I checked.", "sources": docs}


return {"answer": answer, "sources": docs}


# Minimal health endpoint
@app.get('/healthz')
def health():
return {'status': 'ok'}


# Metrics endpoint example
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
@app.get('/metrics')
def metrics():
return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)# auth_jwt.py
import os
from datetime import datetime, timedelta
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt


JWT_SECRET = os.getenv('JWT_SECRET', 'change_me')
JWT_ALG = 'HS256'
ACCESS_EXPIRE_MINUTES = int(os.getenv('ACCESS_EXPIRE_MINUTES', '60'))
security = HTTPBearer()


def create_access_token(sub: str):
now = datetime.utcnow()
payload = {'sub': sub, 'iat': now, 'exp': now + timedelta(minutes=ACCESS_EXPIRE_MINUTES)}
return jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALG)


def decode_token(token: str):
try:
return jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALG])
except jwt.ExpiredSignatureError:
raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Token expired')
except Exception:
raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail='Invalid token')


async def require_jwt(creds: HTTPAuthorizationCredentials = Depends(security)):
token = creds.credentials
payload = decode_token(token)
return payload# model_loader.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os


MODEL_NAME = os.getenv('MODEL_NAME', 'TheBloke/Mistral-7B-Instruct-GPTQ')


def load_model():
print('Loading model:', MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.eval()
return tokenizer, model


try:
tokenizer, model = load_model()
except Exception as e:
print('Model load error - ensure you have proper hardware and internet to download the model')
tokenizer, model = None, None


# simple sync generate helper (used in main_secure)
def generate_sync(prompt: str, max_new_tokens: int = 200):
if tokenizer is None or model is None:
return 'Model not loaded'
inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048).to(model.device)
with torch.inference_mode():
out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
return tokenizer.decode(out[0], skip_special_tokens=True)# rag_engine.py
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss


embedder = SentenceTransformer('all-MiniLM-L6-v2')
_docs = []
_embs = None


def add_document(text: str):
global _docs, _embs
_docs.append(text)
vec = embedder.encode([text], convert_to_numpy=True)
_embs = vec if _embs is None else np.vstack((_embs, vec))


def retrieve(query: str, k: int = 3):
if _embs is None or len(_docs) == 0:
return []
q = embedder.encode([query], convert_to_numpy=True)
dim = _embs.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(_embs)
D, I = index.search(q, k)
return [_docs[i] for i in I[0] if i < len(_docs)]# celery_app.py
from celery import Celery
import os
redis_url = os.getenv('REDIS_URL', 'redis://redis:6379/0')
celery = Celery('chanakya', broker=redis_url, backend=redis_url)# tasks.py
from celery_app import celery
from rag_engine import add_document


@celery.task
def ingest_document_task(text, meta=None):
# lightweight PII scrub could be added here
add_document(text)
return True# logger.py
import logging
logger = logging.getLogger('chanakya')
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)# Windows PowerShell
Compress-Archive -Path * -DestinationPath ChanakyaAI.zip

# Linux/macOS
zip -r ChanakyaAI.zip .
# __init__.py
# Chanakya AI - Core Package
# utils.py
import datetime

def log_event(message: str):
    """Logs important events with timestamps."""
    print(f"[{datetime.datetime.now().isoformat()}] {message}")

def format_response(text: str):
    """Formats model output to be readable."""
    text = text.replace("\n\n", "\n")
    return text.strip()
cd path\to\ChanakyaAI
docker compose up --build
Chanakya AI running at http://localhost:8000
curl -X POST http://localhost:8000/chat \
-H "Content-Type: application/json" \
-d '{"user_input": "Explain Chanakya Neeti on strategy."}'
curl -X POST http://localhost:8000/speak \
-H "Content-Type: application/json" \
-d '{"text": "Victory comes not by chance but by design."}' \
--output voice.mp3
curl -X POST http://localhost:8000/multisearch \
-H "Content-Type: application/json" \
-d '{"query": "Chanakya Neeti lessons for leadership"}'
// webui/src/App.jsx
import React, {useState, useRef} from "react";
import axios from "axios";

export default function App(){
  const [query, setQuery] = useState("");
  const [answer, setAnswer] = useState("");
  const [sources, setSources] = useState([]);
  const [loading, setLoading] = useState(false);
  const [audioUrl, setAudioUrl] = useState(null);
  const audioRef = useRef(null);

  const JWT = localStorage.getItem("JWT") || "";

  async function ask() {
    setLoading(true);
    setAnswer("");
    setSources([]);
    setAudioUrl(null);
    try {
      const res = await axios.post("http://localhost:8000/multisearch",
        { query },
        { headers: { Authorization: `Bearer ${JWT}` } }
      );
      const text = res.data.answer || res.data.response || "";
      setAnswer(text);
      setSources(res.data.sources || []);

      // Request TTS (prefer ElevenLabs if configured)
      const tts = await axios.post("http://localhost:8000/voice/speak_eleven",
        { text },
        { headers: { Authorization: `Bearer ${JWT}` }, responseType: "blob" }
      );
      const blob = new Blob([tts.data], { type: "audio/mpeg" });
      const url = URL.createObjectURL(blob);
      setAudioUrl(url);
      setTimeout(()=> {
        if(audioRef.current) audioRef.current.play();
      }, 100);
    } catch (e) {
      console.error(e);
      setAnswer("Error: " + (e.response?.data || e.message));
    } finally {
      setLoading(false);
    }
  }

  // Microphone recording -> uses MediaRecorder to capture audio and send to backend STT if implemented
  async function recordAndTranscribe() {
    if(!navigator.mediaDevices) {
      alert("Microphone not supported.");
      return;
    }
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      const chunks = [];
      mediaRecorder.ondataavailable = e => chunks.push(e.data);
      mediaRecorder.onstop = async () => {
        const blob = new Blob(chunks, { type: "audio/webm" });
        // If you have a backend STT endpoint (whisper) -> POST blob to /stt
        // Example:
        // const form = new FormData(); form.append("file", blob, "rec.webm");
        // const r = await axios.post("http://localhost:8000/stt", form, { headers: { Authorization: `Bearer ${JWT}`, "Content-Type": "multipart/form-data" }});
        // setQuery(r.data.text);
        // fallback: try browser speech recognition:
        alert("Recorded (client). If you want automatic transcription, enable backend STT endpoint or paste text.");
      };
      mediaRecorder.start();
      setTimeout(() => { mediaRecorder.stop(); stream.getTracks().forEach(t=>t.stop()); }, 3000); // record 3s
    } catch(err) {
      alert("Microphone access denied or error: " + err.message);
    }
  }

  return (
    <div style={{ maxWidth:900, margin:"2rem auto", fontFamily:"Inter, Arial", padding:20 }}>
      <h1>Chanakya AI</h1>
      <textarea value={query} onChange={e=>setQuery(e.target.value)} rows={4} style={{width:"100%", fontSize:16}} placeholder="Ask Chanakya..."/>
      <div style={{marginTop:8}}>
        <button onClick={ask} disabled={loading || !query.trim()}>Ask</button>
        <button onClick={recordAndTranscribe} style={{marginLeft:8}}>Record (3s)</button>
      </div>

      <div style={{marginTop:20}}>
        <h3>Answer</h3>
        <div style={{whiteSpace:"pre-wrap", background:"#fafafa", padding:12, borderRadius:6}}>{ answer || (loading ? "Thinking..." : "No answer yet") }</div>
      </div>

      <div>
        <h4>Sources</h4>
        <ul>{sources.map((s,i)=> <li key={i}><a href={s.url} target="_blank" rel="noreferrer">{s.url}</a> {s.score ? `(${s.score.toFixed(3)})` : ""}</li>)}</ul>
      </div>

      { audioUrl && (
        <div style={{marginTop:12}}>
          <h4>Audio</h4>
          <audio ref={audioRef} controls src={audioUrl} />
        </div>
      )}
    </div>
  );
}
sudo apt-get update
sudo apt-get install -y docker.io
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
# hf_infer.py
import os, requests
HF_TOKEN = os.getenv("HF_API_TOKEN")
MODEL = os.getenv("HF_MODEL", "mistralai/Mistral-7B-Instruct-v0.3")  # or any hosted model

def hf_generate(prompt, max_new_tokens=256):
    url = f"https://api-inference.huggingface.co/models/{MODEL}"
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    data = {"inputs": prompt, "parameters": {"max_new_tokens": max_new_tokens, "return_full_text": False}}
    r = requests.post(url, headers=headers, json=data, timeout=60)
    r.raise_for_status()
    out = r.json()
    if isinstance(out, list) and len(out)>0 and "generated_text" in out[0]:
        return out[0]["generated_text"]
    if isinstance(out, dict) and "generated_text" in out:
        return out["generated_text"]
    return str(out)
# inside auth_jwt.py (add)
from datetime import datetime, timedelta
import os, secrets
ACCESS_EXPIRE_MINUTES = int(os.getenv("ACCESS_EXPIRE_MINUTES", "15"))
REFRESH_EXPIRE_DAYS = int(os.getenv("REFRESH_EXPIRE_DAYS", "30"))

def create_refresh_token(sub: str):
    # create opaque refresh token and store server-side (DB or Redis) with expiry
    token = secrets.token_urlsafe(32)
    expiry = datetime.utcnow() + timedelta(days=REFRESH_EXPIRE_DAYS)
    # store token->sub in DB or Redis with expiry. Example using redis:
    # redis_client.setex(f"refresh:{token}", REFRESH_EXPIRE_DAYS*24*3600, sub)
    return token

def rotate_tokens(refresh_token: str):
    # validate refresh token from store, then issue new access + new refresh token, delete old refresh token
    sub = redis_client.get(f"refresh:{refresh_token}")
    if not sub:
        raise HTTPException(401, "Invalid refresh")
    # delete old
    redis_client.delete(f"refresh:{refresh_token}")
    new_access = create_access_token(sub)
    new_refresh = create_refresh_token(sub)
    return {"access_token": new_access, "refresh_token": new_refresh}
services:
  web:
    secrets:
      - jwt_secret
secrets:
  jwt_secret:
    file: ./secrets/jwt_secret.txt
sudo apt install nginx certbot python3-certbot-nginx
sudo certbot --nginx -d yourdomain.com -d www.yourdomain.com
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chanakya AI</title>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-950 text-white flex flex-col items-center justify-center min-h-screen p-6">
  <h1 class="text-4xl font-bold mb-4 text-yellow-400">ðŸ§  Chanakya AI</h1>
  <textarea id="input" class="w-full md:w-1/2 h-24 p-4 bg-gray-800 rounded-lg mb-4" placeholder="Ask Chanakya..."></textarea>
  <div class="flex gap-4">
    <button id="ask" class="bg-yellow-500 hover:bg-yellow-600 px-6 py-2 rounded-lg font-semibold">Ask</button>
    <button id="speak" class="bg-green-500 hover:bg-green-600 px-6 py-2 rounded-lg font-semibold">Speak</button>
  </div>
  <div id="output" class="mt-6 w-full md:w-1/2 text-lg text-gray-300"></div>

  <script>
    async function askChanakya() {
      const input = document.getElementById('input').value;
      const res = await fetch('http://localhost:8000/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ user_input: input })
      });
      const data = await res.json();
      docume
docker compose up --build
https://chanakya-ai.onrender.com
# auth.py
import time, jwt
SECRET_KEY = "CHANAKYA_SECRET_KEY"  # use a strong, private one

def create_token(user_id: str):
    payload = {"user_id": user_id, "exp": int(time.time()) + 3600}
    return jwt.encode(payload, SECRET_KEY, algorithm="HS256")

def verify_token(token: str):
    try:
        jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        return True
    except jwt.ExpiredSignatureError:
        return False
    except Exception:
        return False
from auth import verify_token
from fastapi import Header, HTTPException

@app.post("/chat")
async def chat(user_input: dict, token: str = Header(None)):
    if not verify_token(token):
        raise HTTPException(status_code=401, detail="Invalid or expired token.")
    ...
# memory.py
import json
from pathlib import Path

MEMORY_FILE = Path("chanakya_memory.json")

def load_memory():
    if MEMORY_FILE.exists():
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"history": []}

def save_memory(memory):
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(memory, f, indent=2, ensure_ascii=False)

def remember(user_input, ai_response):
    memory = load_memory()
    memory["history"].append({
        "user": user_input,
        "chanakya": ai_response
    })
    if len(memory["history"]) > 20:  # Keep only recent 20 messages
        memory["history"] = memory["history"][-20:]
    save_memory(memory)

def recall():
    memory = load_memory()
    history_text = "\n".join(
        [f"User: {m['user']}\nChanakya: {m['chanakya']}" for m in memory["history]()]()
# personality.py
CHANAKYA_PERSONALITY = {
    "name": "Acharya Chanakya",
    "tone": "wise, deep, patient, strategic",
    "values": [
        "truth", "discipline", "strategy", "dharma", "knowledge", "purpose"
    ],
    "speech_style": (
        "Speaks in calm and direct sentences. Uses analogies and principles from Chanakya Neeti. "
        "Prefers short, powerful teachings rather than long paragraphs."
    ),
    "greeting": "Namaste. I am Chanakya â€” here to guide you with ancient wisdom.",
    "farewell": "Remember â€” power without purpose is destruction. Farewell."
}
from memory import remember, recall
from personality import CHANAKYA_PERSONALITY
@app.post("/chat")
async def chat(request: dict, token: str = Header(None)):
    if not verify_token(token):
        raise HTTPException(status_code=401, detail="Invalid or expired token.")
    user_input = request["user_input"]

    past_context = recall()
    persona = f"You are {CHANAKYA_PERSONALITY['name']}, a {CHANAKYA_PERSONALITY['tone']} teacher. {CHANAKYA_PERSONALITY['speech_style']}"
    prompt = f"{persona}\n\nMemory:\n{past_context}\n\nUser: {user_input}\nChanakya:"

    # send to your AI model here
    ai_response = generate_response(prompt)
    remember(user_input, ai_response)

    return {"response": ai_response}
def clear_memory():
    save_memory({"history": []})
# emotion.py
import re
from typing import Tuple

# small lexicon for fast inference (expand as needed)
POSITIVE = {"happy","good","great","fantastic","awesome","thanks","thank you","love","glad"}
NEGATIVE = {"sad","angry","upset","bad","frustrated","hate","disappointed","annoyed","worried"}
ANXIOUS = {"nervous","anxious","scared","fear","panic","afraid"}

def detect_emotion(text: str) -> Tuple[str, float]:
    """
    Returns (emotion_label, confidence_score)
    Labels: neutral, positive, negative, anxious
    """
    t = text.lower()
    # simple lexical match
    pos = sum(1 for w in POSITIVE if w in t)
    neg = sum(1 for w in NEGATIVE if w in t)
    anx = sum(1 for w in ANXIOUS if w in t)
    total = pos + neg + anx
    if total == 0:
        return "neutral", 0.5
    if pos >= neg and pos >= anx:
        return "positive", min(0.9, 0.5 + pos / (total + 1))
    if anx >= neg and anx >= pos:
        return "anxious", min(0.9, 0.5 + anx / (total + 1))
    return "negative", min(0.9, 0.5 + neg / (total + 1))

def tone_for_emotion(emotion: str) -> dict:
    """
    Maps emotion to personality tuning adjustments.
    Example usage: prepend to system prompt or adjust speech rate/volume.
    """
    if emotion == "positive":
        return {"style": "encouraging and warm", "rate": 150, "volume": 1.0}
    if emotion == "anxious":
        return {"style": "calm, reassuring, slow", "rate": 120, "volume": 0.9}
    if emotion == "negative":
        return {"style": "firm but empathetic", "rate": 130, "volume": 1.0}
    return {"style": "calm and wise", "rate": 140, "volume": 1.0}
from emotion import detect_emotion, tone_for_emotion

# inside your chat endpoint before building prompt:
emotion, e_conf = detect_emotion(clean)
tone_settings = tone_for_emotion(emotion)
persona_prefix = f"You are Chanakya, respond in a {tone_settings['style']} tone."
prompt = persona_prefix + "\n\nMemory:\n" + recall() + "\n\nUser: " + clean + "\nChanakya:"
# learning.py
import json
from pathlib import Path
from datetime import datetime
from sanitize import sanitize_user_input
from verify import answer_confidence

FEEDBACK_DIR = Path("feedback")
FEEDBACK_DIR.mkdir(exist_ok=True)

def save_feedback(user_id: str, user_input: str, model_answer: str, rating: int, correction: str = None, sources=None):
    """
    rating: 1 (bad) to 5 (excellent)
    correction: optional corrected answer by user
    """
    item = {
        "ts": datetime.utcnow().isoformat(),
        "user_id": user_id[:8] if user_id else "anon",  # truncated to avoid PII
        "prompt": sanitize_user_input(user_input)[:2000],
        "answer": model_answer[:4000],
        "rating": int(rating),
        "correction": (sanitize_user_input(correction)[:4000] if correction else None),
        "sources": sources or []
    }
    fname = FEEDBACK_DIR / f"fb_{int(datetime.utcnow().timestamp())}.json"
    with open(fname, "w", encoding="utf-8") as f:
        json.dump(item, f, ensure_ascii=False)
    return True

def export_for_finetune(out_path="finetune_data.jsonl", min_rating=4):
    """
    Aggregate high-quality examples (rating >= min_rating) into JSONL: {prompt, response}
    for LoRA training. This is manual and opt-in: run periodically.
    """
    records = []
    for p in FEEDBACK_DIR.glob("fb_*.json"):
        try:
            j = json.load(open(p, "r", encoding="utf-8"))
            if j.get("rating",0) >= min_rating:
                # convert to instruction-response pair
                prompt = j["prompt"]
                response = j.get("correction") or j["answer"]
                records.append({"prompt": prompt, "response": response})
        except Exception:
            continue
    if not records:
        return 0
    with open(out_path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    return len(records)
from learning import save_feedback, export_for_finetune

@app.post("/feedback")
async def feedback(data: dict, user=Depends(require_jwt)):
    user_id = user.get("sub") if isinstance(user, dict) else None
    save_feedback(user_id, data.get("prompt",""), data.get("answer",""), data.get("rating",0), data.get("correction"), data.get("sources"))
    return {"ok": True}

# Admin-only: export finetune file (protect this endpoint)
@app.post("/admin/export_finetune")
async def export_finetune(user=Depends(require_admin_jwt)):
    n = export_for_finetune()
    return {"exported": n}
# plugins.py
from typing import List, Dict, Any
import importlib
import os

PLUGIN_FOLDER = "plugin_impls"

def load_plugin(name: str):
    # plugin modules live under plugin_impls.<name>
    try:
        mod = importlib.import_module(f"{PLUGIN_FOLDER}.{name}")
        return mod
    except Exception as e:
        raise ImportError(f"Cannot load plugin {name}: {e}")

def run_plugins(query: str, plugin_names: List[str]) -> List[Dict[str, Any]]:
    results = []
    for name in plugin_names:
        try:
            p = load_plugin(name)
            text = p.fetch(query)
            if text:
                results.append({"plugin": name, "text": text})
        except Exception as e:
            results.append({"plugin": name, "error": str(e)})
    return results
# plugin_impls/wikipedia_plugin.py
import wikipedia

def fetch(query: str) -> str:
    try:
        # search then return top page summary
        results = wikipedia.search(query, results=3)
        if not results:
            return ""
        page = wikipedia.page(results[0], auto_suggest=False)
        return page.summary[:8000]
    except Exception as e:
        return ""
# plugin_impls/pdf_plugin.py
from pathlib import Path
from PyPDF2 import PdfReader

DOCS_DIR = Path("uploaded_pdfs")

def fetch(query: str) -> str:
    # naive: return content of PDFs that mention query
    out = []
    q = query.lower()
    for p in DOCS_DIR.glob("*.pdf"):
        try:
            r = PdfReader(str(p))
            text = ""
            for page in r.pages:
                text += page.extract_text() or ""
            if q in text.lower():
                out.append(text[:4000])
        except Exception:
            continue
    return "\n\n".join(out)
# plugin_impls/web_plugin.py
import asyncio, aiohttp
from safe_fetch import safe_fetch_page
from bs4 import BeautifulSoup

async def _fetch(url):
    async with aiohttp.ClientSession() as session:
        html = await safe_fetch_page(session, url)
        if not html:
            return ""
        soup = BeautifulSoup(html, "html.parser")
        for s in soup(["script","style","noscript","iframe"]):
            s.extract()
        return " ".join(p.get_text() for p in soup.find_all("p"))[:4000]

def fetch(query: str) -> str:
    # naive: use DuckDuckGo HTML search (no API) could be added; we keep simple offline
    return ""  # leave for custom implementation
from plugins import run_plugins

# In your multisearch or rag flow, call plugins before final retrieval:
plugin_names = ["wikipedia_plugin", "pdf_plugin"]  # configured per user or admin
plugin_results = run_plugins(user_query, plugin_names)
# plugin_results is list of dicts with 'plugin' and 'text'
# include top plugin texts into RAG context (after retrieval ranking)
# emotion.py
import re
from typing import Tuple

# small lexicon for fast inference (expand as needed)
POSITIVE = {"happy","good","great","fantastic","awesome","thanks","thank you","love","glad"}
NEGATIVE = {"sad","angry","upset","bad","frustrated","hate","disappointed","annoyed","worried"}
ANXIOUS = {"nervous","anxious","scared","fear","panic","afraid"}

def detect_emotion(text: str) -> Tuple[str, float]:
    """
    Returns (emotion_label, confidence_score)
    Labels: neutral, positive, negative, anxious
    """
    t = text.lower()
    # simple lexical match
    pos = sum(1 for w in POSITIVE if w in t)
    neg = sum(1 for w in NEGATIVE if w in t)
    anx = sum(1 for w in ANXIOUS if w in t)
    total = pos + neg + anx
    if total == 0:
        return "neutral", 0.5
    if pos >= neg and pos >= anx:
        return "positive", min(0.9, 0.5 + pos / (total + 1))
    if anx >= neg and anx >= pos:
        return "anxious", min(0.9, 0.5 + anx / (total + 1))
    return "negative", min(0.9, 0.5 + neg / (total + 1))

def tone_for_emotion(emotion: str) -> dict:
    """
    Maps emotion to personality tuning adjustments.
    Example usage: prepend to system prompt or adjust speech rate/volume.
    """
    if emotion == "positive":
        return {"style": "encouraging and warm", "rate": 150, "volume": 1.0}
    if emotion == "anxious":
        return {"style": "calm, reassuring, slow", "rate": 120, "volume": 0.9}
    if emotion == "negative":
        return {"style": "firm but empathetic", "rate": 130, "volume": 1.0}
    return {"style": "calm and wise", "rate": 140, "volume": 1.0}
from emotion import detect_emotion, tone_for_emotion

# inside your chat endpoint before building prompt:
emotion, e_conf = detect_emotion(clean)
tone_settings = tone_for_emotion(emotion)
persona_prefix = f"You are Chanakya, respond in a {tone_settings['style']} tone."
prompt = persona_prefix + "\n\nMemory:\n" + recall() + "\n\nUser: " + clean + "\nChanakya:"
# learning.py
import json
from pathlib import Path
from datetime import datetime
from sanitize import sanitize_user_input
from verify import answer_confidence

FEEDBACK_DIR = Path("feedback")
FEEDBACK_DIR.mkdir(exist_ok=True)

def save_feedback(user_id: str, user_input: str, model_answer: str, rating: int, correction: str = None, sources=None):
    """
    rating: 1 (bad) to 5 (excellent)
    correction: optional corrected answer by user
    """
    item = {
        "ts": datetime.utcnow().isoformat(),
        "user_id": user_id[:8] if user_id else "anon",  # truncated to avoid PII
        "prompt": sanitize_user_input(user_input)[:2000],
        "answer": model_answer[:4000],
        "rating": int(rating),
        "correction": (sanitize_user_input(correction)[:4000] if correction else None),
        "sources": sources or []
    }
    fname = FEEDBACK_DIR / f"fb_{int(datetime.utcnow().timestamp())}.json"
    with open(fname, "w", encoding="utf-8") as f:
        json.dump(item, f, ensure_ascii=False)
    return True

def export_for_finetune(out_path="finetune_data.jsonl", min_rating=4):
    """
    Aggregate high-quality examples (rating >= min_rating) into JSONL: {prompt, response}
    for LoRA training. This is manual and opt-in: run periodically.
    """
    records = []
    for p in FEEDBACK_DIR.glob("fb_*.json"):
        try:
            j = json.load(open(p, "r", encoding="utf-8"))
            if j.get("rating",0) >= min_rating:
                # convert to instruction-response pair
                prompt = j["prompt"]
                response = j.get("correction") or j["answer"]
                records.append({"prompt": prompt, "response": response})
        except Exception:
            continue
    if not records:
        return 0
    with open(out_path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    return len(records)
from learning import save_feedback, export_for_finetune

@app.post("/feedback")
async def feedback(data: dict, user=Depends(require_jwt)):
    user_id = user.get("sub") if isinstance(user, dict) else None
    save_feedback(user_id, data.get("prompt",""), data.get("answer",""), data.get("rating",0), data.get("correction"), data.get("sources"))
    return {"ok": True}

# Admin-only: export finetune file (protect this endpoint)
@app.post("/admin/export_finetune")
async def export_finetune(user=Depends(require_admin_jwt)):
    n = export_for_finetune()
    return {"exported": n}
# plugins.py
from typing import List, Dict, Any
import importlib
import os

PLUGIN_FOLDER = "plugin_impls"

def load_plugin(name: str):
    # plugin modules live under plugin_impls.<name>
    try:
        mod = importlib.import_module(f"{PLUGIN_FOLDER}.{name}")
        return mod
    except Exception as e:
        raise ImportError(f"Cannot load plugin {name}: {e}")

def run_plugins(query: str, plugin_names: List[str]) -> List[Dict[str, Any]]:
    results = []
    for name in plugin_names:
        try:
            p = load_plugin(name)
            text = p.fetch(query)
            if text:
                results.append({"plugin": name, "text": text})
        except Exception as e:
            results.append({"plugin": name, "error": str(e)})
    return results
# plugin_impls/wikipedia_plugin.py
import wikipedia

def fetch(query: str) -> str:
    try:
        # search then return top page summary
        results = wikipedia.search(query, results=3)
        if not results:
            return ""
        page = wikipedia.page(results[0], auto_suggest=False)
        return page.summary[:8000]
    except Exception as e:
        return ""
# plugin_impls/pdf_plugin.py
from pathlib import Path
from PyPDF2 import PdfReader

DOCS_DIR = Path("uploaded_pdfs")

def fetch(query: str) -> str:
    # naive: return content of PDFs that mention query
    out = []
    q = query.lower()
    for p in DOCS_DIR.glob("*.pdf"):
        try:
            r = PdfReader(str(p))
            text = ""
            for page in r.pages:
                text += page.extract_text() or ""
            if q in text.lower():
                out.append(text[:4000])
        except Exception:
            continue
    return "\n\n".join(out)
# plugin_impls/web_plugin.py
import asyncio, aiohttp
from safe_fetch import safe_fetch_page
from bs4 import BeautifulSoup

async def _fetch(url):
    async with aiohttp.ClientSession() as session:
        html = await safe_fetch_page(session, url)
        if not html:
            return ""
        soup = BeautifulSoup(html, "html.parser")
        for s in soup(["script","style","noscript","iframe"]):
            s.extract()
        return " ".join(p.get_text() for p in soup.find_all("p"))[:4000]

def fetch(query: str) -> str:
    # naive: use DuckDuckGo HTML search (no API) could be added; we keep simple offline
    return ""  # leave for custom implementation
from plugins import run_plugins

# In your multisearch or rag flow, call plugins before final retrieval:
plugin_names = ["wikipedia_plugin", "pdf_plugin"]  # configured per user or admin
plugin_results = run_plugins(user_query, plugin_names)
# plugin_results is list of dicts with 'plugin' and 'text'
# include top plugin texts into RAG context (after retrieval ranking)
# main_secure.py
# Orchestrator for Chanakya AI â€” secure, memory, emotion-aware, plugin-enabled RAG + TTS

import os
import asyncio
from fastapi import FastAPI, Depends, HTTPException, Header, Body, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# load env
load_dotenv()

# local helpers (must exist in repo)
from auth_jwt import require_jwt, create_access_token  # require_jwt returns token payload
from rate_limiter import rate_limit_dependency
from sanitize import sanitize_user_input, detect_prompt_injection
from moderation import moderate_text
from model_loader import generate_sync  # synchronous model call
from multisearch_rag import multisearch_and_answer
from voice_api import router as voice_router  # contains /voice endpoints
from memory import remember, recall, load_memory, save_memory
from personality import CHANAKYA_PERSONALITY
from emotion import detect_emotion, tone_for_emotion
from plugins import run_plugins
from learning import save_feedback, export_for_finetune
from verify import answer_confidence
from logger import logger

# Basic FastAPI setup
app = FastAPI(title="Chanakya AI â€” Secure Orchestrator")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# include voice router
app.include_router(voice_router, prefix="/voice")

# Models
class ChatRequest(BaseModel):
    user_input: str
    use_plugins: bool = False
    plugin_list: list = None  # optional list of plugin names
    k: int = 4  # RAG top-k

class FeedbackRequest(BaseModel):
    prompt: str
    answer: str
    rating: int
    correction: str = None
    sources: list = None

# Helper - build persona + memory + tone prefix
def build_prompt(user_input: str, tone_settings: dict, plugin_texts: list, top_docs: list):
    persona = f"You are {CHANAKYA_PERSONALITY.get('name','Chanakya')}, a {CHANAKYA_PERSONALITY.get('tone','wise')} teacher. {CHANAKYA_PERSONALITY.get('speech_style','')}"
    tone_line = f"Respond in a {tone_settings.get('style','calm and wise')} tone. Keep answers concise and practical."
    mem = recall()
    # plugin_texts: list of dicts {'plugin':name,'text':text}
    plugin_context = "\n\n".join([f"[PLUGIN:{p['plugin']}]\n{p['text']}" for p in (plugin_texts or []) if p.get("text")])
    docs_context = "\n\n".join([f"[DOC]\n{d}" for d in (top_docs or [])])
    prompt_parts = [
        persona,
        tone_line,
        "\nMemory:\n" + (mem if mem else "No memory yet."),
        "\nPluginContext:\n" + (plugin_context if plugin_context else "No plugin data."),
        "\nRetrievedDocs:\n" + (docs_context if docs_context else "No retrieved docs."),
        "\nUser: " + user_input,
        "\nChanakya:"
    ]
    return "\n\n".join(prompt_parts)

# Health
@app.get("/healthz")
def health():
    return {"status":"ok"}

# Metrics endpoint simple (if prometheus_client used elsewhere)
try:
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    @app.get("/metrics")
    def metrics():
        return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
except Exception:
    pass

# Chat endpoint â€” main orchestrator
@app.post("/chat")
async def chat(req: ChatRequest, token_payload=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    user_input = req.user_input or ""
    # sanitize & moderate
    clean = sanitize_user_input(user_input)
    if not moderate_text(clean):
        raise HTTPException(status_code=400, detail="Request disallowed by policy.")
    if detect_prompt_injection(clean):
        raise HTTPException(status_code=400, detail="Prompt appears malicious or attempts to override system behavior.")
    # emotion detect -> tone adjustments
    emotion_label, emotion_conf = detect_emotion(clean)
    tone_settings = tone_for_emotion(emotion_label)

    # plugin collection (optional)
    plugin_texts = []
    if req.use_plugins and req.plugin_list:
        try:
            plugin_texts = run_plugins(clean, req.plugin_list)
        except Exception as e:
            logger.info(f"Plugin error: {e}")
            plugin_texts = []

    # retrieval via multisearch (this will fetch pages and return answer if you call multisearch_and_answer)
    # Here we use multisearch only to get top docs (not full answer) to include in prompt
    # to avoid double-generation, call a variant that returns top docs. For simplicity, call multisearch_and_answer and ignore `answer`.
    try:
        ms = await multisearch_and_answer(clean, use_local_model=False)  # if you want local generation here set True
        # multisearch_and_answer returns {"answer":..., "sources":[...]}
        top_docs = [s.get("text","") for s in ms.get("sources", []) if s.get("url")]
    except Exception as e:
        logger.info(f"Multisearch failure (continuing without remote docs): {e}")
        top_docs = []

    # Build final prompt
    prompt = build_prompt(clean, tone_settings, plugin_texts, top_docs)

    # Generate (in threadpool to keep FastAPI async)
    loop = asyncio.get_event_loop()
    try:
        answer = await loop.run_in_executor(None, generate_sync, prompt)
    except Exception as e:
        logger.error(f"Model generation error: {e}")
        raise HTTPException(status_code=500, detail="Model generation failed.")

    # Confidence check
    try:
        # verify expects list of dicts with 'text' field; here we use plugin_texts + top_docs as source
        verify_sources = [{"text": p.get("text","")} for p in (plugin_texts or [])] + [{"text": d} for d in top_docs]
        conf = answer_confidence(answer, verify_sources)
    except Exception:
        conf = 0.0

    # If low confidence, give safe fallback
    if conf < 0.25:
        logger.info(f"Low confidence ({conf:.2f}), returning cautious response.")
        safe_reply = "I couldn't find reliable evidence to answer that confidently. Here are the sources I checked."
        # still remember that user asked and we replied with safe message
        remember(clean, safe_reply)
        return {"answer": safe_reply, "confidence": conf, "sources": [s.get("url") for s in ms.get("sources", [])] if isinstance(ms, dict) else []}

    # remember
    remember(clean, answer)

    return {"answer": answer, "confidence": conf, "sources": [s.get("url") for s in ms.get("sources", [])] if isinstance(ms, dict) else []}

# Multisearch endpoint (direct)
@app.post("/multisearch")
async def multisearch_endpoint(body: dict = Body(...), token_payload=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    query = sanitize_user_input(body.get("query",""))
    if not moderate_text(query):
        raise HTTPException(400, "disallowed")
    if detect_prompt_injection(query):
        raise HTTPException(400, "malicious")
    out = await multisearch_and_answer(query, use_local_model=True)  # use local LLM for final answer
    # remember question and short summary
    try:
        remember(query, out.get("answer",""))
    except Exception:
        pass
    return out

# Feedback endpoint â€” save anonymized feedback for learning
@app.post("/feedback")
async def feedback_endpoint(fb: FeedbackRequest, token_payload=Depends(require_jwt)):
    user_id = token_payload.get("sub") if isinstance(token_payload, dict) else None
    save_feedback(user_id, fb.prompt, fb.answer, fb.rating, fb.correction, fb.sources)
    return {"ok": True}

# Admin export finetune (protect this with admin-level JWT in production)
@app.post("/admin/export_finetune")
async def admin_export(token_payload=Depends(require_jwt)):
    # VERY IMPORTANT: ensure only admin can call in production
    # here we assume that token_payload contains 'role' or 'is_admin' flag â€” adjust as needed
    is_admin = isinstance(token_payload, dict) and token_payload.get("role") in ("admin","superuser")
    if not is_admin:
        raise HTTPException(403, "Forbidden")
    n = export_for_finetune()
    return {"exported_count": n}

# Simple token create (dev only) â€” in production use proper auth
@app.post("/auth/dev_token")
def dev_token(username: str = Body(...)):
    # create access token quickly for dev use (not for production)
    t = create_access_token(username)
    return {"token": t}

# Forget memory (admin-only)
@app.post("/admin/forget")
def admin_forget(token_payload=Depends(require_jwt)):
    is_admin = isinstance(token_payload, dict) and token_payload.get("role") in ("admin","superuser")
    if not is_admin:
        raise HTTPException(403, "Forbidden")
    save_memory({"history":[]})
    return {"ok": True}

# Root
@app.get("/")
def index():
    return {"service":"Chanakya AI", "status":"running"}

# Startup / shutdown hooks (optional)
@app.on_event("startup")
async def startup():
    logger.info("Chanakya AI starting up...")

@app.on_event("shutdown")
async def shutdown():
    logger.info("Chanakya AI shutting down...")
docker compose up --build
# or, if running locally:
uvicorn main_secure:app --host 0.0.0.0 --port 8000 --reload
# main_secure.py
# Orchestrator for Chanakya AI â€” secure, memory, emotion-aware, plugin-enabled RAG + TTS

import os
import asyncio
from fastapi import FastAPI, Depends, HTTPException, Header, Body, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# load env
load_dotenv()

# local helpers (must exist in repo)
from auth_jwt import require_jwt, create_access_token  # require_jwt returns token payload
from rate_limiter import rate_limit_dependency
from sanitize import sanitize_user_input, detect_prompt_injection
from moderation import moderate_text
from model_loader import generate_sync  # synchronous model call
from multisearch_rag import multisearch_and_answer
from voice_api import router as voice_router  # contains /voice endpoints
from memory import remember, recall, load_memory, save_memory
from personality import CHANAKYA_PERSONALITY
from emotion import detect_emotion, tone_for_emotion
from plugins import run_plugins
from learning import save_feedback, export_for_finetune
from verify import answer_confidence
from logger import logger

# Basic FastAPI setup
app = FastAPI(title="Chanakya AI â€” Secure Orchestrator")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# include voice router
app.include_router(voice_router, prefix="/voice")

# Models
class ChatRequest(BaseModel):
    user_input: str
    use_plugins: bool = False
    plugin_list: list = None  # optional list of plugin names
    k: int = 4  # RAG top-k

class FeedbackRequest(BaseModel):
    prompt: str
    answer: str
    rating: int
    correction: str = None
    sources: list = None

# Helper - build persona + memory + tone prefix
def build_prompt(user_input: str, tone_settings: dict, plugin_texts: list, top_docs: list):
    persona = f"You are {CHANAKYA_PERSONALITY.get('name','Chanakya')}, a {CHANAKYA_PERSONALITY.get('tone','wise')} teacher. {CHANAKYA_PERSONALITY.get('speech_style','')}"
    tone_line = f"Respond in a {tone_settings.get('style','calm and wise')} tone. Keep answers concise and practical."
    mem = recall()
    # plugin_texts: list of dicts {'plugin':name,'text':text}
    plugin_context = "\n\n".join([f"[PLUGIN:{p['plugin']}]\n{p['text']}" for p in (plugin_texts or []) if p.get("text")])
    docs_context = "\n\n".join([f"[DOC]\n{d}" for d in (top_docs or [])])
    prompt_parts = [
        persona,
        tone_line,
        "\nMemory:\n" + (mem if mem else "No memory yet."),
        "\nPluginContext:\n" + (plugin_context if plugin_context else "No plugin data."),
        "\nRetrievedDocs:\n" + (docs_context if docs_context else "No retrieved docs."),
        "\nUser: " + user_input,
        "\nChanakya:"
    ]
    return "\n\n".join(prompt_parts)

# Health
@app.get("/healthz")
def health():
    return {"status":"ok"}

# Metrics endpoint simple (if prometheus_client used elsewhere)
try:
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    @app.get("/metrics")
    def metrics():
        return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
except Exception:
    pass

# Chat endpoint â€” main orchestrator
@app.post("/chat")
async def chat(req: ChatRequest, token_payload=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    user_input = req.user_input or ""
    # sanitize & moderate
    clean = sanitize_user_input(user_input)
    if not moderate_text(clean):
        raise HTTPException(status_code=400, detail="Request disallowed by policy.")
    if detect_prompt_injection(clean):
        raise HTTPException(status_code=400, detail="Prompt appears malicious or attempts to override system behavior.")
    # emotion detect -> tone adjustments
    emotion_label, emotion_conf = detect_emotion(clean)
    tone_settings = tone_for_emotion(emotion_label)

    # plugin collection (optional)
    plugin_texts = []
    if req.use_plugins and req.plugin_list:
        try:
            plugin_texts = run_plugins(clean, req.plugin_list)
        except Exception as e:
            logger.info(f"Plugin error: {e}")
            plugin_texts = []

    # retrieval via multisearch (this will fetch pages and return answer if you call multisearch_and_answer)
    # Here we use multisearch only to get top docs (not full answer) to include in prompt
    # to avoid double-generation, call a variant that returns top docs. For simplicity, call multisearch_and_answer and ignore `answer`.
    try:
        ms = await multisearch_and_answer(clean, use_local_model=False)  # if you want local generation here set True
        # multisearch_and_answer returns {"answer":..., "sources":[...]}
        top_docs = [s.get("text","") for s in ms.get("sources", []) if s.get("url")]
    except Exception as e:
        logger.info(f"Multisearch failure (continuing without remote docs): {e}")
        top_docs = []

    # Build final prompt
    prompt = build_prompt(clean, tone_settings, plugin_texts, top_docs)

    # Generate (in threadpool to keep FastAPI async)
    loop = asyncio.get_event_loop()
    try:
        answer = await loop.run_in_executor(None, generate_sync, prompt)
    except Exception as e:
        logger.error(f"Model generation error: {e}")
        raise HTTPException(status_code=500, detail="Model generation failed.")

    # Confidence check
    try:
        # verify expects list of dicts with 'text' field; here we use plugin_texts + top_docs as source
        verify_sources = [{"text": p.get("text","")} for p in (plugin_texts or [])] + [{"text": d} for d in top_docs]
        conf = answer_confidence(answer, verify_sources)
    except Exception:
        conf = 0.0

    # If low confidence, give safe fallback
    if conf < 0.25:
        logger.info(f"Low confidence ({conf:.2f}), returning cautious response.")
        safe_reply = "I couldn't find reliable evidence to answer that confidently. Here are the sources I checked."
        # still remember that user asked and we replied with safe message
        remember(clean, safe_reply)
        return {"answer": safe_reply, "confidence": conf, "sources": [s.get("url") for s in ms.get("sources", [])] if isinstance(ms, dict) else []}

    # remember
    remember(clean, answer)

    return {"answer": answer, "confidence": conf, "sources": [s.get("url") for s in ms.get("sources", [])] if isinstance(ms, dict) else []}

# Multisearch endpoint (direct)
@app.post("/multisearch")
async def multisearch_endpoint(body: dict = Body(...), token_payload=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    query = sanitize_user_input(body.get("query",""))
    if not moderate_text(query):
        raise HTTPException(400, "disallowed")
    if detect_prompt_injection(query):
        raise HTTPException(400, "malicious")
    out = await multisearch_and_answer(query, use_local_model=True)  # use local LLM for final answer
    # remember question and short summary
    try:
        remember(query, out.get("answer",""))
    except Exception:
        pass
    return out

# Feedback endpoint â€” save anonymized feedback for learning
@app.post("/feedback")
async def feedback_endpoint(fb: FeedbackRequest, token_payload=Depends(require_jwt)):
    user_id = token_payload.get("sub") if isinstance(token_payload, dict) else None
    save_feedback(user_id, fb.prompt, fb.answer, fb.rating, fb.correction, fb.sources)
    return {"ok": True}

# Admin export finetune (protect this with admin-level JWT in production)
@app.post("/admin/export_finetune")
async def admin_export(token_payload=Depends(require_jwt)):
    # VERY IMPORTANT: ensure only admin can call in production
    # here we assume that token_payload contains 'role' or 'is_admin' flag â€” adjust as needed
    is_admin = isinstance(token_payload, dict) and token_payload.get("role") in ("admin","superuser")
    if not is_admin:
        raise HTTPException(403, "Forbidden")
    n = export_for_finetune()
    return {"exported_count": n}

# Simple token create (dev only) â€” in production use proper auth
@app.post("/auth/dev_token")
def dev_token(username: str = Body(...)):
    # create access token quickly for dev use (not for production)
    t = create_access_token(username)
    return {"token": t}

# Forget memory (admin-only)
@app.post("/admin/forget")
def admin_forget(token_payload=Depends(require_jwt)):
    is_admin = isinstance(token_payload, dict) and token_payload.get("role") in ("admin","superuser")
    if not is_admin:
        raise HTTPException(403, "Forbidden")
    save_memory({"history":[]})
    return {"ok": True}

# Root
@app.get("/")
def index():
    return {"service":"Chanakya AI", "status":"running"}

# Startup / shutdown hooks (optional)
@app.on_event("startup")
async def startup():
    logger.info("Chanakya AI starting up...")

@app.on_event("shutdown")
async def shutdown():
    logger.info("Chanakya AI shutting down...")
Quick next steps after adding this file
Save main_secure.py into your project root (overwrite previous if needed).

Ensure all required modules are present (auth_jwt.py, rate_limiter.py, sanitize.py, moderation.py, model_loader.py, multisearch_rag.py, voice_api.py, memory.py, personality.py, emotion.py, plugins.py, learning.py, verify.py, logger.py, safe_fetch.py).

Update .env with your keys (JWT_SECRET, ELEVENLABS_API_KEY, BING_API_KEY, etc.).

Restart the server:

bash
Copy code
docker compose up --build
# or, if running locally:
uvicorn main_secure:app --host 0.0.0.0 --port 8000 --reload
Test:

Create a dev token: POST /auth/dev_token with JSON body {"username":"devuser"} â€” copy returned token.

Use that token as Authorization: Bearer <token> header for /chat, /multisearch, /voice/speak_offline, /feedback.

If you want, I can now:

(A) produce a short demo script that simulates a user conversation through the full flow (emotion â†’ plugin â†’ RAG â†’ TTS â†’ feedback), or

(B) produce a LoRA fine-tuning script that takes feedback/*.json â†’ trains a LoRA adapter, or

(C) walk you step-by-step to deploy this to AWS GPU EC2 (with commands and driver install script).

Pick A / B / C (or say â€œallâ€) and Iâ€™ll generate it immediately.

# agent.py
"""
Structured-agent (ReAct style) for Chanakya AI.

Usage:
    from agent import run_agent
    out = run_agent("How to negotiate a salary raise?", use_local_model=True)
    print(out["final_answer"])
"""

import time
import json
import re
from typing import Dict, Any, Optional

# Import your existing helpers from repo
from sanitize import sanitize_user_input, detect_prompt_injection
from multisearch_rag import multisearch_and_answer   # returns {"answer":..., "sources":[...]}
from memory import recall, remember
from model_loader import generate_sync  # sync model call (local)
from logger import logger

# Config
MAX_STEPS = 6
STEP_TIMEOUT = 20.0  # seconds per model call
AGENT_NAME = "Chanakya-Agent"
TOOL_NAMES = ["search", "memory_recall", "memory_write", "calculator"]

# Safe calculator (very small DSL)
def safe_calc(expr: str) -> str:
    # allow digits, operators, parentheses, decimals, spaces
    if not re.match(r"^[0-9+\-*/().\s]+$", expr):
        return "ERROR: unsafe expression"
    try:
        # eval in restricted namespace
        val = eval(expr, {"__builtins__": None}, {})
        return str(val)
    except Exception as e:
        return f"ERROR: {e}"

# Tool wrappers
def tool_search(query: str, top_k: int = 3) -> Dict[str, Any]:
    # returns a short consolidated result and list of sources
    try:
        out = multisearch_and_answer(query, use_local_model=False)  # use external or local depending on your setup
        # multisearch_and_answer is async; if it's async you may need to run it accordingly.
        # For compatibility, accept dict or coroutine
        if hasattr(out, "__await__"):
            import asyncio
            out = asyncio.get_event_loop().run_until_complete(out)
        return {"text": out.get("answer",""), "sources": out.get("sources", [])}
    except Exception as e:
        logger.info(f"search tool error: {e}")
        return {"text": "", "sources": []}

def tool_memory_recall(query: str) -> str:
    # simple recall uses entire memory text
    mem = recall()
    return mem or ""

def tool_memory_write(user_input: str, ai_response: str) -> bool:
    try:
        remember(user_input, ai_response)
        return True
    except Exception:
        return False

def tool_calculator(expr: str) -> str:
    return safe_calc(expr)

# Agent prompt template: strict JSON output only
JSON_PROMPT_TEMPLATE = """
You are {agent_name}, a structured reasoning assistant. You MUST OUTPUT ONLY valid JSON in every response, and NOTHING ELSE.
Do NOT output any chain-of-thought, internal monologue, or explanation. Output only the JSON object described below.

The JSON object MUST have exactly one of these shapes:

1) To take an action:
{{ "action": "<action_name>", "action_input": "<string>" }}

2) When finished and ready to give the final answer:
{{ "action": "finish", "final_answer": "<assistant answer (concise)>", "sources": ["url1","url2", ...] }}

Allowed actions: {allowed_actions}

Rules:
- If you want to look up facts, use action 'search' with short query text.
- If you want to consult memory, use 'memory_recall' or 'memory_write'.
- If you need to compute a math expression, use 'calculator'.
- After each action, the system will execute the action and return the observation. Then you will produce another JSON action or finish.
- NEVER include any instructions, thoughts, or commentary outside the JSON object.
- Keep final answers concise and include a short list of sources (can be empty).

Now, given the USER QUESTION below, respond with the FIRST JSON action.

USER QUESTION:
\"\"\"{user_question}\"\"\"
"""

def parse_json_output(text: str) -> Optional[Dict[str, Any]]:
    # Extract first JSON object from model output
    try:
        # find first '{' and parse until matching '}'
        idx = text.find('{')
        if idx == -1:
            return None
        substring = text[idx:]
        # heuristic: try progressively larger slices to find valid JSON
        for end in range(len(substring), 0, -1):
            cand = substring[:end]
            try:
                parsed = json.loads(cand)
                return parsed
            except Exception:
                continue
    except Exception:
        return None
    return None

def run_agent(user_question: str, use_local_model: bool = True, max_steps: int = MAX_STEPS) -> Dict[str, Any]:
    """
    Main loop: sends JSON-only prompts to model, executes tools, and loops until 'finish'.
    Returns final dict with 'final_answer' and 'sources'.
    """
    start_time = time.time()

    # Sanitize + safety checks
    q = sanitize_user_input(user_question)
    if detect_prompt_injection(q):
        return {"error": "Rejected: prompt appears malicious."}
    # Build initial prompt
    prompt = JSON_PROMPT_TEMPLATE.format(
        agent_name=AGENT_NAME,
        allowed_actions=", ".join(TOOL_NAMES),
        user_question=q
    )

    step = 0
    last_observation = ""
    history = []  # for debug but will not be exposed as chain-of-thought

    while step < max_steps:
        step += 1
        # Ask model for next action
        try:
            # use generate_sync wrapper; ensure prompt length kept reasonable
            model_input = prompt
            if last_observation:
                # append last observation in a short system line so agent knows outcome
                model_input += "\n\nPREVIOUS_OBSERVATION:\n" + last_observation + "\n\nNow output the next JSON action."
            # Call model synchronously (blocking) in a safe manner
            if use_local_model:
                raw = generate_sync(model_input, max_new_tokens=256)
            else:
                # If you implement remote HF API, call it here; for now raise
                raw = generate_sync(model_input, max_new_tokens=256)
        except Exception as e:
            logger.error(f"Model call failed: {e}")
            return {"error": "model_call_failed"}

        # Parse JSON output
        parsed = parse_json_output(raw)
        if not parsed or "action" not in parsed:
            # Model did not follow JSON-only rule; abort
            logger.info("Model did not return valid JSON action; aborting.")
            return {"error": "model_invalid_response", "raw": raw}

        action = parsed.get("action")
        action_input = parsed.get("action_input", "")

        # Execute action
        observation = ""
        try:
            if action == "search":
                res = tool_search(action_input)
                observation = f"SEARCH_RESULT_TEXT: {res.get('text','')[:1000]} | SOURCES: {[s.get('url') for s in res.get('sources',[])]}"
            elif action == "memory_recall":
                mem = tool_memory_recall(action_input)
                observation = f"MEMORY: {mem[:1000]}"
            elif action == "memory_write":
                # expects action_input to be JSON-like "user|ai" or simple text; safely split
                parts = action_input.split("||", 1)
                u = parts[0] if parts else ""
                a = parts[1] if len(parts) > 1 else ""
                ok = tool_memory_write(u, a)
                observation = f"MEMORY_WRITE: {'ok' if ok else 'fail'}"
            elif action == "calculator":
                calc_out = tool_calculator(action_input)
                observation = f"CALC: {calc_out}"
            elif action == "finish":
                # model claims finished â€” get final answer
                final_answer = parsed.get("final_answer", "").strip()
                sources = parsed.get("sources", [])
                # optional: store to memory before returning
                try:
                    remember(q, final_answer)
                except Exception:
                    pass
                return {"final_answer": final_answer, "sources": sources, "steps": step}
            else:
                observation = f"ERROR: unknown action {action}"
        except Exception as e:
            observation = f"ERROR executing action: {e}"

        # safety: update last_observation with sanitized, truncated value (no long texts)
        safe_obs = sanitize_user_input(str(observation))[:1500]
        last_observation = safe_obs

        # Append to history (not returned as chain-of-thought)
        history.append({"step": step, "action": action, "action_input": action_input, "observation": safe_obs})

        # Update prompt for next iteration: we append only the brief observation (not thoughts)
        prompt = JSON_PROMPT_TEMPLATE.format(
            agent_name=AGENT_NAME,
            allowed_actions=", ".join(TOOL_NAMES),
            user_question=q
        )
        # Check time
        if time.time() - start_time > (max_steps * STEP_TIMEOUT + 5):
            return {"error": "timeout", "history": history}

    # Max steps reached without finish
    return {"error": "max_steps_exceeded", "history": history}
from agent import run_agent
res = run_agent("How should I prepare for a job interview for a senior manager role?", use_local_model=True)
print(res.get("final_answer"))
from agent import run_agent
@app.post("/agent")
async def agent_endpoint(body: dict, token=Depends(require_jwt)):
    q = sanitize_user_input(body.get("query",""))
    out = run_agent(q, use_local_model=True)
    return out
# tree_of_thoughts.py
import time
import json
import random
import asyncio
from typing import List, Dict, Any
from logger import logger
from sanitize import sanitize_user_input, detect_prompt_injection
from model_loader import generate_sync
from verify import answer_confidence
from multisearch_rag import multisearch_and_answer

# CONFIG
MAX_BRANCHES = 4        # how many candidate trees to explore
MAX_DEPTH = 3           # reasoning depth per branch
GEN_TIMEOUT = 12        # seconds per generate call (soft)
TOP_K_SOURCES = 5

def _safe_gen(prompt: str, max_tokens: int = 120) -> str:
    """Wrapper to call the local model safely and return text or raise."""
    # truncate prompt to reasonable size to avoid OOM
    try:
        text = generate_sync(prompt, max_new_tokens=max_tokens)
        return text
    except Exception as e:
        logger.error("Model generation failed in ToT: %s", e)
        raise

def _build_thought_prompt(question: str, history: List[str]) -> str:
    """
    Short controlled prompt: ask model for the next 'thought' step.
    Crucial: instruct model to output ONLY the thought sentence (no chain-of-thought).
    """
    prompt = (
        "You are Chanakya-Agent. Produce **one short reasoning step** (a single sentence) "
        "that moves toward answering the user's question. OUTPUT ONLY that sentence.\n\n"
        f"Question: {question}\n"
    )
    if history:
        prompt += "Previous steps:\n" + "\n".join(f"- {s}" for s in history) + "\n"
    prompt += "\nNext step:"
    return prompt

def _build_answer_prompt(question: str, steps: List[str], include_sources: str = "") -> str:
    """
    Build a short final-answer prompt that asks the model to produce a concise answer using
    the provided steps as facts. Must instruct model to output only the answer (no thoughts).
    """
    prompt = (
        "You are Chanakya. Using the short factual steps below, give a concise final answer "
        "to the question. Output JUST the answer (no internal thoughts).\n\n"
        "Steps:\n" + "\n".join(f"- {s}" for s in steps) + "\n\n"
        f"{'Sources:\n' + include_sources + '\n\n' if include_sources else ''}"
        f"Question: {question}\nAnswer:"
    )
    return prompt

def run_tree_of_thoughts(question: str, allow_web: bool = True) -> Dict[str, Any]:
    """
    Returns best final answer and metadata.
    1) Optionally run a multisearch to collect sources (used by scoring).
    2) Create MAX_BRANCHES candidate trees:
       - For depth 1..MAX_DEPTH generate short step sentences.
       - For each branch generate a candidate final answer from its steps.
    3) Score each final answer by verifying against retrieved sources (if any) and pick the best.
    """
    start = time.time()
    q = sanitize_user_input(question)
    if detect_prompt_injection(q):
        return {"error": "malicious input"}

    # Step 0: optionally collect web sources once to use for verification/scoring
    sources_text = ""
    retrieved = None
    if allow_web:
        try:
            retrieved = asyncio.get_event_loop().run_until_complete(
                multisearch_and_answer(q, use_local_model=False)
            )
            # build a short sources text snippet for prompts
            sources_text = "\n".join([s.get("url","") for s in retrieved.get("sources", [])][:TOP_K_SOURCES])
        except Exception as e:
            logger.info("ToT: web retrieval failed, continuing without web: %s", e)
            retrieved = None

    # Build candidate trees
    candidates = []
    for b in range(MAX_BRANCHES):
        steps = []
        for depth in range(MAX_DEPTH):
            prompt = _build_thought_prompt(q, steps)
            try:
                step_text = _safe_gen(prompt, max_tokens=64).strip().split("\n")[0]
            except Exception:
                step_text = ""  # tolerate generator failure
            if not step_text:
                break
            steps.append(step_text)
        # produce final answer for this branch
        ans_prompt = _build_answer_prompt(q, steps, include_sources=sources_text)
        try:
            final_answer = _safe_gen(ans_prompt, max_tokens=220).strip()
        except Exception:
            final_answer = ""
        candidates.append({"branch": b, "steps": steps, "final_answer": final_answer})

    # Score candidates
    scored = []
    for c in candidates:
        score = 0.0
        try:
            # if we have retrieved sources, compute confidence against them
            verify_sources = []
            if retrieved:
                # use the texts of top sources if available; multisearch returns 'answer' and 'sources' list
                for s in retrieved.get("sources", [])[:TOP_K_SOURCES]:
                    # some items may be {"url":..., "score":...} depending on implementation
                    if isinstance(s, dict) and s.get("url"):
                        verify_sources.append({"text": s.get("url")})  # fallback: url as text for scoring
            # call answer_confidence; fallback to heuristic length-based score
            if verify_sources:
                score = answer_confidence(c["final_answer"], verify_sources)
            else:
                # fallback heuristic: prefer longer step lists and longer answers (but cap)
                score = min(1.0, 0.1 * len(c["steps"]) + min(0.8, len(c["final_answer"]) / 200.0))
        except Exception as e:
            logger.info("ToT scoring error: %s", e)
            score = 0.0
        scored.append({**c, "score": float(score)})

    # pick best
    best = sorted(scored, key=lambda x: x["score"], reverse=True)[0]
    total_time = time.time() - start
    return {"question": q, "result": best, "candidates": scored, "retrieved": retrieved, "time_s": total_time}
# planner_executor.py
import json
import time
from typing import List, Dict, Any
from sanitize import sanitize_user_input, detect_prompt_injection
from model_loader import generate_sync
from multisearch_rag import multisearch_and_answer
from memory import recall, remember
from logger import logger

MAX_PLAN_STEPS = 6

PLANNER_PROMPT = """
You are Chanakya Planner. Given a user question, output a short JSON array of plan steps.
Each step is an object: {"type":"search"|"recall"|"compute"|"finish", "input":"..."}.
Keep plan concise and focused. Output ONLY the JSON array.
Question: {question}
"""

def plan_question(question: str) -> List[Dict[str, str]]:
    q = sanitize_user_input(question)
    prompt = PLANNER_PROMPT.format(question=q)
    raw = generate_sync(prompt, max_new_tokens=120)
    # try to extract JSON array from model output
    try:
        start = raw.find('[')
        end = raw.rfind(']') + 1
        arr = json.loads(raw[start:end])
        # validate and trim
        plan = []
        for s in arr[:MAX_PLAN_STEPS]:
            if isinstance(s, dict) and s.get("type"):
                plan.append({"type": s["type"], "input": s.get("input","")})
        return plan
    except Exception:
        logger.info("Planner failed to produce structured plan, fallback to single search step.")
        return [{"type":"search", "input": q}, {"type":"finish", "input": q}]

def execute_plan(plan: List[Dict[str,str]], question: str) -> Dict[str,Any]:
    observations = []
    final_answer = ""
    sources = []
    for step in plan:
        t = step.get("type")
        inp = step.get("input","")
        if t == "search":
            # call multisearch to get an answer fragment
            try:
                res = multisearch_and_answer(inp or question, use_local_model=False)
                # if returns directly an 'answer', append observation
                if isinstance(res, dict) and res.get("answer"):
                    observations.append({"type":"search", "value": res.get("answer")})
                    # collect sources for final citation
                    if res.get("sources"):
                        sources.extend([s.get("url") if isinstance(s, dict) else s for s in res.get("sources")])
            except Exception as e:
                observations.append({"type":"search", "value": f"error: {e}"})
        elif t == "recall":
            mem = recall()
            observations.append({"type":"recall", "value": mem})
        elif t == "compute":
            # restricted compute step â€” small math only
            try:
                # intentionally use Python eval with guard â€” reuse safe calculator from agent if available
                from agent import safe_calc
                calc = safe_calc(inp)
                observations.append({"type":"compute", "value": calc})
            except Exception as e:
                observations.append({"type":"compute", "value": f"error: {e}"})
        elif t == "finish":
            # build a short prompt to produce final answer using observations
            obs_text = "\n".join(f"- {o['type']}: {o['value']}" for o in observations[-6:])
            prompt = (
                "You are Chanakya. Using the observations below, produce a concise final answer to the question.\n"
                f"Observations:\n{obs_text}\n\nQuestion: {question}\nAnswer:"
            )
            final_answer = generate_sync(prompt, max_new_tokens=200)
            remember(question, final_answer)
            break
        else:
            observations.append({"type":"unknown", "value": f"unknown step {t}"})
    if not final_answer:
        # fallback: call a direct generate using observations
        obs_text = "\n".join(f"- {o['type']}: {o['value']}" for o in observations[-6:])
        prompt = f"You are Chanakya. Observations:\n{obs_text}\n\nQuestion: {question}\nAnswer:"
        final_answer = generate_sync(prompt, max_new_tokens=180)
        remember(question, final_answer)
    return {"answer": final_answer, "sources": list(dict.fromkeys(sources))}  # unique sources
# at top of main_secure.py add:
from tree_of_thoughts import run_tree_of_thoughts
from planner_executor import plan_question, execute_plan
# voice_api already included earlier in your main_secure

# POST /agent_trees
@app.post("/agent_trees")
async def agent_trees(body: dict = Body(...), token_payload=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    q = sanitize_user_input(body.get("query",""))
    if detect_prompt_injection(q) or not moderate_text(q):
        raise HTTPException(400, "disallowed or malicious")
    out = run_tree_of_thoughts(q, allow_web=True)
    # out["result"] contains 'final_answer' and 'score' etc.
    final = out.get("result", {}).get("final_answer", "")
    sources = out.get("retrieved", {}).get("sources", []) if out.get("retrieved") else []
    # Optionally produce TTS and return audio URL or base64 (here we return text and source list)
    return {"answer": final, "score": out.get("result", {}).get("score", 0.0), "sources": sources}

# POST /agent_plan_execute
@app.post("/agent_plan_execute")
async def agent_plan_execute(body: dict = Body(...), token_payload=Depends(require_jwt), _=Depends(rate_limit_dependency)):
    q = sanitize_user_input(body.get("query",""))
    if detect_prompt_injection(q) or not moderate_text(q):
        raise HTTPException(400, "disallowed or malicious")
    plan = plan_question(q)
    res = execute_plan(plan, q)
    return {"answer": res.get("answer",""), "sources": res.get("sources", [])}
// call agent trees
async function askTree() {
  setLoading(true);
  try {
    const r = await axios.post("http://localhost:8000/agent_trees", { query: q }, { headers: { Authorization: `Bearer ${JWT}` } });
    setAnswer(r.data.answer);
    // request TTS (ElevenLabs)
    const tts = await axios.post("http://localhost:8000/voice/speak_eleven", { text: r.data.answer }, { headers: { Authorization: `Bearer ${JWT}` }, responseType: "blob" });
    const url = URL.createObjectURL(new Blob([tts.data], { type: "audio/mpeg" }));
    setAudioUrl(url);
    new Audio(url).play();
  } finally { setLoading(false); }
}

// call planner+executor
async function askPlan() {
  setLoading(true);
  try {
    const r = await axios.post("http://localhost:8000/agent_plan_execute", { query: q }, { headers: { Authorization: `Bearer ${JWT}` } });
    setAnswer(r.data.answer);
    const tts = await axios.post("http://localhost:8000/voice/speak_eleven", { text: r.data.answer }, { headers: { Authorization: `Bearer ${JWT}` }, responseType: "blob" });
    const url = URL.createObjectURL(new Blob([tts.data], { type: "audio/mpeg" }));
    setAudioUrl(url);
    new Audio(url).play();
  } finally { setLoading(false); }
}
# tree_of_thoughts.py
import time
import json
import asyncio
from typing import List, Dict, Any
from logger import logger
from sanitize import sanitize_user_input, detect_prompt_injection
from model_loader import generate_sync
from verify import answer_confidence
from multisearch_rag import multisearch_and_answer

MAX_BRANCHES = 4
MAX_DEPTH = 3
TOP_K_SOURCES = 5

def _safe_gen(prompt: str, max_tokens: int = 120) -> str:
    try:
        return generate_sync(prompt, max_new_tokens=max_tokens)
    except Exception as e:
        logger.error("Generation failed: %s", e)
        return ""

def _build_thought_prompt(q: str, history: List[str]) -> str:
    base = "You are Chanakya-Agent. Write **one short reasoning step** (1 line) toward answering the question.\n"
    steps = "\n".join(f"- {s}" for s in history)
    return f"{base}Question: {q}\nPrevious steps:\n{steps}\nNext step:"

def _build_answer_prompt(q: str, steps: List[str], sources: str = "") -> str:
    s = "\n".join(f"- {x}" for x in steps)
    src = f"\nSources:\n{sources}" if sources else ""
    return f"You are Chanakya. Using these steps{src}\nQuestion: {q}\nAnswer concisely:\nSteps:\n{s}\n\nAnswer:"

def run_tree_of_thoughts(q: str, allow_web=True) -> Dict[str, Any]:
    q = sanitize_user_input(q)
    if detect_prompt_injection(q):
        return {"error": "malicious input"}

    sources_text, retrieved = "", None
    if allow_web:
        try:
            retrieved = asyncio.get_event_loop().run_until_complete(multisearch_and_answer(q, use_local_model=False))
            sources_text = "\n".join([s.get("url", "") for s in retrieved.get("sources", [])][:TOP_K_SOURCES])
        except Exception:
            pass

    branches = []
    for _ in range(MAX_BRANCHES):
        steps = []
        for _ in range(MAX_DEPTH):
            step = _safe_gen(_build_thought_prompt(q, steps), 64).strip().split("\n")[0]
            if not step:
                break
            steps.append(step)
        ans = _safe_gen(_build_answer_prompt(q, steps, sources_text), 220).strip()
        branches.append({"steps": steps, "final": ans})

    scored = []
    for c in branches:
        score = 0.0
        if retrieved:
            verify_sources = [{"text": s.get("url", "")} for s in retrieved.get("sources", [])[:TOP_K_SOURCES]]
            score = answer_confidence(c["final"], verify_sources)
        else:
            score = min(1.0, 0.2 * len(c["steps"]))
        scored.append({**c, "score": score})

    best = sorted(scored, key=lambda x: x["score"], reverse=True)[0]
    return {"result": best, "candidates": scored, "retrieved": retrieved}
# planner_executor.py
import json
from typing import List, Dict, Any
from sanitize import sanitize_user_input, detect_prompt_injection
from model_loader import generate_sync
from multisearch_rag import multisearch_and_answer
from memory import recall, remember
from logger import logger

PLANNER_PROMPT = """
You are Chanakya Planner. Given a user query, output a JSON array of plan steps.
Each step: {"type":"search"|"recall"|"compute"|"finish", "input":"..."}.
Only output JSON.
Question: {question}
"""

def plan_question(q: str) -> List[Dict[str, str]]:
    q = sanitize_user_input(q)
    prompt = PLANNER_PROMPT.format(question=q)
    raw = generate_sync(prompt, max_new_tokens=120)
    try:
        data = json.loads(raw[raw.find("["):raw.rfind("]")+1])
        return [s for s in data if isinstance(s, dict)]
    except Exception:
        logger.info("Planner fallback")
        return [{"type": "search", "input": q}, {"type": "finish", "input": q}]

def execute_plan(plan: List[Dict[str, str]], q: str) -> Dict[str, Any]:
    observations, final, sources = [], "", []
    for s in plan:
        t, inp = s.get("type"), s.get("input", "")
        if t == "search":
            res = multisearch_and_answer(inp or q, use_local_model=False)
            if res.get("answer"): observations.append(res["answer"])
            if res.get("sources"): sources.extend([x.get("url", "") for x in res["sources"]])
        elif t == "recall":
            observations.append(recall())
        elif t == "compute":
            try:
                result = eval(inp, {"__builtins__": {}})
                observations.append(str(result))
            except Exception:
                observations.append("error")
        elif t == "finish":
            prompt = f"Use these facts to answer concisely:\n{observations}\nQuestion: {q}\nAnswer:"
            final = generate_sync(prompt, max_new_tokens=180)
            remember(q, final)
            break
    if not final:
        prompt = f"Facts:\n{observations}\nQuestion: {q}\nAnswer:"
        final = generate_sync(prompt, max_new_tokens=160)
        remember(q, final)
    return {"answer": final, "sources": list(set(sources))}
from tree_of_thoughts import run_tree_of_thoughts
from planner_executor import plan_question, execute_plan
@app.post("/agent_trees")
async def agent_trees(body: dict = Body(...), token_payload=Depends(require_jwt)):
    q = sanitize_user_input(body.get("query", ""))
    if detect_prompt_injection(q) or not moderate_text(q):
        raise HTTPException(400, "unsafe query")
    out = run_tree_of_thoughts(q, allow_web=True)
    final = out.get("result", {}).get("final", "")
    return {"answer": final, "score": out.get("result", {}).get("score", 0)}

@app.post("/agent_plan_execute")
async def agent_plan_execute(body: dict = Body(...), token_payload=Depends(require_jwt)):
    q = sanitize_user_input(body.get("query", ""))
    if detect_prompt_injection(q) or not moderate_text(q):
        raise HTTPException(400, "unsafe query")
    plan = plan_question(q)
    res = execute_plan(plan, q)
    return {"answer": res["answer"], "sources": res["sources"]}
import axios from "axios";
import { useState } from "react";

export default function ChatUI() {
  const [q, setQ] = useState("");
  const [answer, setAnswer] = useState("");
  const [loading, setLoading] = useState(false);

  async function askAgent(type) {
    setLoading(true);
    const endpoint = type === "tree" ? "agent_trees" : "agent_plan_execute";
    try {
      const res = await axios.post(`http://localhost:8000/${endpoint}`, { query: q });
      setAnswer(res.data.answer);
      // voice playback
      const tts = await axios.post(`http://localhost:8000/voice/speak_eleven`, { text: res.data.answer }, { responseType: "blob" });
      const url = URL.createObjectURL(new Blob([tts.data], { type: "audio/mpeg" }));
      new Audio(url).play();
    } finally {
      setLoading(false);
    }
  }

  return (
    <div className="p-6 max-w-xl mx-auto">
      <h1 className="text-xl font-bold mb-3">Chanakya AI</h1>
      <textarea value={q} onChange={e => setQ(e.target.value)} className="w-full p-2 border rounded" rows="3" placeholder="Ask Chanakya..." />
      <div className="flex gap-2 mt-3">
        <button onClick={() => askAgent("tree")} className="px-3 py-2 bg-blue-600 text-white rounded">Tree-of-Thoughts</button>
        <button onClick={() => askAgent("plan")} className="px-3 py-2 bg-green-600 text-white rounded">Planner</button>
      </div>
      {loading && <p className="mt-2">Thinking...</p>}
      {answer && <p className="mt-4 text-gray-800"><b>Answer:</b> {answer}</p>}
    </div>
  );
}
# memory.py
import json
import time
from pathlib import Path
from model_loader import generate_sync

# File paths
MEMORY_FILE = Path("chanakya_memory.json")
LONG_TERM_FILE = Path("chanakya_longterm.json")

def load_memory():
    if MEMORY_FILE.exists():
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"history": []}

def save_memory(memory):
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(memory, f, indent=2, ensure_ascii=False)

def remember(user_input, ai_response):
    memory = load_memory()
    memory["history"].append({
        "timestamp": time.time(),
        "user": user_input,
        "chanakya": ai_response
    })
    # Keep only the last 50 entries
    if len(memory["history"]) > 50:
        memory["history"] = memory["history"][-50:]
    save_memory(memory)

def recall():
    memory = load_memory()
    return "\n".join(
        [f"User: {m['user']}\nChanakya: {m['chanakya']}" for m in memory["history"][-10:]]
    )

# --------------------------
# Long-term memory section
# --------------------------

def load_longterm():
    if LONG_TERM_FILE.exists():
        with open(LONG_TERM_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"insights": []}

def save_longterm(data):
    with open(LONG_TERM_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def reflect_and_learn():
    """Summarize recent interactions into long-term insights."""
    memory = load_memory()
    longterm = load_longterm()
    if not memory["history"]:
        return None

    last_few = "\n".join(
        [f"User: {m['user']}\nChanakya: {m['chanakya']}" for m in memory["history"][-10:]]
    )
    prompt = f"""
You are Chanakya, reflecting on your recent conversations.
Summarize the most important insights or lessons in 3 short bullet points.
Focus on understanding the user's interests, tone, and growth.

Conversations:
{last_few}

Now summarize key lessons:
"""
    try:
        summary = generate_sync(prompt, max_new_tokens=120)
        longterm["insights"].append({
            "timestamp": time.time(),
            "summary": summary.strip()
        })
        # Keep only the last 20 summaries
        longterm["insights"] = longterm["insights"][-20:]
        save_longterm(longterm)
        return summary
    except Exception as e:
        return f"Reflection failed: {e}"

def recall_longterm():
    data = load_longterm()
    all_insights = [x["summary"] for x in data.get("insights", [])]
    return "\n".join(all_insights[-5:]) if all_insights else ""
from memory import reflect_and_learn

# every 5th message, trigger reflection
if len(load_memory()["history"]) % 5 == 0:
    summary = reflect_and_learn()
    print("ðŸ§  Chanakya reflected:", summary)
from memory import recall_longterm

@app.get("/chanakya_insights")
async def get_longterm_insights(token_payload=Depends(require_jwt)):
    insights = recall_longterm()
    return {"insights": insights}
from memory import recall_longterm, remember
longterm_knowledge = recall_longterm()
context = f"""
You are Chanakya, reasoning using your past knowledge.
Here are your accumulated lessons:

{longterm_knowledge}

Now reason step by step about the current question.
"""
response = generate_sync(context + user_prompt, max_new_tokens=300)
remember(user_prompt, response)
from memory import recall_longterm, remember

def execute_plan(user_goal):
    wisdom = recall_longterm()
    planning_prompt = f"""
You are Chanakya creating a strategy.
Past wisdom:
{wisdom}

Goal:
{user_goal}

Make a practical, step-by-step plan using your prior lessons.
"""
    plan = generate_sync(planning_prompt, max_new_tokens=200)
    remember(user_goal, plan)
    return plan
from memory import recall_longterm

@app.on_event("startup")
async def load_ai_memory():
    print("ðŸ“œ Chanakya's past insights loaded:")
    print(recall_longterm())
User â†’ Think (Tree-of-Thoughts)
      â†˜ recall_longterm()
        â†˜ use wisdom + plan
          â†˜ remember() new learnings
            â†˜ reflect_and_learn() every few steps
# auth_admin.py
from fastapi import HTTPException, status, Depends
from auth_jwt import require_jwt

def require_admin(token_payload = Depends(require_jwt)):
    """
    Use as a dependency to restrict endpoints to admin users.
    `require_jwt` must return a dict-like payload that includes a 'role' key.
    For dev/testing you can create tokens that include {'sub': 'dev', 'role': 'admin'}.
    """
    try:
        if not isinstance(token_payload, dict):
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")
        role = token_payload.get("role") or token_payload.get("roles") or ""
        if role != "admin":
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Admin access required")
        return token_payload
    except Exception:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Admin access required")
# dashboard.py
import json
from fastapi import APIRouter, Depends, Request, Body, File, UploadFile, Response
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from auth_admin import require_admin
from auth_jwt import require_jwt
from memory import load_memory, save_memory, load_longterm, save_longterm, reflect_and_learn, recall_longterm
from logger import logger
from pathlib import Path

router = APIRouter()
templates = Jinja2Templates(directory="templates")

# Serve dashboard page (admin only)
@router.get("/dashboard", response_class=HTMLResponse, dependencies=[Depends(require_admin)])
async def dashboard_page(request: Request):
    return templates.TemplateResponse("dashboard.html", {"request": request})

# API: list short-term memory
@router.get("/api/memories", dependencies=[Depends(require_admin)])
def api_memories():
    mem = load_memory()
    # Attach indices for editing/deleting
    entries = mem.get("history", [])
    out = [{"idx": i, "timestamp": e.get("timestamp"), "user": e.get("user"), "chanakya": e.get("chanakya")} for i,e in enumerate(entries)]
    return JSONResponse(out)

# API: update a memory entry (edit user/chanakya text)
@router.post("/api/memory/edit", dependencies=[Depends(require_admin)])
def api_memory_edit(payload: dict = Body(...)):
    idx = int(payload.get("idx", -1))
    user_text = payload.get("user", "")
    ai_text = payload.get("chanakya", "")
    mem = load_memory()
    entries = mem.get("history", [])
    if idx < 0 or idx >= len(entries):
        return JSONResponse({"ok": False, "error": "index_out_of_range"}, status_code=400)
    if user_text:
        entries[idx]["user"] = user_text
    if ai_text:
        entries[idx]["chanakya"] = ai_text
    mem["history"] = entries
    save_memory(mem)
    return JSONResponse({"ok": True})

# API: delete a memory entry
@router.post("/api/memory/delete", dependencies=[Depends(require_admin)])
def api_memory_delete(payload: dict = Body(...)):
    idx = int(payload.get("idx", -1))
    mem = load_memory()
    entries = mem.get("history", [])
    if idx < 0 or idx >= len(entries):
        return JSONResponse({"ok": False, "error": "index_out_of_range"}, status_code=400)
    entries.pop(idx)
    mem["history"] = entries
    save_memory(mem)
    return JSONResponse({"ok": True})

# API: list long-term insights
@router.get("/api/insights", dependencies=[Depends(require_admin)])
def api_insights():
    lt = load_longterm()
    entries = lt.get("insights", [])
    out = [{"idx": i, "timestamp": e.get("timestamp"), "summary": e.get("summary")} for i,e in enumerate(entries)]
    return JSONResponse(out)

# API: delete an insight
@router.post("/api/insight/delete", dependencies=[Depends(require_admin)])
def api_insight_delete(payload: dict = Body(...)):
    idx = int(payload.get("idx", -1))
    lt = load_longterm()
    entries = lt.get("insights", [])
    if idx < 0 or idx >= len(entries):
        return JSONResponse({"ok": False, "error": "index_out_of_range"}, status_code=400)
    entries.pop(idx)
    lt["insights"] = entries
    save_longterm(lt)
    return JSONResponse({"ok": True})

# API: trigger reflection now (generate and save summary)
@router.post("/api/reflect", dependencies=[Depends(require_admin)])
def api_reflect():
    try:
        s = reflect_and_learn()
        return JSONResponse({"ok": True, "summary": s})
    except Exception as e:
        logger.error("Reflect failed: %s", e)
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

# API: clear all memories (admin destructive)
@router.post("/api/memory/clear", dependencies=[Depends(require_admin)])
def api_memory_clear():
    save_memory({"history": []})
    save_longterm({"insights": []})
    return JSONResponse({"ok": True})
<!-- templates/dashboard.html -->
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Chanakya â€” Wisdom Dashboard</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <meta name="viewport" content="width=device-width,initial-scale=1"/>
  <style>body { background: #0f172a; color: #e6eef8; }</style>
</head>
<body class="min-h-screen p-6">
  <div class="max-w-6xl mx-auto">
    <header class="flex items-center justify-between mb-6">
      <h1 class="text-3xl font-bold text-yellow-400">Chanakya â€” Wisdom Dashboard</h1>
      <div>
        <button id="btn-reflect" class="bg-green-600 hover:bg-green-700 px-3 py-2 rounded mr-2">Reflect Now</button>
        <button id="btn-clear" class="bg-red-600 hover:bg-red-700 px-3 py-2 rounded">Clear All</button>
      </div>
    </header>

    <section class="grid grid-cols-1 md:grid-cols-2 gap-6">
      <div class="bg-gray-800 p-4 rounded">
        <h2 class="text-xl font-semibold mb-2">Short-term Memories</h2>
        <div id="mem-list" class="space-y-3 max-h-[60vh] overflow-auto"></div>
      </div>

      <div class="bg-gray-800 p-4 rounded">
        <h2 class="text-xl font-semibold mb-2">Long-term Insights</h2>
        <div id="insights-list" class="space-y-3 max-h-[60vh] overflow-auto"></div>
      </div>
    </section>

    <div id="status" class="mt-6 text-sm text-gray-300"></div>
  </div>

<script>
const JWT = localStorage.getItem("JWT") || ""; // admin token must be stored here
if(!JWT) alert("No JWT found in localStorage. Save an admin JWT as localStorage.setItem('JWT','<token>')");

async function api(path, method='GET', body=null){
  const headers = { Authorization: `Bearer ${JWT}` };
  if(body && !(body instanceof FormData)) headers['Content-Type'] = 'application/json';
  const res = await fetch(path, { method, headers, body: body && !(body instanceof FormData) ? JSON.stringify(body) : body });
  if(!res.ok) {
    const text = await res.text();
    throw new Error(text || res.statusText);
  }
  return res.json();
}

function renderMem(mem){
  const div = document.createElement('div');
  div.className = "p-3 bg-gray-700 rounded";
  div.innerHTML = `
    <div class="text-xs text-gray-400">#${mem.idx} â€¢ ${new Date((mem.timestamp || Date.now())*1000).toLocaleString()}</div>
    <div class="mt-1"><b>User:</b><div class="mt-1 p-2 bg-gray-800 rounded" contenteditable="false" data-role="user">${escapeHtml(mem.user || '')}</div></div>
    <div class="mt-2"><b>Chanakya:</b><div class="mt-1 p-2 bg-gray-800 rounded" contenteditable="false" data-role="chanakya">${escapeHtml(mem.chanakya || '')}</div></div>
    <div class="mt-3 flex gap-2">
      <button class="edit-btn px-2 py-1 bg-yellow-500 rounded text-black">Edit</button>
      <button class="save-btn px-2 py-1 bg-green-600 rounded hidden">Save</button>
      <button class="delete-btn px-2 py-1 bg-red-600 rounded">Delete</button>
    </div>
  `;
  // attach handlers
  const editBtn = div.querySelector('.edit-btn');
  const saveBtn = div.querySelector('.save-btn');
  const deleteBtn = div.querySelector('.delete-btn');
  const userEl = div.querySelector('[data-role="user"]');
  const aiEl = div.querySelector('[data-role="chanakya"]');

  editBtn.onclick = () => {
    userEl.contentEditable = true;
    aiEl.contentEditable = true;
    userEl.classList.add('ring','ring-yellow-400');
    aiEl.classList.add('ring','ring-yellow-400');
    editBtn.classList.add('hidden');
    saveBtn.classList.remove('hidden');
  };
  saveBtn.onclick = async () => {
    try {
      const payload = { idx: mem.idx, user: userEl.innerText.trim(), chanakya: aiEl.innerText.trim() };
      await api('/api/memory/edit','POST', payload);
      status('Saved.');
      userEl.contentEditable = false; aiEl.contentEditable = false;
      editBtn.classList.remove('hidden'); saveBtn.classList.add('hidden');
      await loadMemories();
    } catch(e){ status('Error saving: '+e.message); }
  };
  deleteBtn.onclick = async () => {
    if(!confirm('Delete memory #'+mem.idx+' ?')) return;
    try{
      await api('/api/memory/delete','POST',{idx: mem.idx});
      status('Deleted.');
      await loadMemories();
    }catch(e){ status('Delete error: '+e.message); }
  };

  return div;
}

function renderInsight(ins){
  const div = document.createElement('div');
  div.className = "p-3 bg-gray-700 rounded";
  div.innerHTML = `
    <div class="text-xs text-gray-400">#${ins.idx} â€¢ ${new Date((ins.timestamp || Date.now())*1000).toLocaleString()}</div>
    <div class="mt-2">${escapeHtml(ins.summary || '')}</div>
    <div class="mt-3">
      <button class="del px-2 py-1 bg-red-600 rounded">Delete</button>
    </div>
  `;
  div.querySelector('.del').onclick = async () => {
    if(!confirm('Delete insight #'+ins.idx+'?')) return;
    await api('/api/insight/delete','POST',{idx: ins.idx});
    await loadInsights();
  };
  return div;
}

async function loadMemories(){
  try{
    const list = await api('/api/memories','GET');
    const container = document.getElementById('mem-list');
    container.innerHTML = '';
    for(const m of list){ container.appendChild(renderMem(m)); }
  }catch(e){ status('Load memories error: '+e.message); }
}

async function loadInsights(){
  try{
    const list = await api('/api/insights','GET');
    const container = document.getElementById('insights-list');
    container.innerHTML = '';
    for(const ins of list){ container.appendChild(renderInsight(ins)); }
  }catch(e){ status('Load insights error: '+e.message); }
}

function escapeHtml(unsafe){
  return unsafe.replaceAll('&','&amp;').replaceAll('<','&lt;').replaceAll('>','&gt;');
}

function status(msg){
  document.getElementById('status').innerText = msg;
  setTimeout(()=> { document.getElementById('status').innerText = ''; }, 3000);
}

document.getElementById('btn-reflect').onclick = async () => {
  if(!confirm('Run reflection now? This will call the model.')) return;
  status('Reflecting...');
  try {
    const r = await api('/api/reflect','POST');
    status('Reflection done.');
    await loadInsights();
  } catch(e){ status('Reflect failed: '+e.message); }
};
document.getElementById('btn-clear').onclick = async () => {
  if(!confirm('Clear all memory and insights? This is destructive.')) return;
  await api('/api/memory/clear','POST');
  await loadMemories(); await loadInsights();
  status('Cleared.');
};

// initial load
loadMemories();
loadInsights();
</script>
</body>
</html>
from dashboard import router as dashboard_router
app.include_router(dashboard_router)
# create_admin_token.py (run once, on a dev box)
import jwt, time, os
secret = os.getenv("JWT_SECRET","change_me")
payload = {"sub":"devadmin","role":"admin","iat": int(time.time()), "exp": int(time.time()) + 60*60*24}
token = jwt.encode(payload, secret, algorithm="HS256")
print(token)
localStorage.setItem("JWT", "<paste_token_here>");
chanakya_ai/
â”œâ”€ main_secure.py          # Your FastAPI app
â”œâ”€ auth_admin.py           # Admin check dependency
â”œâ”€ dashboard.py            # Routes + API
â”œâ”€ memory.py               # Memory system
â”œâ”€ templates/
â”‚   â””â”€ dashboard.html      # Dashboard UI
â”œâ”€ users.json              # Local admin users
â”œâ”€ audit_logs.json         # Stores audit events
â””â”€ db.sqlite               # SQLite DB for multi-admins
[
  {
    "timestamp": 1700000000,
    "user": "admin1",
    "action": "edit",
    "target": "memory #3"
  }
]
chanakya_ai/
â”œâ”€ main_secure.py          # FastAPI app
â”œâ”€ auth_admin.py           # Admin check dependency
â”œâ”€ dashboard.py            # Dashboard routes & API
â”œâ”€ memory.py               # Your memory system
â”œâ”€ oauth.py                # OAuth integration
â”œâ”€ users.json              # Local admin users
â”œâ”€ audit_logs.json         # Stores audit events
â”œâ”€ db.sqlite               # SQLite for multi-admins
â”œâ”€ templates/
â”‚   â””â”€ dashboard.html      # Dashboard UI
â”œâ”€ static/                 # Optional: CSS/JS if needed
# auth_admin.py
from fastapi import HTTPException, Depends, status
from auth_jwt import require_jwt
import sqlite3
import json, time, os

AUDIT_FILE = "audit_logs.json"

def log_audit(user, action, target=""):
    try:
        if os.path.exists(AUDIT_FILE):
            with open(AUDIT_FILE,"r",encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = []
        data.append({"timestamp": time.time(), "user": user, "action": action, "target": target})
        with open(AUDIT_FILE,"w",encoding="utf-8") as f:
            json.dump(data,f,indent=2)
    except Exception as e:
        print("Audit log error:", e)

def require_admin(token_payload=Depends(require_jwt)):
    """
    Checks JWT token payload for admin access.
    Logs access.
    """
    if not isinstance(token_payload, dict):
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Forbidden")
    role = token_payload.get("role") or ""
    username = token_payload.get("sub") or "unknown"
    if role != "admin":
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Admin access required")
    log_audit(username, "access_dashboard")
    return token_payload

# SQLite multi-admin verification
def verify_sqlite_admin(username, password_hash):
    conn = sqlite3.connect("db.sqlite")
    c = conn.cursor()
    c.execute("SELECT role FROM admins WHERE username=? AND password_hash=?", (username,password_hash))
    row = c.fetchone()
    conn.close()
    return row and row[0]=="admin"
# dashboard.py
import csv, io, json, time
from fastapi import APIRouter, Depends, Body
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from auth_admin import require_admin, log_audit
from memory import load_memory, save_memory, load_longterm, save_longterm, reflect_and_learn, recall_longterm

router = APIRouter()
templates = Jinja2Templates(directory="templates")

# Dashboard page
@router.get("/dashboard", response_class=HTMLResponse, dependencies=[Depends(require_admin)])
async def dashboard_page():
    return templates.TemplateResponse("dashboard.html", {"request": {}})

# API: Memories
@router.get("/api/memories", dependencies=[Depends(require_admin)])
def api_memories():
    mem = load_memory().get("history",[])
    out = [{"idx":i,"timestamp":m.get("timestamp"),"user":m.get("user"),"chanakya":m.get("chanakya")} for i,m in enumerate(mem)]
    return JSONResponse(out)

@router.post("/api/memory/edit", dependencies=[Depends(require_admin)])
def api_memory_edit(payload: dict = Body(...), token=Depends(require_admin)):
    idx = int(payload.get("idx",-1))
    mem = load_memory()
    entries = mem.get("history",[])
    if idx<0 or idx>=len(entries): return JSONResponse({"ok":False,"error":"index_out_of_range"},status_code=400)
    if "user" in payload: entries[idx]["user"]=payload["user"]
    if "chanakya" in payload: entries[idx]["chanakya"]=payload["chanakya"]
    save_memory(mem)
    log_audit(token["sub"], "edit_memory", f"memory #{idx}")
    return JSONResponse({"ok":True})

@router.post("/api/memory/delete", dependencies=[Depends(require_admin)])
def api_memory_delete(payload: dict = Body(...), token=Depends(require_admin)):
    idx = int(payload.get("idx",-1))
    mem = load_memory()
    entries = mem.get("history",[])
    if idx<0 or idx>=len(entries): return JSONResponse({"ok":False,"error":"index_out_of_range"},status_code=400)
    entries.pop(idx)
    mem["history"]=entries
    save_memory(mem)
    log_audit(token["sub"], "delete_memory", f"memory #{idx}")
    return JSONResponse({"ok":True})

# Insights
@router.get("/api/insights", dependencies=[Depends(require_admin)])
def api_insights():
    lt = load_longterm().get("insights",[])
    out = [{"idx":i,"timestamp":e.get("timestamp"),"summary":e.get("summary")} for i,e in enumerate(lt)]
    return JSONResponse(out)

@router.post("/api/insight/delete", dependencies=[Depends(require_admin)])
def api_insight_delete(payload: dict = Body(...), token=Depends(require_admin)):
    idx = int(payload.get("idx",-1))
    lt = load_longterm()
    entries = lt.get("insights",[])
    if idx<0 or idx>=len(entries): return JSONResponse({"ok":False,"error":"index_out_of_range"},status_code=400)
    entries.pop(idx)
    lt["insights"]=entries
    save_longterm(lt)
    log_audit(token["sub"], "delete_insight", f"insight #{idx}")
    return JSONResponse({"ok":True})

# Reflection
@router.post("/api/reflect", dependencies=[Depends(require_admin)])
def api_reflect(token=Depends(require_admin)):
    summary = reflect_and_learn()
    log_audit(token["sub"], "reflect")
    return JSONResponse({"ok":True,"summary":summary})

# Clear all
@router.post("/api/memory/clear", dependencies=[Depends(require_admin)])
def api_memory_clear(token=Depends(require_admin)):
    save_memory({"history":[]})
    save_longterm({"insights":[]})
    log_audit(token["sub"], "clear_all")
    return JSONResponse({"ok":True})

# CSV exports
@router.get("/api/export/memories", dependencies=[Depends(require_admin)])
def export_memories_csv():
    mem = load_memory().get("history",[])
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(["idx","timestamp","user","chanakya"])
    for i,m in enumerate(mem):
        writer.writerow([i,m.get("timestamp"),m.get("user"),m.get("chanakya")])
    output.seek(0)
    return StreamingResponse(io.BytesIO(output.getvalue().encode("utf-8-sig")), media_type="text/csv", headers={"Content-Disposition":"attachment; filename=memories.csv"})

@router.get("/api/export/insights", dependencies=[Depends(require_admin)])
def export_insights_csv():
    lt = load_longterm().get("insights",[])
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(["idx","timestamp","summary"])
    for i,e in enumerate(lt):
        writer.writerow([i,e.get("timestamp"),e.get("summary")])
    output.seek(0)
    return StreamingResponse(io.BytesIO(output.getvalue().encode("utf-8-sig")), media_type="text/csv", headers={"Content-Disposition":"attachment; filename=insights.csv"})
chanakya_ai/
â”œâ”€ main_secure.py
â”œâ”€ auth_admin.py
â”œâ”€ dashboard.py
â”œâ”€ memory.py
â”œâ”€ oauth.py               # OAuth logic
â”œâ”€ users.json             # Local admin users
â”œâ”€ db.sqlite              # SQLite multi-admin table
â”œâ”€ templates/
â”‚   â”œâ”€ dashboard.html
â”‚   â””â”€ login.html         # New login page
{
  "admins": [
    {
      "username": "chanakya",
      "password_hash": "$2b$12$Pz0p6xwQ9n/0kY9p8hKdSeFQO6iZpZCjXy/JF4lCBx1Y8mA1u0T8K" 
      // bcrypt hash of your password
    }
  ]
}
import bcrypt
password = b"yourpassword"
hashed = bcrypt.hashpw(password, bcrypt.gensalt())
print(hashed.decode())
# oauth.py
from fastapi import APIRouter, Request
from fastapi.responses import RedirectResponse, JSONResponse
from auth_jwt import create_access_token
from auth_admin import log_audit
from authlib.integrations.starlette_client import OAuth
import os

router = APIRouter()
oauth = OAuth()
oauth.register(
    name='google',
    client_id=os.getenv("GOOGLE_CLIENT_ID"),
    client_secret=os.getenv("GOOGLE_CLIENT_SECRET"),
    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',
    client_kwargs={'scope': 'openid email profile'}
)
oauth.register(
    name='github',
    client_id=os.getenv("GITHUB_CLIENT_ID"),
    client_secret=os.getenv("GITHUB_CLIENT_SECRET"),
    access_token_url='https://github.com/login/oauth/access_token',
    authorize_url='https://github.com/login/oauth/authorize',
    client_kwargs={'scope': 'read:user user:email'}
)

@router.get("/login/oauth/{provider}")
async def oauth_login(provider: str, request: Request):
    if provider not in ["google","github"]: return JSONResponse({"error":"invalid provider"},status_code=400)
    redirect_uri = request.url_for("oauth_callback", provider=provider)
    return await oauth.create_client(provider).authorize_redirect(request, redirect_uri)

@router.get("/login/oauth/callback/{provider}")
async def oauth_callback(provider: str, request: Request):
    token = await oauth.create_client(provider).authorize_access_token(request)
    user_info = await oauth.create_client(provider).parse_id_token(request, token)
    username = user_info.get("email") or user_info.get("login")
    jwt_token = create_access_token({"sub": username,"role":"admin"})
    log_audit(username, f"oauth_login_{provider}")
    response = RedirectResponse("/dashboard")
    response.set_cookie("access_token", f"Bearer {jwt_token}", httponly=True, secure=True)
    return response
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Chanakya AI Login</title>
<script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="flex justify-center items-center h-screen bg-gray-900 text-white">
<div class="w-full max-w-md p-6 bg-gray-800 rounded shadow">
<h2 class="text-2xl mb-4 font-bold text-yellow-400">Chanakya AI Admin Login</h2>
<form id="loginForm" class="space-y-4">
  <input type="text" id="username" placeholder="Username" class="w-full p-2 rounded bg-gray-700"/>
  <input type="password" id="password" placeholder="Password" class="w-full p-2 rounded bg-gray-700"/>
  <button type="submit" class="w-full bg-yellow-500 hover:bg-yellow-600 p-2 rounded font-bold">Login</button>
</form>
<div class="mt-4 flex justify-between">
  <a href="/login/oauth/google" class="bg-red-600 hover:bg-red-700 px-3 py-2 rounded">Login with Google</a>
  <a href="/login/oauth/github" class="bg-gray-600 hover:bg-gray-700 px-3 py-2 rounded">Login with GitHub</a>
</div>
<div id="status" class="mt-3 text-red-400"></div>
</div>

<script>
document.getElementById("loginForm").onsubmit = async (e) => {
    e.preventDefault();
    const u = document.getElementById("username").value;
    const p = document.getElementById("password").value;
    const res = await fetch("/login/local",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify({username:u,password:p})
    });
    const data = await res.json();
    if(data.ok){
        localStorage.setItem("JWT",data.token);
        window.location="/dashboard";
    } else {
        document.getElementById("status").innerText = data.error;
    }
};
</script>
</body>
</html>
from fastapi import FastAPI, Request, Body
from fastapi.responses import JSONResponse, HTMLResponse, RedirectResponse
from auth_admin import log_audit
import json, bcrypt
from auth_jwt import create_access_token
from oauth import router as oauth_router
import os

app = FastAPI()
app.include_router(oauth_router)

USERS_FILE = "users.json"

@app.get("/login", response_class=HTMLResponse)
async def login_page():
    with open("templates/login.html","r",encoding="utf-8") as f:
        return HTMLResponse(f.read())

@app.post("/login/local")
async def login_local(payload: dict = Body(...)):
    username = payload.get("username")
    password = payload.get("password","").encode()
    if not os.path.exists(USERS_FILE):
        return JSONResponse({"ok":False,"error":"users.json missing"})
    with open(USERS_FILE,"r",encoding="utf-8") as f:
        users = json.load(f).get("admins",[])
    user = next((u for u in users if u["username"]==username),None)
    if not user:
        return JSONResponse({"ok":False,"error":"Invalid username"})
    if not bcrypt.checkpw(password,user["password_hash"].encode()):
        return JSONResponse({"ok":False,"error":"Invalid password"})
    token = create_access_token({"sub":username,"role":"admin"})
    log_audit(username,"local_login")
    return JSONResponse({"ok":True,"token":token})
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Chanakya AI â€” Wisdom Dashboard</title>
<script src="https://cdn.tailwindcss.com"></script>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<style>
body { background: #0f172a; color: #e6eef8; }
<
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Chanakya AI â€” Wisdom Dashboard</title>
<script src="https://cdn.tailwindcss.com"></script>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<style>
body { background: #0f172a; color: #e6eef8; }
</style>
</head>
<body class="min-h-screen p-6">
<div class="max-w-7xl mx-auto">

<header class="flex flex-col md:flex-row items-center justify-between mb-6">
  <h1 class="text-3xl font-bold text-yellow-400 mb-4 md:mb-0">Chanakya AI â€” Wisdom Dashboard</h1>
  <div class="flex gap-2 flex-wrap">
    <button id="btn-reflect" class="bg-green-600 hover:bg-green-700 px-3 py-2 rounded">Reflect Now</button>
    <button id="btn-clear" class="bg-red-600 hover:bg-red-700 px-3 py-2 rounded">Clear All</button>
    <button id="btn-export-mem" class="bg-blue-600 hover:bg-blue-700 px-3 py-2 rounded">Export Memories CSV</button>
    <button id="btn-export-ins" class="bg-blue-600 hover:bg-blue-700 px-3 py-2 rounded">Export Insights CSV</button>
  </div>
</header>

<section class="grid grid-cols-1 md:grid-cols-3 gap-6">

  <!-- Memories -->
  <div class="bg-gray-800 p-4 rounded">
    <h2 class="text-xl font-semibold mb-2">Short-term Memories</h2>
    <div id="mem-list" class="space-y-3 max-h-[60vh] overflow-auto"></div>
  </div>

  <!-- Insights -->
  <div class="bg-gray-800 p-4 rounded">
    <h2 class="text-xl font-semibold mb-2">Long-term Insights</h2>
    <div id="insights-list" class="space-y-3 max-h-[60vh] overflow-auto"></div>
  </div>

  <!-- Audit Logs -->
  <div class="bg-gray-800 p-4 rounded">
    <h2 class="text-xl font-semibold mb-2">Audit Logs</h2>
    <div id="audit-list" class="space-y-2 max-h-[60vh] overflow-auto text-xs text-gray-300"></div>
  </div>

</section>

<div id="status" class="mt-6 text-sm text-gray-300"></div>
</div>

<script>
const JWT = localStorage.getItem("JWT") || "";
if(!JWT) alert("No JWT found. Please log in first.");

async function api(path, method='GET', body=null){
  const headers = { Authorization: `Bearer ${JWT}` };
  if(body && !(body instanceof FormData)) headers['Content-Type'] = 'application/json';
  const res = await fetch(path, { method, headers, body: body && !(body instanceof FormData) ? JSON.stringify(body) : body });
  if(!res.ok) {
    const text = await res.text();
    throw new Error(text || res.statusText);
  }
  return res.json();
}

function escapeHtml(unsafe){
  return unsafe.replaceAll('&','&amp;').replaceAll('<','&lt;').replaceAll('>','&gt;');
}

function status(msg){
  document.getElementById("status").innerText = msg;
  setTimeout(()=>{ document.getElementById("status").innerText='';},3000);
}

// Render memory
function renderMem(mem){
  const div = document.createElement('div');
  div.className = "p-3 bg-gray-700 rounded";
  div.innerHTML = `
    <div class="text-xs text-gray-400">#${mem.idx} â€¢ ${new Date((mem.timestamp||Date.now())*1000).toLocaleString()}</div>
    <div class="mt-1"><b>User:</b><div class="mt-1 p-2 bg-gray-800 rounded" contenteditable="false" data-role="user">${escapeHtml(mem.user||'')}</div></div>
    <div class="mt-2"><b>Chanakya:</b><div class="mt-1 p-2 bg-gray-800 rounded" contenteditable="false" data-role="chanakya">${escapeHtml(mem.chanakya||'')}</div></div>
    <div class="mt-3 flex gap-2">
      <button class="edit-btn px-2 py-1 bg-yellow-500 rounded text-black">Edit</button>
      <button class="save-btn px-2 py-1 bg-green-600 rounded hidden">Save</button>
      <button class="delete-btn px-2 py-1 bg-red-600 rounded">Delete</button>
    </div>
  `;
  const editBtn = div.querySelector('.edit-btn');
  const saveBtn = div.querySelector('.save-btn');
  const deleteBtn = div.querySelector('.delete-btn');
  const userEl = div.querySelector('[data-role="user"]');
  const aiEl = div.querySelector('[data-role="chanakya"]');

  editBtn.onclick = ()=>{
    userEl.contentEditable=true; aiEl.contentEditable=true;
    userEl.classList.add('ring','ring-yellow-400');
    aiEl.classList.add('ring','ring-yellow-400');
    editBtn.classList.add('hidden'); saveBtn.classList.remove('hidden');
  };
  saveBtn.onclick = async ()=>{
    try{
      const payload = { idx: mem.idx, user: userEl.innerText.trim(), chanakya: aiEl.innerText.trim() };
      await api('/api/memory/edit','POST',payload);
      status('Memory saved.');
      userEl.contentEditable=false; aiEl.contentEditable=false;
      editBtn.classList.remove('hidden'); saveBtn.classList.add('hidden');
      await loadMemories(); await loadAudit();
    } catch(e){ status('Error: '+e.message); }
  };
  deleteBtn.onclick = async ()=>{
    if(!confirm('Delete memory #'+mem.idx+'?')) return;
    await api('/api/memory/delete','POST',{idx:mem.idx});
    status('Deleted.');
    await loadMemories(); await loadAudit();
  };
  return div;
}

// Render insight
function renderInsight(ins){
  const div = document.createElement('div');
  div.className="p-3 bg-gray-700 rounded";
  div.innerHTML=`<div class="text-xs text-gray-400">#${ins.idx} â€¢ ${new Date((ins.timestamp||Date.now())*1000).toLocaleString()}</div>
                 <div class="mt-2">${escapeHtml(ins.summary||'')}</div>
                 <div class="mt-3"><button class="del px-2 py-1 bg-red-600 rounded">Delete</button></div>`;
  div.querySelector('.del').onclick=async ()=>{
    if(!confirm('Delete insight #'+ins.idx+'?')) return;
    await api('/api/insight/delete','POST',{idx:ins.idx});
    status('Deleted.');
    await loadInsights(); await loadAudit();
  };
  return div;
}

// Render audit log
function renderAudit(a){
  const div = document.createElement('div');
  div.textContent = `[${new Date(a.timestamp*1000).toLocaleString()}] ${a.user} â†’ ${a.action} ${a.target||''}`;
  return div;
}

// Load all
async function loadMemories(){ const list = await api('/api/memories'); const c=document.getElementById('mem-list'); c.innerHTML=''; list.forEach(m=>c.appendChild(renderMem(m))); }
async function loadInsights(){ const list = await api('/api/insights'); const c=document.getElementById('insights-list'); c.innerHTML=''; list.forEach(i=>c.appendChild(renderInsight(i))); }
async function loadAudit(){ const list = await api('/api/audit'); const c=document.getElementById('audit-list'); c.innerHTML=''; list.forEach(a=>c.appendChild(renderAudit(a))); }

// Buttons
document.getElementById('btn-reflect').onclick=async ()=>{
  if(!confirm('Run reflection now?')) return;
  status('Reflecting...');
  await api('/api/reflect','POST'); status('Reflection done.');
  await loadInsights(); await loadAudit();
};
document.getElementById('btn-clear').onclick=async ()=>{
  if(!confirm('Clear all memory and insights?')) return;
  await api('/api/memory/clear','POST');
  status('Cleared all.');
  await loadMemories(); await loadInsights(); await loadAudit();
};
document.getElementById('btn-export-mem').onclick=()=>window.open('/api/export/memories','_blank');
document.getElementById('btn-export-ins').onclick=()=>window.open('/api/export/insights','_blank');

// Initial load
loadMemories(); loadInsights(); loadAudit();
</script>
</body>
</html>
Query â†’ Memory search â†’ Planner â†’ LLM reasoning â†’ Answer â†’ Reflect â†’ Store insight
chanakya_ai_2/
â”œâ”€ main.py               # FastAPI app
â”œâ”€ memory.py             # Short-term & long-term memory + embeddings
â”œâ”€ reasoning.py          # Planner / multi-step reasoning
â”œâ”€ websearch.py          # Async search / aggregation
â”œâ”€ vector_db.py          # FAISS vector DB integration
â”œâ”€ auth_admin.py         # JWT + OAuth + audit logs
â”œâ”€ dashboard.py          # Dashboard endpoints
â”œâ”€ oauth.py              # OAuth login
â”œâ”€ users.json            # Local admins
â”œâ”€ db.sqlite             # Multi-admin DB
â”œâ”€ templates/
â”‚   â”œâ”€ login.html
â”‚   â””â”€ dashboard.html
â””â”€ requirements.txt
# memory.py
import json, os, time
from sentence_transformers import SentenceTransformer
import numpy as np

SHORT_TERM_FILE = "memories.json"
LONG_TERM_FILE = "insights.json"

# Load embedding model
embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight, fast

def load_memory():
    if os.path.exists(SHORT_TERM_FILE):
        with open(SHORT_TERM_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"history": []}

def save_memory(mem):
    with open(SHORT_TERM_FILE, "w", encoding="utf-8") as f:
        json.dump(mem, f, indent=2)

def load_longterm():
    if os.path.exists(LONG_TERM_FILE):
        with open(LONG_TERM_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"insights": []}

def save_longterm(lt):
    with open(LONG_TERM_FILE, "w", encoding="utf-8") as f:
        json.dump(lt, f, indent=2)

def add_memory(user_text, chanakya_text):
    mem = load_memory()
    mem["history"].append({
        "timestamp": time.time(),
        "user": user_text,
        "chanakya": chanakya_text,
        "embedding": embed_model.encode(chanakya_text).tolist()
    })
    save_memory(mem)

def add_insight(summary_text):
    lt = load_longterm()
    lt["insights"].append({
        "timestamp": time.time(),
        "summary": summary_text,
        "embedding": embed_model.encode(summary_text).tolist()
    })
    save_longterm(lt)

def semantic_search(query, top_k=3):
    """Search memories by semantic similarity"""
    mem = load_memory()["history"]
    if not mem: return []
    query_emb = embed_model.encode(query)
    results = []
    for i, m in enumerate(mem):
        sim = np.dot(query_emb, np.array(m["embedding"])) / (
            np.linalg.norm(query_emb)*np.linalg.norm(np.array(m["embedding"])) + 1e-6)
        results.append((sim, i, m))
    results.sort(reverse=True)
    return [r[2] for r in results[:top_k]]
# reasoning.py
from memory import semantic_search, add_insight
import random

def plan_response(query):
    """Multi-step reasoning with memory + reflection"""
    relevant_memories = semantic_search(query, top_k=3)
    # Combine memories to formulate answer
    answer = " ".join([m["chanakya"] for m in relevant_memories])
    # Add random wisdom to emulate Chanakya style
    wisdom = [
        "A person who is overly cautious never accomplishes greatness.",
        "Knowledge is the foundation of strategy and success.",
        "Opportunistic action is the hallmark of the wise."
    ]
    answer += " " + random.choice(wisdom)
    # Save as new insight
    add_insight(answer)
    return answer
# websearch.py
import asyncio, aiohttp
from bs4 import BeautifulSoup

async def fetch(session, url):
    try:
        async with session.get(url, timeout=5) as r:
            text = await r.text()
            soup = BeautifulSoup(text, 'html.parser')
            return soup.get_text()[:1000]  # first 1000 chars
    except:
        return ""

async def multi_search(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
    return results

def search_web(query):
    # Example: search 3 predefined sites
    urls = [
        f"https://en.wikipedia.org/wiki/{query.replace(' ','_')}",
        f"https://www.brainyquote.com/search_results?q={query}",
        f"https://www.history.com/search?q={query}"
    ]
    results = asyncio.run(multi_search(urls))
    return " ".join(results)
# vector_db.py
import faiss, numpy as np
from memory import load_memory

INDEX_FILE = "faiss_index.index"
DIM = 384  # embedding dimension of all-MiniLM-L6-v2

def build_index():
    mem = load_memory()["history"]
    if not mem: return None
    index = faiss.IndexFlatL2(DIM)
    embeddings = np.array([m["embedding"] for m in mem]).astype('float32')
    index.add(embeddings)
    faiss.write_index(index, INDEX_FILE)
    return index

def search_index(query_emb, top_k=3):
    index = faiss.read_index(INDEX_FILE)
    D, I = index.search(np.array([query_emb]).astype('float32'), top_k)
    mem = load_memory()["history"]
    return [mem[i] for i in I[0]]
# main.py
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from reasoning import plan_response
from websearch import search_web
from memory import add_memory

app = FastAPI()

@app.get("/ask/{query}")
async def ask_chanakya(query: str):
    # Step 1: Search memory
    answer_mem = plan_response(query)
    # Step 2: Search web (optional)
    web_text = search_web(query)
    # Combine memory + web knowledge
    answer = answer_mem + "\nWeb info:\n" + web_text[:500]  # first 500 chars
    # Save new memory
    add_memory(query, answer)
    return JSONResponse({"answer": answer})
chanakya_ai_3/
â”œâ”€ main.py               # FastAPI main app
â”œâ”€ memory.py             # Memory & embeddings
â”œâ”€ reasoning.py          # Planner / multi-step reasoning
â”œâ”€ websearch.py          # Async web search
â”œâ”€ vector_db.py          # FAISS vector DB
â”œâ”€ tts.py                # Chanakya deep voice
â”œâ”€ auth_admin.py         # JWT + OAuth + audit logs
â”œâ”€ oauth.py              # OAuth login
â”œâ”€ dashboard.py          # Dashboard endpoints
â”œâ”€ users.json            # Local admins
â”œâ”€ db.sqlite             # Multi-admin DB
â”œâ”€ templates/
â”‚   â”œâ”€ login.html
â”‚   â””â”€ dashboard.html
â””â”€ requirements.txt
# tts.py
import pyttsx3

engine = pyttsx3.init()
engine.setProperty('rate', 130)  # slower pace for deep wisdom
engine.setProperty('volume', 1.0)

# Deep male voice (select first male voice available)
voices = engine.getProperty('voices')
for v in voices:
    if "male" in v.name.lower() or "english" in v.name.lower():
        engine.setProperty('voice', v.id)
        break

def speak(text):
    """Speak text in Chanakya deep voice"""
    engine.say(text)
    engine.runAndWait()
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, HTMLResponse, RedirectResponse
from reasoning import plan_response
from websearch import search_web
from memory import add_memory
from tts import speak
from dashboard import get_dashboard_html
from auth_admin import auth_required

app = FastAPI()

# Dashboard route (secure)
@app.get("/dashboard")
@auth_required
async def dashboard(request: Request):
    html = get_dashboard_html()
    return HTMLResponse(html)

# Ask Chanakya (text input)
@app.get("/ask/{query}")
@auth_required
async def ask_chanakya(query: str):
    # Memory reasoning
    answer_mem = plan_response(query)
    # Web aggregation
    web_text = search_web(query)
    answer = answer_mem + "\nWeb info:\n" + web_text[:500]
    # Store memory
    add_memory(query, answer)
    # Speak answer (TTS)
    speak(answer)
    return JSONResponse({"answer": answer})
# dashboard.py
def get_dashboard_html():
    with open("templates/dashboard.html","r",encoding="utf-8") as f:
        return f.read()
fastapi
uvicorn[standard]
jinja2
sentence-transformers
numpy
faiss-cpu
aiohttp
beautifulsoup4
pyttsx3
bcrypt
python-jose
authlib
sqlite3
chanakya_ai/
â”œâ”€ main.py
â”œâ”€ memory.py
â”œâ”€ reasoning.py
â”œâ”€ websearch.py
â”œâ”€ vector_db.py
â”œâ”€ tts.py
â”œâ”€ auth_admin.py
â”œâ”€ oauth.py
â”œâ”€ dashboard.py
â”œâ”€ users.json
â”œâ”€ db.sqlite
â”œâ”€ templates/
â”‚   â”œâ”€ login.html
â”‚   â””â”€ dashboard.html
â”œâ”€ requirements.txt
# memory.py
import json, os, time
from sentence_transformers import SentenceTransformer
import numpy as np

SHORT_TERM_FILE = "memories.json"
LONG_TERM_FILE = "insights.json"

embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # fast & small

def load_memory():
    if os.path.exists(SHORT_TERM_FILE):
        with open(SHORT_TERM_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    return {"history":[]}

def save_memory(mem):
    with open(SHORT_TERM_FILE,"w",encoding="utf-8") as f:
        json.dump(mem,f,indent=2)

def load_longterm():
    if os.path.exists(LONG_TERM_FILE):
        with open(LONG_TERM_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    return {"insights":[]}

def save_longterm(lt):
    with open(LONG_TERM_FILE,"w",encoding="utf-8") as f:
        json.dump(lt,f,indent=2)

def add_memory(user_text, chanakya_text):
    mem = load_memory()
    mem["history"].append({
        "timestamp": time.time(),
        "user": user_text,
        "chanakya": chanakya_text,
        "embedding": embed_model.encode(chanakya_text).tolist()
    })
    save_memory(mem)

def add_insight(summary_text):
    lt = load_longterm()
    lt["insights"].append({
        "timestamp": time.time(),
        "summary": summary_text,
        "embedding": embed_model.encode(summary_text).tolist()
    })
    save_longterm(lt)

def semantic_search(query, top_k=3):
    mem = load_memory()["history"]
    if not mem: return []
    query_emb = embed_model.encode(query)
    results=[]
    for m in mem:
        sim = np.dot(query_emb,np.array(m["embedding"]))/(np.linalg.norm(query_emb)*np.linalg.norm(np.array(m["embedding"]))+1e-6)
        results.append((sim,m))
    results.sort(reverse=True,key=lambda x:x[0])
    return [r[1] for r in results[:top_k]]
# reasoning.py
from memory import semantic_search, add_insight
import random

def plan_response(query):
    mems = semantic_search(query,3)
    answer = " ".join([m["chanakya"] for m in mems])
    wisdom = [
        "A person who is overly cautious never accomplishes greatness.",
        "Knowledge is the foundation of strategy and success.",
        "Opportunistic action is the hallmark of the wise."
    ]
    answer += " " + random.choice(wisdom)
    add_insight(answer)
    return answer
# websearch.py
import asyncio, aiohttp
from bs4 import BeautifulSoup

async def fetch(session,url):
    try:
        async with session.get(url,timeout=5) as r:
            text = await r.text()
            soup = BeautifulSoup(text,'html.parser')
            return soup.get_text()[:1000]
    except:
        return ""

async def multi_search(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session,u) for u in urls]
        return await asyncio.gather(*tasks)

def search_web(query):
    urls=[
        f"https://en.wikipedia.org/wiki/{query.replace(' ','_')}",
        f"https://www.history.com/search?q={query}",
        f"https://www.brainyquote.com/search_results?q={query}"
    ]
    return " ".join(asyncio.run(multi_search(urls)))
# tts.py
import pyttsx3
engine = pyttsx3.init()
engine.setProperty('rate',130)
engine.setProperty('volume',1.0)
voices = engine.getProperty('voices')
for v in voices:
    if "male" in v.name.lower() or "english" in v.name.lower():
        engine.setProperty('voice',v.id)
        break
def speak(text):
    engine.say(text)
    engine.runAndWait()
# main.py
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, HTMLResponse
from reasoning import plan_response
from websearch import search_web
from memory import add_memory
from tts import speak
from dashboard import get_dashboard_html
from auth_admin import auth_required

app = FastAPI()

@app.get("/dashboard")
@auth_required
async def dashboard(request: Request):
    return HTMLResponse(get_dashboard_html())

@app.get("/ask/{query}")
@auth_required
async def ask(query: str):
    answer_mem = plan_response(query)
    web_text = search_web(query)
    answer = answer_mem + "\nWeb info:\n" + web_text[:500]
    add_memory(query,answer)
    speak(answer)
    return JSONResponse({"answer":answer})

# dashboard.py
def get_dashboard_html():
    with open("templates/dashboard.html","r",encoding="utf-8") as f:
        return f.read()
pip install -r requirements.txt
uvicorn main:app --reload
mkdir chanakya_ai
cd chanakya_ai
mkdir chanakya_ai
cd chanakya_ai
python -m venv venv
source venv/bin/activate    # Linux/Mac
venv\Scripts\activate       # Windows
fastapi
uvicorn[standard]
jinja2
sentence-transformers
numpy
faiss-cpu
aiohttp
beautifulsoup4
pyttsx3
bcrypt
python-jose
authlib
sqlite3
pip install -r requirements.txt
chanakya_ai/
â”œâ”€ main.py
â”œâ”€ memory.py
â”œâ”€ reasoning.py
â”œâ”€ websearch.py
â”œâ”€ vector_db.py
â”œâ”€ tts.py
â”œâ”€ auth_admin.py
â”œâ”€ oauth.py
â”œâ”€ dashboard.py
â”œâ”€ users.json
â”œâ”€ db.sqlite
â”œâ”€ templates/
â”‚   â”œâ”€ login.html
â”‚   â””â”€ dashboard.html
â””â”€ requirements.txt
# memory.py (as given before)
# add_memory(user_text, chanakya_text)
# add_insight(summary_text)
# semantic_search(query, top_k=3)
# reasoning.py (as given before)
# plan_response(query)
# websearch.py (as given before)
# search_web(query)
# tts.py (as given before)
# speak(text)
# vector_db.py (as given before)
# build_index()
# search_index(query_emb, top_k=3)
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, HTMLResponse
from reasoning import plan_response
from websearch import search_web
from memory import add_memory
from tts import speak
from dashboard import get_dashboard_html
from auth_admin import auth_required

app = FastAPI()

@app.get("/dashboard")
@auth_required
async def dashboard(request: Request):
    return HTMLResponse(get_dashboard_html())

@app.get("/ask/{query}")
@auth_required
async def ask(query: str):
    answer_mem = plan_response(query)
    web_text = search_web(query)
    answer = answer_mem + "\nWeb info:\n" + web_text[:500]
    add_memory(query, answer)
    speak(answer)
    return JSONResponse({"answer": answer})
uvicorn main:app --reload
http://127.0.0.1:8000/dashboard
chanakya_ai/
â”œâ”€ main.py               # FastAPI main app
â”œâ”€ memory.py             # Memory & embeddings
â”œâ”€ reasoning.py          # Planner / multi-step reasoning
â”œâ”€ websearch.py          # Async web search
â”œâ”€ vector_db.py          # FAISS vector DB
â”œâ”€ tts.py                # Chanakya deep voice TTS
â”œâ”€ auth_admin.py         # JWT + OAuth + audit logs
â”œâ”€ oauth.py              # OAuth login
â”œâ”€ dashboard.py          # Dashboard HTML loader
â”œâ”€ users.json            # Admin users
â”œâ”€ db.sqlite             # Admin database
â”œâ”€ templates/
â”‚   â”œâ”€ login.html
â”‚   â””â”€ dashboard.html
â”œâ”€ requirements.txt      # Python dependencies
fastapi
uvicorn[standard]
jinja2
sentence-transformers
numpy
faiss-cpu
aiohttp
beautifulsoup4
pyttsx3
bcrypt
python-jose
authlib
sqlite3
pip install -r requirements.txt
pip install -r requirements.txt
import json, os, time
from sentence_transformers import SentenceTransformer
import numpy as np

SHORT_TERM_FILE = "memories.json"
LONG_TERM_FILE = "insights.json"

embed_model = SentenceTransformer('all-MiniLM-L6-v2')

def load_memory():
    if os.path.exists(SHORT_TERM_FILE):
        with open(SHORT_TERM_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    return {"history":[]}

def save_memory(mem):
    with open(SHORT_TERM_FILE,"w",encoding="utf-8") as f:
        json.dump(mem,f,indent=2)

def load_longterm():
    if os.path.exists(LONG_TERM_FILE):
        with open(LONG_TERM_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    return {"insights":[]}

def save_longterm(lt):
    with open(LONG_TERM_FILE,"w",encoding="utf-8") as f:
        json.dump(lt,f,indent=2)

def add_memory(user_text, chanakya_text):
    mem = load_memory()
    mem["history"].append({
        "timestamp": time.time(),
        "user": user_text,
        "chanakya": chanakya_text,
        "embedding": embed_model.encode(chanakya_text).tolist()
    })
    save_memory(mem)

def add_insight(summary_text):
    lt = load_longterm()
    lt["insights"].append({
        "timestamp": time.time(),
        "summary": summary_text,
        "embedding": embed_model.encode(summary_text).tolist()
    })
    save_longterm(lt)

def semantic_search(query, top_k=3):
    mem = load_memory()["history"]
    if not mem: return []
    query_emb = embed_model.encode(query)
    results=[]
    for m in mem:
        sim = np.dot(query_emb,np.array(m["embedding"]))/(np.linalg.norm(query_emb)*np.linalg.norm(np.array(m["embedding"]))+1e-6)
        results.append((sim,m))
    results.sort(reverse=True,key=lambda x:x[0])
    return [r[1] for r in results[:top_k]]
from memory import semantic_search, add_insight
import random

def plan_response(query):
    mems = semantic_search(query,3)
    answer = " ".join([m["chanakya"] for m in mems])
    wisdom = [
        "A person who is overly cautious never accomplishes greatness.",
        "Knowledge is the foundation of strategy and success.",
        "Opportunistic action is the hallmark of the wise."
    ]
    answer += " " + random.choice(wisdom)
    add_insight(answer)
    return answer
from memory import semantic_search, add_insight
import random

def plan_response(query):
    mems = semantic_search(query,3)
    answer = " ".join([m["chanakya"] for m in mems])
    wisdom = [
        "A person who is overly cautious never accomplishes greatness.",
        "Knowledge is the foundation of strategy and success.",
        "Opportunistic action is the hallmark of the wise."
    ]
    answer += " " + random.choice(wisdom)
    add_insight(answer)
    return answer
import asyncio, aiohttp
from bs4 import BeautifulSoup

async def fetch(session,url):
    try:
        async with session.get(url,timeout=5) as r:
            text = await r.text()
            soup = BeautifulSoup(text,'html.parser')
            return soup.get_text()[:1000]
    except:
        return ""

async def multi_search(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session,u) for u in urls]
        return await asyncio.gather(*tasks)

def search_web(query):
    urls=[
        f"https://en.wikipedia.org/wiki/{query.replace(' ','_')}",
        f"https://www.history.com/search?q={query}",
        f"https://www.brainyquote.com/search_results?q={query}"
    ]
    return " ".join(asyncio.run(multi_search(urls)))
import pyttsx3
engine = pyttsx3.init()
engine.setProperty('rate',130)
engine.setProperty('volume',1.0)
voices = engine.getProperty('voices')
for v in voices:
    if "male" in v.name.lower() or "english" in v.name.lower():
        engine.setProperty('voice',v.id)
        break

def speak(text):
    engine.say(text)
    engine.runAndWait()
def get_dashboard_html():
    with open("templates/dashboard.html","r",encoding="utf-8") as f:
        return f.read()
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, HTMLResponse
from reasoning import plan_response
from websearch import search_web
from memory import add_memory
from tts import speak
from dashboard import get_dashboard_html
from auth_admin import auth_required

app = FastAPI()

@app.get("/dashboard")
@auth_required
async def dashboard(request: Request):
    return HTMLResponse(get_dashboard_html())

@app.get("/ask/{query}")
@auth_required
async def ask(query: str):
    answer_mem = plan_response(query)
    web_text = search_web(query)
    answer = answer_mem + "\nWeb info:\n" + web_text[:500]
    add_memory(query, answer)
    speak(answer)
    return JSONResponse({"answer": answer})
uvicorn main:app --reload
http://127.0.0.1:8000/dashboard



